{
    "docs": [
        {
            "location": "/",
            "text": "Tutorials and protocols\n\n\nThese tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the \nAustralian-made Genomics Virtual Laboratory\n and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.",
            "title": "Home"
        },
        {
            "location": "/#tutorials-and-protocols",
            "text": "These tutorials have been developed by bioinformaticians at Melbourne Bioinformatics (formerly VLSCI). They are regularly delivered on-site or may be run in-house for your group. Many of the training materials were developed for use on the  Australian-made Genomics Virtual Laboratory  and these are used in our formal workshops and are also available for use to deliver your own workshops or for self-directed learning.",
            "title": "Tutorials and protocols"
        },
        {
            "location": "/tutorials/python_overview/python_overview/",
            "text": "Authors:\n\n\n\n\n\n\nBernie Pope, Melbourne Bioinformatics (formerly VLSCI)\n\n\n\n\n\n\nCatherine de Burgh-Day, Dept. of Physics, The University of Melbourne\n\n\n\n\n\n\nGeneral information\n\n\n\n\n\n\nPython modules are stored in files containing a \".py\" suffix (e.g solver.py).\n\n\n\n\n\n\nThe main implementation of Python is called CPython (it is written in C). It is byte-code interpreted.\n\n\n\n\n\n\nPython can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates\n  it immediately and then prints the result before prompting for a\n  new input. The interactive prompt is usually rendered as the\n  \nchevron\n \n>>>\n. In scripted mode your program is stored in one\n  or more files which are executed as one monolithic entity. Such\n  programs behave like ordinary applications.\n\n\n\n\n\n\nPython has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed\n  automatically when no longer used.\n\n\n\n\n\n\nPython 2 versus Python 3\n\n\nCurrently there are two distinct flavours of Python available:\n\n\n\n\n\n\nPython 2 (2.7.10 at the time of writing)\n\n\n\n\n\n\nPython 3 (3.4.3 at the time of writing)\n\n\n\n\n\n\nPython 3 is the new and improved version of the language. Python 3 is\nnot entirely backwards compatible, but the two versions share much in\ncommon. Version 2 is now in maintenance mode; new features will only be\nadded to version 3. The public transition from 2 to 3 has been slower\nthan some people would like. You are encouraged to use version 3 where\npossible. These notes are generally compatible with both versions, but\nwe will point out key differences where necessary.\n\n\nIndentation for grouping code blocks\n\n\n\n\n\n\nPython uses indentation to group code blocks. Most other languages use some kind of brackets for grouping.\n\n\n\n\n\n\nThe recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth).\n\n\n\n\n\n\nYou are encouraged \nnot\n to use tabs for indentation because there is no standard width for a tab.\n\n\n\n\n\n\nMost good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.\n\n\n\n\n\n\nStyle Guide\n\n\nA popular style guide for Python is known as \nPEP 0008\n, there is a\ncorresponding tool called \npep8\n which will check your code against\nthe guide and report any transgressions.\n\n\nExample, Python compared to C:\n\n\nPython program for computing factorial:\n\n\n# Compute factorial of n,\n# assuming n >= 0\n\ndef factorial(n):\n  result = 1\n  while n > 0:\n      result *= n\n      n -= 1\n  return result\n\nprint(factorial(10))\n\n\n\n\nC program for computing factorial:\n\n\n#include <stdio.h>\n\n/* Compute factorial of n,\n  assuming n >= 0 */\n\nint factorial(int n) {\n   int result = 1;\n\n   while (n > 0) {\n      result *= n;\n      n -= 1;\n   }\n   return result;\n}\n\nint main(void) {\n   printf(\"%d\\n\", factorial(10));\n}\n\n\n\n\nThings to note:\n\n\n\n\n\n\nThe difference in commenting style.\n\n\n\n\n\n\nC programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed.\n\n\n\n\n\n\nCode blocks in C are grouped by braces { }; Python uses indentation for grouping.\n\n\n\n\n\n\nThe C program must have a main function. Python does not require a\n    main function, it just executes the top-level statements of the\n    module.\n\n\n\n\n\n\nThe result returned by the C function is limited to the size of a\n    machine integer (say 32 bits). However, the result returned by the\n    Python function is unlimited in its size - it can compute\n    arbitrarily large factorials (up to the limit of the available\n    memory in your computer).\n\n\n\n\n\n\nComments\n\n\nProgram comments start with a hash character \"#\" and continue until the\nend of the line. There are no multi-line comment markers, but that can\nsometimes be faked with multi-line string literals.\n\n\nExamples:\n\n\n# This is a comment.\n# This is another comment.\nx = 5 # This is a comment that follows some code.\n'''This is\na multi-line\nstring literal\nwhich can sometimes act like\na\ncomment.\n'''\n\n\n\n\nRunning a Python program\n\n\nThere are many ways to run Python code:\n\n\n\n\n\n\nYou can run the interpreter in interactive mode. On Unix (Linux, OS\n    X) you can run the python command at the command line.\n\n\n\n\n\n\nIf you have Python code stores in a file, say example.py, you can\n    run it from the command line like so: python example.py\n\n\n\n\n\n\nYou can use one of several integrated programming environments.\n    Python ships with a fairly minimal one called \nIDLE\n, though\n    many scientists prefer the more comprehensive \nIPython\n.\n\n\n\n\n\n\nIf your Python code was installed as a package (see below), then it\n    may be executed like an ordinary application without the user\n    being aware of how the program was implemented.\n\n\n\n\n\n\nObjects and types\n\n\n\n\n\n\nEvery value in Python is an \nobject\n (including functions!).\n\n\n\n\n\n\nObjects can have attributes and methods, which are accessed via the\n    dot \".\" operator.\n\n\n\n\n\n\nAll objects have a type.\n\n\n\n\n\n\nTypes are also objects!\n\n\n\n\n\n\nPython is dynamically typed: you may get type errors at runtime but\n    never at compile time.\n\n\n\n\n\n\ntype(x)\n returns the type of x.\n\n\n\n\n\n\nPython variables may be assigned to values of different types at\n    different points in the program.\n\n\n\n\n\n\nInteractive examples (Python 3):\n\n\n>>> # Create a list, assign to the variable x\n>>> x = [3, 1, 2, 3]\n>>> # Ask for the type of the value assigned to x\n>>> type(x)\n<class 'list'>\n>>> # Ask for the type of the first item in the list (an integer)\n>>> type(x[0])\n<class 'int'>\n>>> # Count the number of times 3 appears in the list\n>>> # by calling the count method\n>>> x.count(3)\n2\n>>> # Sort the contents of the list in-place.\n>>> # Note that this mutates the list object!\n>>> # Also note that Python does not print the result in this case.\n>>> x.sort()\n>>> # Ask Python to show the value of the list\n>>> # assigned to the variable x (note it is now sorted)\n>>> x\n[1, 2, 3, 3]\n>>> # Assign x to an object of a different type (a float)\n>>> x = 3.142\n>>> type(x)\n<class 'float'>\n\n\n\n\nBooleans\n\n\n\n\nRepresent truth values\n\n\nValues: \nTrue\n, \nFalse\n\n\nType: \nbool\n\n\nOperators: \nand\n, \nor\n, \nnot\n\n\nbool(x)\n will convert x to a boolean. The heuristic is that empty things and zero-ish things are \nFalse\n, everything else is \nTrue\n (but the user can override for their own types).\n\n\nFalse\n values:\n\n\nFalse\n\n\n0\n (zero integer)\n\n\n0.0\n (zero float)\n\n\n{}\n (empty dictionary)\n\n\n()\n (empty tuple)\n\n\n[]\n (empty list)\n\n\n''\n (empty string)\n\n\nNone\n\n\n\n\n\n\nTrue\n values:\n\n\neverything else\n\n\n\n\n\n\n\n\n\n\nIn numerical contexts \nTrue\n is considered equal to the integer \n1\n and\n    \nFalse\n is considered equal to the integer \n0\n. However, these\n    conversions are a common cause of bugs and should be avoided.\n\n\nPython will automatically test the \ntruthiness\n of a value if it\n    appears in a boolean context.\n\n\n\n\nInteractive examples:\n\n\n>>> not True\nFalse\n>>> not False\nTrue\n>>> not ()\nTrue\n>>> not [1,2,3]\nFalse\n>>> True and False\nFalse\n>>> True and ()\n()\n\n\n\n\nConditional Statements\n\n\n\n\nConditional statements use the keywords: \nif\n, \nelif\n, \nelse\n. The syntax\n    for a conditional statement is:\n\n\n\n\nif expression:\n    statement-block\nelif expression:\n    statement-block\n...\nelse:\n    statement-block\n\n\n\n\n\n\n\n\nA conditional statement must have exactly one \nif\n part. It may have\n    zero or more \nelif\n parts, and a single optional \nelse\n part at the\n    end.\n\n\n\n\n\n\nThe \nif\n and \nelif\n parts test the value of their boolean expressions.\n    If the expression evaluates to something which is \nTrue\n or can be\n    converted to \nTrue\n (see the rules for Booleans above) then the\n    statement block immediately beneath that part is executed.\n    Otherwise the following condition (if any) is tried. The \nelse\n\n    part, if it exists, is always and only executed if no preceding\n    condition was \nTrue\n.\n\n\n\n\n\n\nInteractive examples:\n\n\n>>> if []:\n...     print(\"Was considered True\")\n... else:\n...     print(\"Was considered False\")\n...\nWas considered False\n\n\n\n\nNumbers and basic mathematics\n\n\nIntegers\n\n\n\n\n\n\nRepresent whole negative and positive numbers (and zero).\n\n\n\n\n\n\nThe range of integer values is unbounded (up to some limit defined\n    by how much memory you have on your computer).\n\n\n\n\n\n\nPython 2 distinguishes between two integer types \nint\n and \nlong\n, and\n    automatically promotes \nint\n to long where necessary, whereas Python\n    3 considers them all one type called \nint\n.\n\n\n\n\n\n\nBase ten is the default literal notation: \n42\n (means \n(4 * 10) + 2\n)\n\n\n\n\n\n\nHexadecimal literals start with \n0x\n, octal literals start with \n0o\n,\n    binary literals start with \n0b\n.\n\n\n\n\n\n\nint(x)\n will try to convert x to an integer, x can be another numeric\n    type (including booleans) or a string. You may specify an optional\n    base for the conversion.\n\n\n\n\n\n\nInteractive examples (in Python 3):\n\n\n>>> 2 ** 200\n1606938044258990275541962092341162602522202993782792835301376\n>>> 0x10\n16\n>>> 0b10\n2\n>>> -0 == 0\nTrue\n>>> int(\"123\")\n123\n>>> int(\"3.142\")\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\n    ValueError: invalid literal for int() with base 10: '3.142'\n\n\n\n\nFloating Point Numbers\n\n\n\n\n\n\nRepresent a finite approximation to the real numbers.\n\n\n\n\n\n\nType: \nfloat\n.\n\n\n\n\n\n\n(On most platforms) Python uses IEEE-754 double precision floating\n    point numbers which provide 53 bits of precision.\n\n\n\n\n\n\nsys.float_info\n contains details about max, min, epsilon etcetera.\n\n\n\n\n\n\nLiterals can be in ordinary notation or in exponential notation:\n\n\n\n\nOrdinary: \n3.142\n\n\nExponential: \n314.2e-2\n\n\n\n\n\n\n\n\nOrdinary notation requires a point \n.\n, but digits following the\n    point are optional.\n\n\n\n\n\n\nExponential notation does not require a point unless you have a\n    fractional component.\n\n\n\n\n\n\nfloat(x)\n will try to convert x to a floating point number, x can be\n    another numeric type (including booleans) or a string.\n\n\n\n\n\n\nNumeric operators will automatically convert integer arguments to\n    floating point in mixed-type expressions.\n\n\n\n\n\n\nIn Python 3 the division operator \n/\n computes a floating point result\n    for integer arguments. However, in Python 2 it computes an integer\n    result for integer arguments.\n\n\n\n\n\n\nInteractive examples:\n\n\n>>> type(3.142)\n<class 'float'>\n>>> type(12)\n<class 'int'>\n>>> 3.142 + 12\n15.142\n>>> 3.142 == 314.2e-2\nTrue\n>>> 3. == 3.0\nTrue\n>>> 1/0\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nZeroDivisionError: division by zero\n>>> # Integer divided by integer yields a float in Python 3\n>>> 10 / 3\n3.3333333333333335\n>>> float(\"123\")\n123.0\n>>> float(\"3.142\")\n3.142\n\n\n\n\nComplex Numbers\n\n\n\n\n\n\nRepresent a finite approximation to the complex numbers.\n\n\n\n\n\n\nType: \ncomplex\n\n\n\n\n\n\nA pair of floating point numbers: real +/- imaginary.\n\n\n\n\n\n\nThe real part is optional (defaults to 0). The imaginary part is\n    followed immediately by the character \nj\n.\n\n\n\n\n\n\nInteractive Examples:\n\n\n>>> 5j + 3j\n8j\n>>> 2-5j\n(2-5j)\n>>> 2-5j + 3j\n(2-2j)\n\n\n\n\nNumeric Operators\n\n\n\n\n\n\n\n\nName\n\n\nOperation\n\n\nPrecedence\n\n\nAssociativity\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n+\n\n\nadd\n\n\nlow\n\n\nleft\n\n\nCan also be used to concatenate strings together.\n\n\n\n\n\n\n*\n\n\nmultiply\n\n\nmedium\n\n\nleft\n\n\n\n\n\n\n\n\n-\n\n\nsubtract\n\n\nlow\n\n\nleft\n\n\n\n\n\n\n\n\n/\n\n\ndivide\n\n\nmedium\n\n\nleft\n\n\nIn Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers.\n\n\n\n\n\n\n//\n\n\nfloor-divide\n\n\nmedium\n\n\nleft\n\n\ndivide then floor, result is an integer\n\n\n\n\n\n\n**\n\n\nexponent\n\n\nhigh\n\n\nright\n\n\n\n\n\n\n\n\n%\n\n\nmodulus\n\n\nmedium\n\n\nleft\n\n\nremainder after division\n\n\n\n\n\n\n\n\nInteractive Examples (Python 3):\n\n\n>>> 3 + 4 * 5\n23\n>>> (3 + 4) * 5\n35\n>>> 10 / 3\n3.3333333333333335\n>>> 10 // 3\n3\n>>> 10 % 3\n1\n>>> 2 ** 3 ** 4\n2417851639229258349412352\n>>> (2 ** 3) ** 4\n4096\n\n\n\n\nStrings\n\n\n\n\n\n\nRepresent text\n\n\n\n\n\n\nType: \nstr\n\n\n\n\n\n\nIn Python 3, the str type contains Unicode characters.\n\n\n\n\n\n\nIn Python 2, the str type contains ASCII characters (sometimes\n    called byte strings). Python 2 has a separate type for unicode\n    strings, the type is called unicode; literals of this type are\n    prefixed by the letter \nu\n.\n\n\n\n\n\n\nString literals must be quoted. There are 3 quoting styles:\n\n\n\n\n\n\nsingle quote characters: \n'hello'\n\n\n\n\n\n\ndouble quote characters: \n\"hello\"\n\n\n\n\n\n\ntriple quote characters: \n'''hello'''\n (three single quotes in a\n    row) or \n\"\"\"hello\"\"\"\n (three double quote characters in a row)\n\n\n\n\n\n\n\n\n\n\nThe single quote and double quote versions of strings have the same\n    value. The purpose of the different quotation styles is to make it\n    convenient to have literal quotation marks inside strings\n    (avoiding the need to escape the quote character). For example:\n\n\n\n\n\n\n>>> \"This inverted comma won't be a problem inside quotation marks\"\n\"This inverted comma won't be a problem inside quotation marks\"\n>>> 'this \"quote\" will work'\n'this \"quote\" will work'\n>>> 'this isn't going to work though'\nFile \"<stdin>\", line 1\n'this isn't going to work though'\n^ SyntaxError: invalid syntax\n\n\n\n\n\n\n\n\nTriple quoted strings can be written on multiple lines. The line\n    breaks will be preserved within the string. Useful for docstrings\n    (see section on functions).\n\n\n\n\n\n\nThe usual set of escape characters are supported:\n\n\n\n\n\n\n\\n\n newline\n\n\n\n\n\n\n\\t\n tab\n\n\n\n\n\n\n\\\\\n backslash\n\n\n\n\n\n\n\\'\n single quote\n\n\n\n\n\n\n\\\"\n double quote\n\n\n\n\n\n\nand many more\n\n\n\n\n\n\n\n\n\n\nPython does not have a separate type for representing individual\n    characters. Instead you use strings of length one.\n\n\n\n\n\n\nStrings are iterable. If you iterate over a string (using a for\n    loop) you process it one character at a time from left to right.\n\n\n\n\n\n\nStrings can be indexed to obtain individual characters, e.g. \ns[5]\n\n\n\n\n\n\nIndices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the string).\n\n\n\n\n\n\nStrings are immutable: you cannot modify a string once it has been\n    created.\n\n\n\n\n\n\nInteractive Examples (Python 3):\n\n\n>>> type(\"hello\")\n<class 'str'>\n>>> \"hello\" == 'hello'\nTrue\n>>> '''This string\n... is on\n... multiple\n... lines'''\n'This string\\\\nis on\\\\nmultiple\\\\nlines'\n>>> \"bonjour\".upper()\n'BONJOUR'\n>>> len(\"bonjour\")\n7\n>>> \"bonjour\".startswith(\"b\")\nTrue\n>>> \"cat,sat,flat\".split(\",\")\n['cat', 'sat', 'flat']\n>>> # Print the first 5 Chinese unicode characters\n>>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04')\n\u4e00\u4e01\u4e02\u4e03\u4e04\n>>> x = \"floyd\"\n>>> x[0]\n'f'\n>>> \"hello\" + \" \" + \"world\"\n'hello world'\n\n\n\n\nExample program:\n\n\n# Prompt the user to input a string:\ninput = raw_input(\"Enter string: \")\n\n# Count the number of vowels in the input string\nvowels = 'aeiou'\ncount = 0\n\nfor char in input:\n    if char in vowels:\n        count += 1\n\n# Print the count to the standard output\nprint(count)\n\n\n\n\nExample usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called \nvowels.py\n:\n\n\npython vowels.py\nEnter string: abracadabra\n5\n\n\n\n\nLists\n\n\n\n\n\n\nRepresent mutable ordered sequences of values.\n\n\n\n\n\n\nType: \nlist\n\n\n\n\n\n\nList literals are written in between square brackets, e.g. \n[1, 2, 3]\n\n\n\n\n\n\nList elements can be objects of any type (including other lists).\n\n\n\n\n\n\nLike strings, lists can be indexed like so: \nx[3]\n\n\n\n\n\n\nIndices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the list).\n\n\n\n\n\n\nLists are mutable. You can update items, delete items and add new\n    items.\n\n\n\n\n\n\nIndexing into a list is a constant time (amortised) operation.\n\n\n\n\n\n\nInteractive Examples:\n\n\n>>> type([1, 2, 3])\n<class 'list'>\n>>> x = []\n>>> len(x)\n0\n>>> x.append(\"hello\")\n>>> x\n['hello']\n>>> len(x)\n1\n>>> x[0]\n'hello'\n>>> x.insert(0, True)\n>>> x\n[True, 'hello']\n>>> del x[1]\n>>> x\n[True]\n>>> x += [42, \"Newton\", 3.142]\n>>> x\n[True, 42, 'Newton', 3.142]\n\n\n\n\nDictionaries\n\n\n\n\n\n\nRepresent finite mappings from keys to values.\n\n\n\n\n\n\nAre implemented as \nhash tables\n. The key objects must be hashable\n    (which rules out mutable objects, such as lists).\n\n\n\n\n\n\nType: \ndict\n\n\n\n\n\n\nDictionary literals are written inside curly brackets, with\n    key-value pairs separated by colons: e.g. \n{12: \"XII\", 6: \"VI\"}\n\n\n\n\n\n\nDictionaries can be indexed by keys. If the key exists in the\n    dictionary its corresponding value is returned, otherwise a\n    \nKeyError\n exception is raised.\n\n\n\n\n\n\nThe cost of indexing a dictionary is proportional to the time taken\n    to hash the key. For many keys this can be considered constant\n    time. For variable sized objects, such as strings, this can be\n    considered to be proportional to the size of the object.\n\n\n\n\n\n\nIterating over a dictionary yields one key at a time. All keys in\n    the dictionary are visited exactly once. The order in which the\n    keys are visited is arbitrary.\n\n\n\n\n\n\nYou may test if an object is a key of a dictionary using the in\n    operator.\n\n\n\n\n\n\nInteractive Examples:\n\n\n>>> type({12: \"XII\", 6: \"VI\"})\n<class 'dict'>\n>>> friends = {}\n>>> friends['Fred'] = ['Barney', 'Dino']\n>>> friends\n{'Fred': ['Barney', 'Dino']}\n>>> friends['Fred']\n['Barney', 'Dino']\n>>> friends['Barney']\nTraceback (most recent call last):\nFile \"\\<stdin\\>\", line 1, in \\<module\\>\nKeyError: 'Barney'\n>>> friends['Wilma'] = ['Betty']\n>>> friends\n{'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']}\n>>> friends.keys()\ndict_keys(['Fred', 'Wilma'])\n>>> friends.values()\ndict_values([['Barney', 'Dino'], ['Betty']])\n>>> 'Dino' in friends\nFalse\n\n\n\n\nExample program:\n\n\n# Compute and print a histogram of a sequence of integers entered\n# on standard input, one number per line\n\nimport sys\n\nhistogram = {}\n\n# Iterate over each line in the standard input\nfor line in sys.stdin:\n    # Parse the next input as an integer\n    next_integer = int(line)\n    # Update the histogram accordingly\n    if next_integer in histogram:\n        # We've seen this integer before\n        histogram[next_integer] += 1\n    else:\n        # First occurrence of this integer in the input\n        histogram[next_integer] = 1\n\n# Print each key: value pair in the histogram in ascending\n# sorted order of keys\nfor key in sorted(histogram):\n    print(\"{} {}\".format(key, histogram[key]))s\n\n\n\n\nExample usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called \nhisto.py\n:\n\n\npython histo.py\n\n\n\n\nUser types in a sequence of integers to the program, one per line, and\npresses control-d to terminate the input:\n\n\n3\n43\n12\n19\n3\n12\n12\n43\n\n\n\n\nProgram prints its output:\n\n\n3 2\n12 3\n19 1\n43 2\n\n\n\n\nTuples\n\n\n\n\n\n\nRepresent \nimmutable\n ordered sequences of values.\n\n\n\n\n\n\nVery much like lists except they cannot be modified once created.\n\n\n\n\n\n\nType: \ntuple\n\n\n\n\n\n\nLiterals are written in between parentheses: \n(1, 2, 3)\n\n\n\n\n\n\nThe can be used as keys in dictionaries (unlike lists).\n\n\n\n\n\n\nLoops\n\n\nWhile loops\n\n\n\n\n\n\nIterate until condition is \nFalse\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nwhile expression:\n    statement_block\n\n\n\n\n\n\nThe value of the boolean expression is tested. If it evaluates to\n    \nTrue\n then the statement block is executed once, before repeating\n    the loop. If it evaluates to False then the program continues\n    execution immediately after the loop.\n\n\n\n\nExample:\n\n\ndef factorial(n):\n    result = 1\n    while n > 0:\n        result *= n\n        n -= 1\n    return result\n\n\n\n\nFor loops\n\n\n\n\n\n\nIterate over each item in a collection (e.g. list, string, tuple,\n    dictionary, file).\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nfor variable in expression:\n    statement_block\n\n\n\n\n\n\n\n\nEach item from the iterator expression is selected and assigned to\n    the variable, then the statement block is executed. The loop ends\n    when every item in the iterator has been visited.\n\n\n\n\n\n\nThe order of items visited in the iterator depends on the type of\n    the iterator. Lists, strings and tuples proceed in a left-to-right\n    fashion. Files proceed one line at a time. Dictionaries proceed in\n    an arbitrary order.\n\n\n\n\n\n\nThe \nrange()\n function is useful for generating iterators of numbers\n    within a range. Note that the lower bound is inclusive and the\n    upper bound is exclusive.\n\n\n\n\n\n\nExample:\n\n\ndef factorial(n):\n    result = 1\n    for item in range(n + 1):\n        result *= item\n    return result\n\n\n\n\nBreak and continue\n\n\n\n\n\n\nBoth types of loops support the \nbreak\n and \ncontinue\n keywords.\n\n\n\n\n\n\nbreak\n terminates the loop immediately.\n\n\n\n\n\n\ncontinue\n jumps immediately back to the start of the loop.\n\n\n\n\n\n\nThey can sometimes simplify the conditional logic of a loop, but\n    should be used sparingly.\n\n\n\n\n\n\nFunctions\n\n\n\n\n\n\nAllow you to define reusable abstractions. Sometimes called\n    \nprocedures\n.\n\n\n\n\n\n\nAre generally defined at the top level of a module, and can also be\n    nested.\n\n\n\n\n\n\nType: \nfunction\n\n\n\n\n\n\nNamed functions are bound to a variable name and may have complex\n    bodies.\n\n\n\n\n\n\nAnonymous functions are used in-line, and may only have expression\n    bodies.\n\n\n\n\n\n\nNamed function syntax:\n\n\n\n\n\n\ndef variable(parameter_list):\n    statement_block\n\n\n\n\n\n\nAnonymous function syntax:\n\n\n\n\nlambda parameter_list: expression\n\n\n\n\nExample:\n\n\ndef is_leap_year(year):\n    if year % 4 == 0 and year % 100 != 0:\n        return True\n    else:\n        return year % 400 == 0\n\nfor year in range(2000, 2100 + 1):\n    result = is_leap_year(year)\n    print(\"{} {}\".format(year, result))\n\n\n\n\nAnonymous function example:\n\n\n>>> squared = lambda x: x ** 2\n>>> squared(2)\n4\n>>> list(map(lambda x: x + 1, [1, 2, 3]))\n[2, 3, 4]\n\n\n\n\nInput and output\n\n\n\n\n\n\nThe \nprint\n function is useful for displaying text (and other values\n    converted to text).\n\n\n\n\n\n\nIn Python 2 \nprint\n was a special keyword. In Python 3 it is a\n    function defined in the builtins.\n\n\n\n\n\n\nFancy string formatting can be done with the format method on\n    strings. Older Python code uses the string interpolation operator\n    for the same task \n%\n, but its use is now discouraged.\n\n\n\n\n\n\nFiles must be opened before than can be manipulated. A file can be\n    opened in different modes: read \n\"r\"\n, write \n\"w\"\n, read-write \n\"r+\"\n,\n    and append \n\"a\"\n. Opening a new file in write or append modes\n    creates a new file. Opening an existing file in write mode\n    overwrites its contents from the start. Opening an existing file\n    in append mode adds new content at the end of the old content.\n\n\n\n\n\n\nWhen you are finished processing a file you should close it as soon\n    as possible. Closing a file releases limited operating system\n    resources, and ensures that any pending buffered writes a flushed\n    to the storage system.\n\n\n\n\n\n\nCertain file types have libraries for convenient processing. One\n    example is the CSV (comma separated values) library for processing\n    tabular data. It is very handy for working with spreadsheets.\n\n\n\n\n\n\nThe command line arguments of a Python program are contained in a\n    list called \nsys.argv\n (it is a variable exported from the \nsys\n\n    module). For complex program you should consider using a command\n    line argument parsing library such as \nargparse\n.\n\n\n\n\n\n\nExample program:\n\n\n# Count the number of words and lines in a file\n\nimport sys\n\n# Get the input file name from the command line arguments\nfilename = sys.argv[1]\n\n# Open the file\nfile = open(filename)\n\n# Count the number of lines in the file\nnum_lines = 0\nnum_words = 0\n\nfor line in file:\n    num_lines += 1\n    num_words += len(line.split())\n\nfile.close()\n\nprint(\"Number of lines and words in {}: {} {}\" \\\n        .format(filename, num_lines, num_words))\n\n\n\n\nAdvanced Topics\n\n\nClasses\n\n\n\n\n\n\nClasses allow you to define your own types.\n\n\n\n\n\n\nClass definitions may define methods for the type.\n\n\n\n\n\n\nA class may inherit, and possibly override, some functionality from\n    a superclass.\n\n\n\n\n\n\nSyntax:\n\n\n\n\n\n\nclass variable(superclass_list):\n    body\n\n\n\n\n\n\n\n\nThe name of the class is given by the variable in the definition.\n    The superclass list defines the superclasses of the new class\n    (very often the base type object is used). The body of the class\n    typically defines one or more methods.\n\n\n\n\n\n\nInstances of classes are created by calling the class name as if it\n    were a function.\n\n\n\n\n\n\nIf defined, the special method called \n__init__\n is used to\n    initialise a newly created instance of a class.\n\n\n\n\n\n\nThe first parameter to each method is the object upon which the\n    method was called. The convention is to use the variable called\n    self, however any variable name will do. Many object oriented\n    languages make this variable an implicit parameter called this.\n\n\n\n\n\n\nExample:\n\n\nclass Vector(object):\n    def __init__(self, x=0, y=0, z=0):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def magnitude(self):\n        return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2)\n\n    def normalise(self):\n        magnitude = self.magnitude()\n        if magnitude == 0:\n            # Somehow we have a degenerate vector.\n            return self\n        else:\n            return self / self.magnitude()\n\n    def angle(self, other):\n        dp = self.dot_product(other)\n        return acos(dp / self.magnitude() * other.magnitude())\n\n    def dot_product(self, other):\n        return self.x * other.x + self.y * other.y + self.z * other.z\n\n\n\n\nExceptions\n\n\n\n\n\n\nExceptions allow Python programs to handle erroneous program\n    conditions.\n\n\n\n\n\n\nAn exception is raised (or thrown) at the point of the error and\n    handled (or caught) at some other place in the program.\n\n\n\n\n\n\nException handlers have the syntax:\n\n\n\n\n\n\ntry:\n    statement_block\nexcept exception_type as variable:\n    statement_block\n...\n\n\n\n\n\n\n\n\nThe statement block after \ntry\n is executed. If no exceptions are\n    raised in that block the program continues immediately after the\n    exception handler. If an exception is raised in the block then\n    program control jumps to the innermost closing \nexcept\n clause.\n    Except clauses may optionally specify the set of exception types\n    that they can handle. If the raised exception is an instance of\n    the handled type then the body of the except clause is executed,\n    otherwise the next except clause (if any) is tried.\n\n\n\n\n\n\nIf no matching exception handler is found then the program will\n    terminate with an unhandled exception error. Python will normally\n    print a stack trace at this point for error diagnosis.\n\n\n\n\n\n\nYou may raise your own exceptions using the \nraise\n keyword.\n\n\n\n\n\n\nExample:\n\n\n# alternative version of the histogram code from the section on\n# dictionaries\nfor line in sys.stdin:\n    next_integer = int(line)\n    try:\n        histogram[next_integer] += 1\n    except KeyError:\n        histogram[next_integer] = 1\n\n\n\n\nModules\n\n\n\n\n\n\nA module is a file which contains Python code.\n\n\n\n\n\n\nAny Python file you create is automatically a module.\n\n\n\n\n\n\nIt is considered good programming style to decompose complex\n    programs into multiple modules. Each module should collect\n    together code with similar purpose.\n\n\n\n\n\n\nVariables defined at the top level of a module (such as global\n    variables, functions and classes) can be imported into other\n    modules.\n\n\n\n\n\n\nPython comes with many standard modules.\n\n\n\n\n\n\nThe \nimport\n keyword is used to import an entire module.\n\n\n\n\n\n\nYou may import a subset of things from a module using the \nfrom ... import ...\n syntax.\n\n\n\n\n\n\nYou may import a module with a new name using the \nfrom ... import ... as ...\n or \nimport ... as ...\n\n\n\n\n\n\nWhen a module is first imported in a program, all of its top-level\n    statements are executed from top to bottom. Subsequent imports use\n    a cached version of its definitions, its statements are not\n    re-executed.\n\n\n\n\n\n\nA special module called \nbuiltins\n is imported into every other module\n    by default, and it is automatically imported at the interactive\n    prompt in the interpreter.\n\n\n\n\n\n\nInteractive Example:\n\n\n>>> import math\n>>> math.sqrt(100)\n10.0\n>>> sqrt(100)\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nNameError: name 'sqrt' is not defined\n>>> from math import sqrt\n>>> sqrt(100)\n10.0\n>>> import math as m\n>>> m.sqrt(100)\n10.0\n\n\n\n\nPackages\n\n\n\n\n\n\nA package is a collection of modules in a hierarchy.\n\n\n\n\n\n\nPackages are the common way to structure Python libraries.\n\n\n\n\n\n\nThe \nPython Package Index (PyPI)\n is a big collection of open\n    source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of\n    writing).\n\n\n\n\n\n\nPackage installation tools such as \npip\n, make it easy to install\n    packages onto your computer.\n\n\n\n\n\n\nIf you want to make your own Python code easy for others to install\n    and use then you should consider making it a package. You can even\n    upload it to PyPI.\n\n\n\n\n\n\nMany people use \nvirtualenv\n to install packages into a local\n    \"sandboxed\" Python environment. This avoids conflicts with the\n    central Python package database on your computer, and allows\n    multiple different versions of packages to be used.",
            "title": "Python Overview"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#authors",
            "text": "Bernie Pope, Melbourne Bioinformatics (formerly VLSCI)    Catherine de Burgh-Day, Dept. of Physics, The University of Melbourne",
            "title": "Authors:"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#general-information",
            "text": "Python modules are stored in files containing a \".py\" suffix (e.g solver.py).    The main implementation of Python is called CPython (it is written in C). It is byte-code interpreted.    Python can be used in two modes: interactive and scripted. In interactive mode you enter a program fragment and Python evaluates\n  it immediately and then prints the result before prompting for a\n  new input. The interactive prompt is usually rendered as the\n   chevron   >>> . In scripted mode your program is stored in one\n  or more files which are executed as one monolithic entity. Such\n  programs behave like ordinary applications.    Python has automatic memory management (via garbage collection). Memory is allocated automatically as needed and freed\n  automatically when no longer used.",
            "title": "General information"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#python-2-versus-python-3",
            "text": "Currently there are two distinct flavours of Python available:    Python 2 (2.7.10 at the time of writing)    Python 3 (3.4.3 at the time of writing)    Python 3 is the new and improved version of the language. Python 3 is\nnot entirely backwards compatible, but the two versions share much in\ncommon. Version 2 is now in maintenance mode; new features will only be\nadded to version 3. The public transition from 2 to 3 has been slower\nthan some people would like. You are encouraged to use version 3 where\npossible. These notes are generally compatible with both versions, but\nwe will point out key differences where necessary.",
            "title": "Python 2 versus Python 3"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#indentation-for-grouping-code-blocks",
            "text": "Python uses indentation to group code blocks. Most other languages use some kind of brackets for grouping.    The recommended style is to use 4 space characters for a single indent (thus 8 spaces for two indents and so forth).    You are encouraged  not  to use tabs for indentation because there is no standard width for a tab.    Most good text editors can be configured so that that tab key is rendered as 4 space characters when editing Python code.",
            "title": "Indentation for grouping code blocks"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#style-guide",
            "text": "A popular style guide for Python is known as  PEP 0008 , there is a\ncorresponding tool called  pep8  which will check your code against\nthe guide and report any transgressions.  Example, Python compared to C:  Python program for computing factorial:  # Compute factorial of n,\n# assuming n >= 0\n\ndef factorial(n):\n  result = 1\n  while n > 0:\n      result *= n\n      n -= 1\n  return result\n\nprint(factorial(10))  C program for computing factorial:  #include <stdio.h>\n\n/* Compute factorial of n,\n  assuming n >= 0 */\n\nint factorial(int n) {\n   int result = 1;\n\n   while (n > 0) {\n      result *= n;\n      n -= 1;\n   }\n   return result;\n}\n\nint main(void) {\n   printf(\"%d\\n\", factorial(10));\n}  Things to note:    The difference in commenting style.    C programs are statically typed, and you must declare the type of functions and variables. Python is dynamically typed.    Code blocks in C are grouped by braces { }; Python uses indentation for grouping.    The C program must have a main function. Python does not require a\n    main function, it just executes the top-level statements of the\n    module.    The result returned by the C function is limited to the size of a\n    machine integer (say 32 bits). However, the result returned by the\n    Python function is unlimited in its size - it can compute\n    arbitrarily large factorials (up to the limit of the available\n    memory in your computer).",
            "title": "Style Guide"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#comments",
            "text": "Program comments start with a hash character \"#\" and continue until the\nend of the line. There are no multi-line comment markers, but that can\nsometimes be faked with multi-line string literals.  Examples:  # This is a comment.\n# This is another comment.\nx = 5 # This is a comment that follows some code.\n'''This is\na multi-line\nstring literal\nwhich can sometimes act like\na\ncomment.\n'''",
            "title": "Comments"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#running-a-python-program",
            "text": "There are many ways to run Python code:    You can run the interpreter in interactive mode. On Unix (Linux, OS\n    X) you can run the python command at the command line.    If you have Python code stores in a file, say example.py, you can\n    run it from the command line like so: python example.py    You can use one of several integrated programming environments.\n    Python ships with a fairly minimal one called  IDLE , though\n    many scientists prefer the more comprehensive  IPython .    If your Python code was installed as a package (see below), then it\n    may be executed like an ordinary application without the user\n    being aware of how the program was implemented.",
            "title": "Running a Python program"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#objects-and-types",
            "text": "Every value in Python is an  object  (including functions!).    Objects can have attributes and methods, which are accessed via the\n    dot \".\" operator.    All objects have a type.    Types are also objects!    Python is dynamically typed: you may get type errors at runtime but\n    never at compile time.    type(x)  returns the type of x.    Python variables may be assigned to values of different types at\n    different points in the program.    Interactive examples (Python 3):  >>> # Create a list, assign to the variable x\n>>> x = [3, 1, 2, 3]\n>>> # Ask for the type of the value assigned to x\n>>> type(x)\n<class 'list'>\n>>> # Ask for the type of the first item in the list (an integer)\n>>> type(x[0])\n<class 'int'>\n>>> # Count the number of times 3 appears in the list\n>>> # by calling the count method\n>>> x.count(3)\n2\n>>> # Sort the contents of the list in-place.\n>>> # Note that this mutates the list object!\n>>> # Also note that Python does not print the result in this case.\n>>> x.sort()\n>>> # Ask Python to show the value of the list\n>>> # assigned to the variable x (note it is now sorted)\n>>> x\n[1, 2, 3, 3]\n>>> # Assign x to an object of a different type (a float)\n>>> x = 3.142\n>>> type(x)\n<class 'float'>",
            "title": "Objects and types"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#booleans",
            "text": "Represent truth values  Values:  True ,  False  Type:  bool  Operators:  and ,  or ,  not  bool(x)  will convert x to a boolean. The heuristic is that empty things and zero-ish things are  False , everything else is  True  (but the user can override for their own types).  False  values:  False  0  (zero integer)  0.0  (zero float)  {}  (empty dictionary)  ()  (empty tuple)  []  (empty list)  ''  (empty string)  None    True  values:  everything else      In numerical contexts  True  is considered equal to the integer  1  and\n     False  is considered equal to the integer  0 . However, these\n    conversions are a common cause of bugs and should be avoided.  Python will automatically test the  truthiness  of a value if it\n    appears in a boolean context.   Interactive examples:  >>> not True\nFalse\n>>> not False\nTrue\n>>> not ()\nTrue\n>>> not [1,2,3]\nFalse\n>>> True and False\nFalse\n>>> True and ()\n()",
            "title": "Booleans"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#conditional-statements",
            "text": "Conditional statements use the keywords:  if ,  elif ,  else . The syntax\n    for a conditional statement is:   if expression:\n    statement-block\nelif expression:\n    statement-block\n...\nelse:\n    statement-block    A conditional statement must have exactly one  if  part. It may have\n    zero or more  elif  parts, and a single optional  else  part at the\n    end.    The  if  and  elif  parts test the value of their boolean expressions.\n    If the expression evaluates to something which is  True  or can be\n    converted to  True  (see the rules for Booleans above) then the\n    statement block immediately beneath that part is executed.\n    Otherwise the following condition (if any) is tried. The  else \n    part, if it exists, is always and only executed if no preceding\n    condition was  True .    Interactive examples:  >>> if []:\n...     print(\"Was considered True\")\n... else:\n...     print(\"Was considered False\")\n...\nWas considered False",
            "title": "Conditional Statements"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#numbers-and-basic-mathematics",
            "text": "",
            "title": "Numbers and basic mathematics"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#integers",
            "text": "Represent whole negative and positive numbers (and zero).    The range of integer values is unbounded (up to some limit defined\n    by how much memory you have on your computer).    Python 2 distinguishes between two integer types  int  and  long , and\n    automatically promotes  int  to long where necessary, whereas Python\n    3 considers them all one type called  int .    Base ten is the default literal notation:  42  (means  (4 * 10) + 2 )    Hexadecimal literals start with  0x , octal literals start with  0o ,\n    binary literals start with  0b .    int(x)  will try to convert x to an integer, x can be another numeric\n    type (including booleans) or a string. You may specify an optional\n    base for the conversion.    Interactive examples (in Python 3):  >>> 2 ** 200\n1606938044258990275541962092341162602522202993782792835301376\n>>> 0x10\n16\n>>> 0b10\n2\n>>> -0 == 0\nTrue\n>>> int(\"123\")\n123\n>>> int(\"3.142\")\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\n    ValueError: invalid literal for int() with base 10: '3.142'",
            "title": "Integers"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#floating-point-numbers",
            "text": "Represent a finite approximation to the real numbers.    Type:  float .    (On most platforms) Python uses IEEE-754 double precision floating\n    point numbers which provide 53 bits of precision.    sys.float_info  contains details about max, min, epsilon etcetera.    Literals can be in ordinary notation or in exponential notation:   Ordinary:  3.142  Exponential:  314.2e-2     Ordinary notation requires a point  . , but digits following the\n    point are optional.    Exponential notation does not require a point unless you have a\n    fractional component.    float(x)  will try to convert x to a floating point number, x can be\n    another numeric type (including booleans) or a string.    Numeric operators will automatically convert integer arguments to\n    floating point in mixed-type expressions.    In Python 3 the division operator  /  computes a floating point result\n    for integer arguments. However, in Python 2 it computes an integer\n    result for integer arguments.    Interactive examples:  >>> type(3.142)\n<class 'float'>\n>>> type(12)\n<class 'int'>\n>>> 3.142 + 12\n15.142\n>>> 3.142 == 314.2e-2\nTrue\n>>> 3. == 3.0\nTrue\n>>> 1/0\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nZeroDivisionError: division by zero\n>>> # Integer divided by integer yields a float in Python 3\n>>> 10 / 3\n3.3333333333333335\n>>> float(\"123\")\n123.0\n>>> float(\"3.142\")\n3.142",
            "title": "Floating Point Numbers"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#complex-numbers",
            "text": "Represent a finite approximation to the complex numbers.    Type:  complex    A pair of floating point numbers: real +/- imaginary.    The real part is optional (defaults to 0). The imaginary part is\n    followed immediately by the character  j .    Interactive Examples:  >>> 5j + 3j\n8j\n>>> 2-5j\n(2-5j)\n>>> 2-5j + 3j\n(2-2j)",
            "title": "Complex Numbers"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#numeric-operators",
            "text": "Name  Operation  Precedence  Associativity  Notes      +  add  low  left  Can also be used to concatenate strings together.    *  multiply  medium  left     -  subtract  low  left     /  divide  medium  left  In Python 3 the result is always a floating point number. In Python 2 the result is an integer if both operands are integers.    //  floor-divide  medium  left  divide then floor, result is an integer    **  exponent  high  right     %  modulus  medium  left  remainder after division     Interactive Examples (Python 3):  >>> 3 + 4 * 5\n23\n>>> (3 + 4) * 5\n35\n>>> 10 / 3\n3.3333333333333335\n>>> 10 // 3\n3\n>>> 10 % 3\n1\n>>> 2 ** 3 ** 4\n2417851639229258349412352\n>>> (2 ** 3) ** 4\n4096",
            "title": "Numeric Operators"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#strings",
            "text": "Represent text    Type:  str    In Python 3, the str type contains Unicode characters.    In Python 2, the str type contains ASCII characters (sometimes\n    called byte strings). Python 2 has a separate type for unicode\n    strings, the type is called unicode; literals of this type are\n    prefixed by the letter  u .    String literals must be quoted. There are 3 quoting styles:    single quote characters:  'hello'    double quote characters:  \"hello\"    triple quote characters:  '''hello'''  (three single quotes in a\n    row) or  \"\"\"hello\"\"\"  (three double quote characters in a row)      The single quote and double quote versions of strings have the same\n    value. The purpose of the different quotation styles is to make it\n    convenient to have literal quotation marks inside strings\n    (avoiding the need to escape the quote character). For example:    >>> \"This inverted comma won't be a problem inside quotation marks\"\n\"This inverted comma won't be a problem inside quotation marks\"\n>>> 'this \"quote\" will work'\n'this \"quote\" will work'\n>>> 'this isn't going to work though'\nFile \"<stdin>\", line 1\n'this isn't going to work though'\n^ SyntaxError: invalid syntax    Triple quoted strings can be written on multiple lines. The line\n    breaks will be preserved within the string. Useful for docstrings\n    (see section on functions).    The usual set of escape characters are supported:    \\n  newline    \\t  tab    \\\\  backslash    \\'  single quote    \\\"  double quote    and many more      Python does not have a separate type for representing individual\n    characters. Instead you use strings of length one.    Strings are iterable. If you iterate over a string (using a for\n    loop) you process it one character at a time from left to right.    Strings can be indexed to obtain individual characters, e.g.  s[5]    Indices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the string).    Strings are immutable: you cannot modify a string once it has been\n    created.    Interactive Examples (Python 3):  >>> type(\"hello\")\n<class 'str'>\n>>> \"hello\" == 'hello'\nTrue\n>>> '''This string\n... is on\n... multiple\n... lines'''\n'This string\\\\nis on\\\\nmultiple\\\\nlines'\n>>> \"bonjour\".upper()\n'BONJOUR'\n>>> len(\"bonjour\")\n7\n>>> \"bonjour\".startswith(\"b\")\nTrue\n>>> \"cat,sat,flat\".split(\",\")\n['cat', 'sat', 'flat']\n>>> # Print the first 5 Chinese unicode characters\n>>> print('\\u4E00\\u4E01\\u4E02\\u4E03\\u4E04')\n\u4e00\u4e01\u4e02\u4e03\u4e04\n>>> x = \"floyd\"\n>>> x[0]\n'f'\n>>> \"hello\" + \" \" + \"world\"\n'hello world'  Example program:  # Prompt the user to input a string:\ninput = raw_input(\"Enter string: \")\n\n# Count the number of vowels in the input string\nvowels = 'aeiou'\ncount = 0\n\nfor char in input:\n    if char in vowels:\n        count += 1\n\n# Print the count to the standard output\nprint(count)  Example usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called  vowels.py :  python vowels.py\nEnter string: abracadabra\n5",
            "title": "Strings"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#lists",
            "text": "Represent mutable ordered sequences of values.    Type:  list    List literals are written in between square brackets, e.g.  [1, 2, 3]    List elements can be objects of any type (including other lists).    Like strings, lists can be indexed like so:  x[3]    Indices are zero-based (but you may also use negative indices to\n    access items with respect to the right end of the list).    Lists are mutable. You can update items, delete items and add new\n    items.    Indexing into a list is a constant time (amortised) operation.    Interactive Examples:  >>> type([1, 2, 3])\n<class 'list'>\n>>> x = []\n>>> len(x)\n0\n>>> x.append(\"hello\")\n>>> x\n['hello']\n>>> len(x)\n1\n>>> x[0]\n'hello'\n>>> x.insert(0, True)\n>>> x\n[True, 'hello']\n>>> del x[1]\n>>> x\n[True]\n>>> x += [42, \"Newton\", 3.142]\n>>> x\n[True, 42, 'Newton', 3.142]",
            "title": "Lists"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#dictionaries",
            "text": "Represent finite mappings from keys to values.    Are implemented as  hash tables . The key objects must be hashable\n    (which rules out mutable objects, such as lists).    Type:  dict    Dictionary literals are written inside curly brackets, with\n    key-value pairs separated by colons: e.g.  {12: \"XII\", 6: \"VI\"}    Dictionaries can be indexed by keys. If the key exists in the\n    dictionary its corresponding value is returned, otherwise a\n     KeyError  exception is raised.    The cost of indexing a dictionary is proportional to the time taken\n    to hash the key. For many keys this can be considered constant\n    time. For variable sized objects, such as strings, this can be\n    considered to be proportional to the size of the object.    Iterating over a dictionary yields one key at a time. All keys in\n    the dictionary are visited exactly once. The order in which the\n    keys are visited is arbitrary.    You may test if an object is a key of a dictionary using the in\n    operator.    Interactive Examples:  >>> type({12: \"XII\", 6: \"VI\"})\n<class 'dict'>\n>>> friends = {}\n>>> friends['Fred'] = ['Barney', 'Dino']\n>>> friends\n{'Fred': ['Barney', 'Dino']}\n>>> friends['Fred']\n['Barney', 'Dino']\n>>> friends['Barney']\nTraceback (most recent call last):\nFile \"\\<stdin\\>\", line 1, in \\<module\\>\nKeyError: 'Barney'\n>>> friends['Wilma'] = ['Betty']\n>>> friends\n{'Fred': ['Barney', 'Dino'], 'Wilma': ['Betty']}\n>>> friends.keys()\ndict_keys(['Fred', 'Wilma'])\n>>> friends.values()\ndict_values([['Barney', 'Dino'], ['Betty']])\n>>> 'Dino' in friends\nFalse  Example program:  # Compute and print a histogram of a sequence of integers entered\n# on standard input, one number per line\n\nimport sys\n\nhistogram = {}\n\n# Iterate over each line in the standard input\nfor line in sys.stdin:\n    # Parse the next input as an integer\n    next_integer = int(line)\n    # Update the histogram accordingly\n    if next_integer in histogram:\n        # We've seen this integer before\n        histogram[next_integer] += 1\n    else:\n        # First occurrence of this integer in the input\n        histogram[next_integer] = 1\n\n# Print each key: value pair in the histogram in ascending\n# sorted order of keys\nfor key in sorted(histogram):\n    print(\"{} {}\".format(key, histogram[key]))s  Example usage of the above program from the operating system command\nprompt, assuming the program is saved in a file called  histo.py :  python histo.py  User types in a sequence of integers to the program, one per line, and\npresses control-d to terminate the input:  3\n43\n12\n19\n3\n12\n12\n43  Program prints its output:  3 2\n12 3\n19 1\n43 2",
            "title": "Dictionaries"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#tuples",
            "text": "Represent  immutable  ordered sequences of values.    Very much like lists except they cannot be modified once created.    Type:  tuple    Literals are written in between parentheses:  (1, 2, 3)    The can be used as keys in dictionaries (unlike lists).",
            "title": "Tuples"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#loops",
            "text": "",
            "title": "Loops"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#while-loops",
            "text": "Iterate until condition is  False    Syntax:    while expression:\n    statement_block   The value of the boolean expression is tested. If it evaluates to\n     True  then the statement block is executed once, before repeating\n    the loop. If it evaluates to False then the program continues\n    execution immediately after the loop.   Example:  def factorial(n):\n    result = 1\n    while n > 0:\n        result *= n\n        n -= 1\n    return result",
            "title": "While loops"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#for-loops",
            "text": "Iterate over each item in a collection (e.g. list, string, tuple,\n    dictionary, file).    Syntax:    for variable in expression:\n    statement_block    Each item from the iterator expression is selected and assigned to\n    the variable, then the statement block is executed. The loop ends\n    when every item in the iterator has been visited.    The order of items visited in the iterator depends on the type of\n    the iterator. Lists, strings and tuples proceed in a left-to-right\n    fashion. Files proceed one line at a time. Dictionaries proceed in\n    an arbitrary order.    The  range()  function is useful for generating iterators of numbers\n    within a range. Note that the lower bound is inclusive and the\n    upper bound is exclusive.    Example:  def factorial(n):\n    result = 1\n    for item in range(n + 1):\n        result *= item\n    return result",
            "title": "For loops"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#break-and-continue",
            "text": "Both types of loops support the  break  and  continue  keywords.    break  terminates the loop immediately.    continue  jumps immediately back to the start of the loop.    They can sometimes simplify the conditional logic of a loop, but\n    should be used sparingly.",
            "title": "Break and continue"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#functions",
            "text": "Allow you to define reusable abstractions. Sometimes called\n     procedures .    Are generally defined at the top level of a module, and can also be\n    nested.    Type:  function    Named functions are bound to a variable name and may have complex\n    bodies.    Anonymous functions are used in-line, and may only have expression\n    bodies.    Named function syntax:    def variable(parameter_list):\n    statement_block   Anonymous function syntax:   lambda parameter_list: expression  Example:  def is_leap_year(year):\n    if year % 4 == 0 and year % 100 != 0:\n        return True\n    else:\n        return year % 400 == 0\n\nfor year in range(2000, 2100 + 1):\n    result = is_leap_year(year)\n    print(\"{} {}\".format(year, result))  Anonymous function example:  >>> squared = lambda x: x ** 2\n>>> squared(2)\n4\n>>> list(map(lambda x: x + 1, [1, 2, 3]))\n[2, 3, 4]",
            "title": "Functions"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#input-and-output",
            "text": "The  print  function is useful for displaying text (and other values\n    converted to text).    In Python 2  print  was a special keyword. In Python 3 it is a\n    function defined in the builtins.    Fancy string formatting can be done with the format method on\n    strings. Older Python code uses the string interpolation operator\n    for the same task  % , but its use is now discouraged.    Files must be opened before than can be manipulated. A file can be\n    opened in different modes: read  \"r\" , write  \"w\" , read-write  \"r+\" ,\n    and append  \"a\" . Opening a new file in write or append modes\n    creates a new file. Opening an existing file in write mode\n    overwrites its contents from the start. Opening an existing file\n    in append mode adds new content at the end of the old content.    When you are finished processing a file you should close it as soon\n    as possible. Closing a file releases limited operating system\n    resources, and ensures that any pending buffered writes a flushed\n    to the storage system.    Certain file types have libraries for convenient processing. One\n    example is the CSV (comma separated values) library for processing\n    tabular data. It is very handy for working with spreadsheets.    The command line arguments of a Python program are contained in a\n    list called  sys.argv  (it is a variable exported from the  sys \n    module). For complex program you should consider using a command\n    line argument parsing library such as  argparse .    Example program:  # Count the number of words and lines in a file\n\nimport sys\n\n# Get the input file name from the command line arguments\nfilename = sys.argv[1]\n\n# Open the file\nfile = open(filename)\n\n# Count the number of lines in the file\nnum_lines = 0\nnum_words = 0\n\nfor line in file:\n    num_lines += 1\n    num_words += len(line.split())\n\nfile.close()\n\nprint(\"Number of lines and words in {}: {} {}\" \\\n        .format(filename, num_lines, num_words))",
            "title": "Input and output"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#advanced-topics",
            "text": "",
            "title": "Advanced Topics"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#classes",
            "text": "Classes allow you to define your own types.    Class definitions may define methods for the type.    A class may inherit, and possibly override, some functionality from\n    a superclass.    Syntax:    class variable(superclass_list):\n    body    The name of the class is given by the variable in the definition.\n    The superclass list defines the superclasses of the new class\n    (very often the base type object is used). The body of the class\n    typically defines one or more methods.    Instances of classes are created by calling the class name as if it\n    were a function.    If defined, the special method called  __init__  is used to\n    initialise a newly created instance of a class.    The first parameter to each method is the object upon which the\n    method was called. The convention is to use the variable called\n    self, however any variable name will do. Many object oriented\n    languages make this variable an implicit parameter called this.    Example:  class Vector(object):\n    def __init__(self, x=0, y=0, z=0):\n        self.x = x\n        self.y = y\n        self.z = z\n\n    def magnitude(self):\n        return sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2)\n\n    def normalise(self):\n        magnitude = self.magnitude()\n        if magnitude == 0:\n            # Somehow we have a degenerate vector.\n            return self\n        else:\n            return self / self.magnitude()\n\n    def angle(self, other):\n        dp = self.dot_product(other)\n        return acos(dp / self.magnitude() * other.magnitude())\n\n    def dot_product(self, other):\n        return self.x * other.x + self.y * other.y + self.z * other.z",
            "title": "Classes"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#exceptions",
            "text": "Exceptions allow Python programs to handle erroneous program\n    conditions.    An exception is raised (or thrown) at the point of the error and\n    handled (or caught) at some other place in the program.    Exception handlers have the syntax:    try:\n    statement_block\nexcept exception_type as variable:\n    statement_block\n...    The statement block after  try  is executed. If no exceptions are\n    raised in that block the program continues immediately after the\n    exception handler. If an exception is raised in the block then\n    program control jumps to the innermost closing  except  clause.\n    Except clauses may optionally specify the set of exception types\n    that they can handle. If the raised exception is an instance of\n    the handled type then the body of the except clause is executed,\n    otherwise the next except clause (if any) is tried.    If no matching exception handler is found then the program will\n    terminate with an unhandled exception error. Python will normally\n    print a stack trace at this point for error diagnosis.    You may raise your own exceptions using the  raise  keyword.    Example:  # alternative version of the histogram code from the section on\n# dictionaries\nfor line in sys.stdin:\n    next_integer = int(line)\n    try:\n        histogram[next_integer] += 1\n    except KeyError:\n        histogram[next_integer] = 1",
            "title": "Exceptions"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#modules",
            "text": "A module is a file which contains Python code.    Any Python file you create is automatically a module.    It is considered good programming style to decompose complex\n    programs into multiple modules. Each module should collect\n    together code with similar purpose.    Variables defined at the top level of a module (such as global\n    variables, functions and classes) can be imported into other\n    modules.    Python comes with many standard modules.    The  import  keyword is used to import an entire module.    You may import a subset of things from a module using the  from ... import ...  syntax.    You may import a module with a new name using the  from ... import ... as ...  or  import ... as ...    When a module is first imported in a program, all of its top-level\n    statements are executed from top to bottom. Subsequent imports use\n    a cached version of its definitions, its statements are not\n    re-executed.    A special module called  builtins  is imported into every other module\n    by default, and it is automatically imported at the interactive\n    prompt in the interpreter.    Interactive Example:  >>> import math\n>>> math.sqrt(100)\n10.0\n>>> sqrt(100)\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nNameError: name 'sqrt' is not defined\n>>> from math import sqrt\n>>> sqrt(100)\n10.0\n>>> import math as m\n>>> m.sqrt(100)\n10.0",
            "title": "Modules"
        },
        {
            "location": "/tutorials/python_overview/python_overview/#packages",
            "text": "A package is a collection of modules in a hierarchy.    Packages are the common way to structure Python libraries.    The  Python Package Index (PyPI)  is a big collection of open\n    source packages contributed by the Python community (PyPI contains more than 64 thousand packages at the time of\n    writing).    Package installation tools such as  pip , make it easy to install\n    packages onto your computer.    If you want to make your own Python code easy for others to install\n    and use then you should consider making it a package. You can even\n    upload it to PyPI.    Many people use  virtualenv  to install packages into a local\n    \"sandboxed\" Python environment. This avoids conflicts with the\n    central Python package database on your computer, and allows\n    multiple different versions of packages to be used.",
            "title": "Packages"
        },
        {
            "location": "/tutorials/using_git/Using_Git/",
            "text": "Using Git and Github for revision control\n\n\nWhat is Git?\n\n\nGit is a revision control system. It is designed to help you keep track\nof collections of files which stem from a common source and undergo\nmodifications over time. The files tend to be human generated text. It\nis very good at managing source code repositories, but it can also be\nused to manage other things, such as configuration files and text\ndocuments. It is not, however, a file backup system.\n\n\nGit encourages a distributed style of project development. Each\ncontributor to a project has their own complete repository. Changes are\nshared between repositories by \npushing\n changes to, or \npulling\n\nchanges from other repositories. Collaboration between developers is\ngreatly enhanced by websites such as \ngithub\n,\n\nbitbucket\n\nand \ngitorious\n which\nprovide convenient interfaces to managing multiple repositories.\n\n\nThere are many alternatives to git which each have their pros and cons.\nTwo of the more popular alternatives are:\n\n\n\n\n\n\nSubversion\n is particularly suited to a centralised model of development.\n\n\n\n\n\n\nMercurial\n is very similar to Git, but is sometimes considered more user friendly.\n\n\n\n\n\n\nGetting help\n\n\nThere are lots of resources on the web for learning how to use Git. A\npopular reference is \nPro Git\n, which is freely available online\n(\nhttp://git-scm.com/book\n). Another good\nreference is the book \nVersion Control with Git\n, by Loeliger and\nMcCullough.\n\n\nA simple workflow\n\n\nStep 1, create a github account.\n\n\nCreate a github account (\nhttps://github.com/\n).\nDo this step once only (unless you need multiple accounts).\n\n\nYou get unlimited numbers of (world readable) public repositories for\nfree.\n\n\nPrivate repositories (that can be shared with selected users) cost money\n(see \nhttps://github.com/plans\n), but \ndiscounts are available for academics\n.\n\n\nStep 2, sign into github and create a repository.\n\n\nSign in to your github account and create a new repository. Do this once\nfor every new project you have.\n\n\n\n\nYou will need to provide some information:\n\n\n\n\n\n\nthe repository name\n\n\n\n\n\n\na description of the repository\n\n\n\n\n\n\nchoose whether it is public (free) or private (costs money)\n\n\n\n\n\n\nwhether to initialise with a dummy README file (it is useful)\n\n\n\n\n\n\nwhether to provide an initial .gitignore file (probably leave this\n    > out in the beginning)\n\n\n\n\n\n\n\n\nStep 3, clone your repository to your local computer.\n\n\nClone your new repository from github to your local computer.\n\n\nEach repository on github is identified by a URL, which will look like\nthe one below:\n\n\n\n\nRun the command below on your development machine in the directory where\nyou want to keep the repository (of course you should use the actual URL\nof your own repository, not the one in the example).\n\n\n$ git clone https://github.com/bjpop/test.git\nCloning into 'test'...\nremote: Counting objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\n\n\n\n\nThis will create a directory with the same name as your repository (in this example it is called \ntest\n).\n\n\nIf you change into that directory and list its contents you will see a\n\n.git\n subdirectory, which is where Git keeps all the data for your\nrepository. You will also see working copies of the files in the\nproject. In this example the only such file is \nREADME.md\n which was\ncreated automatically by github when the repository was first created.\n(The .md extension on the file suggests that it uses the \nMarkdown\n\nsyntax, see \nhttps://help.github.com/articles/github-flavored-markdown\n).\n\n\n$ cd test\n$ ls -a\n. .. .git README.md\n$ ls .git\nbranches config description HEAD hooks index info logs objects packed-refs refs\n\n\n\n\nStep 4, commit a file to the repository.\n\n\nCreate a new file in the repository on your local computer and commit it\nto your local repository.\n\n\nHow you create the file is immaterial. You could copy it from somewhere\nelse, create it in a text editor. In this case we\u2019ll make a little\npython program:\n\n\n$ echo 'print(\"hello world\")' > hello.py\n\n\n\n\nTest that your new file is satisfactory, in this case we test our code:\n\n\n$ python hello.py\nhello world\n\n\n\n\nCheck the status of your repository:\n\n\n$ git status\n# On branch master\n# Untracked files:\n# (use \"git add <file>...\" to include in what will be committed)\n#\n# hello.py\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\n\nNotice that git tells you that the new file \nhello.py\n is not tracked\n(not in the repository).\n\n\nWhen you are happy with your file, you can stage it (this is not a\ncommit), but it will cause the file to be tracked:\n\n\n$ git add hello.py\n\n\n\n\nNote that git uses a two-stage process for committing changes. The first\nstage is to \"stage\" your changes. Staged changes appear in the\nrepository index, but are not committed. You can stage many changes\ntogether, and even amend or undo previously staged (but not committed)\nchanges. The second stage is to commit the current staged changes to the\nrepository. Committing causes the changes to be reflected in the state\nof the repository.\n\n\nRe-check the status of your repository:\n\n\n$ git status\n# On branch master\n# Changes to be committed:\n# (use \"git reset HEAD <file>...\" to unstage)\n#\n# new file: hello.py\n#\n\n\n\n\nNow we can see that the changes to \nhello.py\n have been staged and are\nready to be committed. Notice that \nhello.py\n is no longer untracked.\n\n\nCommit your changes with a commit message:\n\n\n$ git commit -m \"A little greeting program\"\n[master b1cce11] A little greeting program\n1 files changed, 1 insertions(+), 0 deletions(-)\ncreate mode 100644 hello.py\n\n\n\n\nRe-check the status of your repository:\n\n\n$ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)\n\n\n\n\nNow we see that there a no uncommitted changes in the repository,\nhowever git tells us that our local repository is one commit ahead of\nthe github version (which it calls \norigin/master\n).\n\n\nStep 5, push your changes to github.\n\n\nPush the commit in your local repository to github (thus synchronising them).\n\n\n$ git push origin\nUsername for 'https://github.com': <type your github username>\nPassword for 'https://<your github username>@github.com': \nCounting objects: 4, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 305 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n71a771a..b1cce11 master -> master\n\n\n\n\nNow if you look at your repository on github you should see the file\n\nhello.py\n has been uploaded, along with its commit time and commit\nmessage.\n\n\n\n\nYou can inspect the contents of the file on github by clicking on its\nname:\n\n\n\n\nStep 6, create a branch in your local repository.\n\n\nYou can ask git to tell you about the names of the current branches:\n\n\n$ git branch\n* master\n\n\n\n\nBy default your repository starts with a branch called master. The\nasterisk next to the branch name tells you which is the current branch\n(at the moment there is only one branch).\n\n\n$ git branch documentation\n$ git branch\ndocumentation\n* master\n\n\n\n\nThe first command above creates a new branch called \ndocumentation\n. The\nsecond command shows us that the new branch has been created, but the\ncurrent branch is still \nmaster\n.\n\n\nTo switch to another branch you must check it out:\n\n\n$ git checkout documentation\nSwitched to branch 'documentation'\n$ git branch\n* documentation\nmaster\n\n\n\n\nLet\u2019s add a change to our existing \nhello.py\n file:\n\n\n$ echo '#this is a comment' >> hello.py\n\n\n\n\nCheck the status of the repository (now in the documentation branch):\n\n\n$ git status\n# On branch documentation\n# Changes not staged for commit:\n# (use \"git add <file>...\" to update what will be committed)\n# (use \"git checkout -- <file>...\" to discard changes in working\n directory)\n#\n# modified: hello.py\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\n\n\nStage the new changes and commit them, and check the status again:\n\n\n$ git add hello.py \n$ git commit -m \"Added a comment\"\n[documentation 9bbe430] Added a comment\n1 files changed, 1 insertions(+), 0 deletions(-)\n$ git status\n# On branch documentation\nnothing to commit (working directory clean)\n\n\n\n\nNow we can push the new \u201cdocumentation\u201d branch to github:\n\n\n$ git push origin documentation\nUsername for 'https://github.com': <your github username>\nPassword for 'https://<your github username>@github.com': \nCounting objects: 5, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 314 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n* [new branch] documentation -> documentation\n\n\n\n\nOn github you should be able to see the new branch:\n\n\n\n\nStep 7, merge the changes back into the master\n\n\nbranch.\n\n\nTo go back to the master branch you must check it out:\n\n\n$ git checkout master\nSwitched to branch 'master'\n\n\n\n\nYou can confirm that the master branch does not yet have the changes\nmade in the documentation branch:\n\n\n$ cat hello.py \nprint(\"hello world\")\n\n\n\n\nNotice that the comment is missing.\n\n\nYou can pull the changes in the documentation branch back into the\nmaster branch with the merge command:\n\n\n$ git merge documentation\nUpdating b1cce11..9bbe430\nFast-forward\nhello.py | 1 +\n1 files changed, 1 insertions(+), 0 deletions(-)\n\n\n\n\nIn this case the merge was easy because there were no conflicts between\nmaster and documentation. In this case git automatically updates the\ntracked files in the current branch.\n\n\nWe can test that the changes have taken place by looking at the contents\nof hello.py:\n\n\n$ cat hello.py \nprint(\"hello world\")\n#this is a comment\n\n\n\n\nCheck the status of the master branch:\n\n\n$ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)\n\n\n\n\nPush the changes in the master branch back to github:\n\n\n$ git push origin master\nUsername for 'https://github.com': bjpop\nPassword for 'https://bjpop@github.com': \nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\nb1cce11..9bbe430 master -> master\n\n\n\n\nAgain you can verify on github that the changes have taken place.\n\n\nTo get an idea of the history of a project you can ask for a log of the\ncommit messages:\n\n\n$ git log\ncommit 9bbe430f6e8b70187927b4a70a8402f71b17b426\nAuthor: Bernie <florbitous@gmail.com>\nDate: Fri Mar 15 12:30:39 2013 +1100\nAdded a comment\ncommit b1cce115fb40a9b11917db7eb73c8295e276bb09\nAuthor: Bernie <florbitous@gmail.com>\nDate: Fri Mar 15 12:08:01 2013 +1100\nA little greeting program\ncommit 71a771a86b8116c3f93c99db5416bfa371a6f772\nAuthor: Bernie Pope <florbitous@gmail.com>\nDate: Thu Mar 14 17:29:02 2013 -0700",
            "title": "Using Git and Github for revision control"
        },
        {
            "location": "/tutorials/using_git/Using_Git/#using-git-and-github-for-revision-control",
            "text": "",
            "title": "Using Git and Github for revision control"
        },
        {
            "location": "/tutorials/using_git/Using_Git/#what-is-git",
            "text": "Git is a revision control system. It is designed to help you keep track\nof collections of files which stem from a common source and undergo\nmodifications over time. The files tend to be human generated text. It\nis very good at managing source code repositories, but it can also be\nused to manage other things, such as configuration files and text\ndocuments. It is not, however, a file backup system.  Git encourages a distributed style of project development. Each\ncontributor to a project has their own complete repository. Changes are\nshared between repositories by  pushing  changes to, or  pulling \nchanges from other repositories. Collaboration between developers is\ngreatly enhanced by websites such as  github , bitbucket \nand  gitorious  which\nprovide convenient interfaces to managing multiple repositories.  There are many alternatives to git which each have their pros and cons.\nTwo of the more popular alternatives are:    Subversion  is particularly suited to a centralised model of development.    Mercurial  is very similar to Git, but is sometimes considered more user friendly.",
            "title": "What is Git?"
        },
        {
            "location": "/tutorials/using_git/Using_Git/#getting-help",
            "text": "There are lots of resources on the web for learning how to use Git. A\npopular reference is  Pro Git , which is freely available online\n( http://git-scm.com/book ). Another good\nreference is the book  Version Control with Git , by Loeliger and\nMcCullough.",
            "title": "Getting help"
        },
        {
            "location": "/tutorials/using_git/Using_Git/#a-simple-workflow",
            "text": "",
            "title": "A simple workflow"
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-1-create-a-github-account",
            "text": "Create a github account ( https://github.com/ ).\nDo this step once only (unless you need multiple accounts).  You get unlimited numbers of (world readable) public repositories for\nfree.  Private repositories (that can be shared with selected users) cost money\n(see  https://github.com/plans ), but  discounts are available for academics .",
            "title": "Step 1, create a github account."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-2-sign-into-github-and-create-a-repository",
            "text": "Sign in to your github account and create a new repository. Do this once\nfor every new project you have.   You will need to provide some information:    the repository name    a description of the repository    choose whether it is public (free) or private (costs money)    whether to initialise with a dummy README file (it is useful)    whether to provide an initial .gitignore file (probably leave this\n    > out in the beginning)",
            "title": "Step 2, sign into github and create a repository."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-3-clone-your-repository-to-your-local-computer",
            "text": "Clone your new repository from github to your local computer.  Each repository on github is identified by a URL, which will look like\nthe one below:   Run the command below on your development machine in the directory where\nyou want to keep the repository (of course you should use the actual URL\nof your own repository, not the one in the example).  $ git clone https://github.com/bjpop/test.git\nCloning into 'test'...\nremote: Counting objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.  This will create a directory with the same name as your repository (in this example it is called  test ).  If you change into that directory and list its contents you will see a .git  subdirectory, which is where Git keeps all the data for your\nrepository. You will also see working copies of the files in the\nproject. In this example the only such file is  README.md  which was\ncreated automatically by github when the repository was first created.\n(The .md extension on the file suggests that it uses the  Markdown \nsyntax, see  https://help.github.com/articles/github-flavored-markdown ).  $ cd test\n$ ls -a\n. .. .git README.md\n$ ls .git\nbranches config description HEAD hooks index info logs objects packed-refs refs",
            "title": "Step 3, clone your repository to your local computer."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-4-commit-a-file-to-the-repository",
            "text": "Create a new file in the repository on your local computer and commit it\nto your local repository.  How you create the file is immaterial. You could copy it from somewhere\nelse, create it in a text editor. In this case we\u2019ll make a little\npython program:  $ echo 'print(\"hello world\")' > hello.py  Test that your new file is satisfactory, in this case we test our code:  $ python hello.py\nhello world  Check the status of your repository:  $ git status\n# On branch master\n# Untracked files:\n# (use \"git add <file>...\" to include in what will be committed)\n#\n# hello.py\nnothing added to commit but untracked files present (use \"git add\" to track)  Notice that git tells you that the new file  hello.py  is not tracked\n(not in the repository).  When you are happy with your file, you can stage it (this is not a\ncommit), but it will cause the file to be tracked:  $ git add hello.py  Note that git uses a two-stage process for committing changes. The first\nstage is to \"stage\" your changes. Staged changes appear in the\nrepository index, but are not committed. You can stage many changes\ntogether, and even amend or undo previously staged (but not committed)\nchanges. The second stage is to commit the current staged changes to the\nrepository. Committing causes the changes to be reflected in the state\nof the repository.  Re-check the status of your repository:  $ git status\n# On branch master\n# Changes to be committed:\n# (use \"git reset HEAD <file>...\" to unstage)\n#\n# new file: hello.py\n#  Now we can see that the changes to  hello.py  have been staged and are\nready to be committed. Notice that  hello.py  is no longer untracked.  Commit your changes with a commit message:  $ git commit -m \"A little greeting program\"\n[master b1cce11] A little greeting program\n1 files changed, 1 insertions(+), 0 deletions(-)\ncreate mode 100644 hello.py  Re-check the status of your repository:  $ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)  Now we see that there a no uncommitted changes in the repository,\nhowever git tells us that our local repository is one commit ahead of\nthe github version (which it calls  origin/master ).",
            "title": "Step 4, commit a file to the repository."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-5-push-your-changes-to-github",
            "text": "Push the commit in your local repository to github (thus synchronising them).  $ git push origin\nUsername for 'https://github.com': <type your github username>\nPassword for 'https://<your github username>@github.com': \nCounting objects: 4, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 305 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n71a771a..b1cce11 master -> master  Now if you look at your repository on github you should see the file hello.py  has been uploaded, along with its commit time and commit\nmessage.   You can inspect the contents of the file on github by clicking on its\nname:",
            "title": "Step 5, push your changes to github."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-6-create-a-branch-in-your-local-repository",
            "text": "You can ask git to tell you about the names of the current branches:  $ git branch\n* master  By default your repository starts with a branch called master. The\nasterisk next to the branch name tells you which is the current branch\n(at the moment there is only one branch).  $ git branch documentation\n$ git branch\ndocumentation\n* master  The first command above creates a new branch called  documentation . The\nsecond command shows us that the new branch has been created, but the\ncurrent branch is still  master .  To switch to another branch you must check it out:  $ git checkout documentation\nSwitched to branch 'documentation'\n$ git branch\n* documentation\nmaster  Let\u2019s add a change to our existing  hello.py  file:  $ echo '#this is a comment' >> hello.py  Check the status of the repository (now in the documentation branch):  $ git status\n# On branch documentation\n# Changes not staged for commit:\n# (use \"git add <file>...\" to update what will be committed)\n# (use \"git checkout -- <file>...\" to discard changes in working\n directory)\n#\n# modified: hello.py\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")  Stage the new changes and commit them, and check the status again:  $ git add hello.py \n$ git commit -m \"Added a comment\"\n[documentation 9bbe430] Added a comment\n1 files changed, 1 insertions(+), 0 deletions(-)\n$ git status\n# On branch documentation\nnothing to commit (working directory clean)  Now we can push the new \u201cdocumentation\u201d branch to github:  $ git push origin documentation\nUsername for 'https://github.com': <your github username>\nPassword for 'https://<your github username>@github.com': \nCounting objects: 5, done.\nDelta compression using up to 16 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 314 bytes, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\n* [new branch] documentation -> documentation  On github you should be able to see the new branch:",
            "title": "Step 6, create a branch in your local repository."
        },
        {
            "location": "/tutorials/using_git/Using_Git/#step-7-merge-the-changes-back-into-the-master",
            "text": "branch.  To go back to the master branch you must check it out:  $ git checkout master\nSwitched to branch 'master'  You can confirm that the master branch does not yet have the changes\nmade in the documentation branch:  $ cat hello.py \nprint(\"hello world\")  Notice that the comment is missing.  You can pull the changes in the documentation branch back into the\nmaster branch with the merge command:  $ git merge documentation\nUpdating b1cce11..9bbe430\nFast-forward\nhello.py | 1 +\n1 files changed, 1 insertions(+), 0 deletions(-)  In this case the merge was easy because there were no conflicts between\nmaster and documentation. In this case git automatically updates the\ntracked files in the current branch.  We can test that the changes have taken place by looking at the contents\nof hello.py:  $ cat hello.py \nprint(\"hello world\")\n#this is a comment  Check the status of the master branch:  $ git status\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#\nnothing to commit (working directory clean)  Push the changes in the master branch back to github:  $ git push origin master\nUsername for 'https://github.com': bjpop\nPassword for 'https://bjpop@github.com': \nTotal 0 (delta 0), reused 0 (delta 0)\nTo https://github.com/bjpop/test.git\nb1cce11..9bbe430 master -> master  Again you can verify on github that the changes have taken place.  To get an idea of the history of a project you can ask for a log of the\ncommit messages:  $ git log\ncommit 9bbe430f6e8b70187927b4a70a8402f71b17b426\nAuthor: Bernie <florbitous@gmail.com>\nDate: Fri Mar 15 12:30:39 2013 +1100\nAdded a comment\ncommit b1cce115fb40a9b11917db7eb73c8295e276bb09\nAuthor: Bernie <florbitous@gmail.com>\nDate: Fri Mar 15 12:08:01 2013 +1100\nA little greeting program\ncommit 71a771a86b8116c3f93c99db5416bfa371a6f772\nAuthor: Bernie Pope <florbitous@gmail.com>\nDate: Thu Mar 14 17:29:02 2013 -0700",
            "title": "Step 7, merge the changes back into the master"
        },
        {
            "location": "/tutorials/hpc/robinson-hpc-link/",
            "text": "High Performance Computing\n\n\nPlease see the link \nhere\n.",
            "title": "Introduction to HPC"
        },
        {
            "location": "/tutorials/hpc/robinson-hpc-link/#high-performance-computing",
            "text": "Please see the link  here .",
            "title": "High Performance Computing"
        },
        {
            "location": "/tutorials/unix/robinson-unix-link/",
            "text": "Introduction to Unix\n\n\nPlease see the link \nhere\n.",
            "title": "Introduction to Unix"
        },
        {
            "location": "/tutorials/unix/robinson-unix-link/#introduction-to-unix",
            "text": "Please see the link  here .",
            "title": "Introduction to Unix"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/",
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\nLaunching a Personal GVL Server on the NeCTAR Research Cloud\n\n\n\n\nTutorial Overview\n\n\nThis document guides you to launch your own GVL Analysis platform, with Galaxy,\nusing your default NeCTAR allocation.\n\n\nThe tutorial will go over:\n\n\n\n\nAccessing the NeCTAR dashboard using Australian Access Federation (AAF)\n   credentials\n\n\nGetting your NeCTAR Research Cloud credentials\n\n\nLaunching the GVL image\n\n\nAccessing your GVL instance\n\n\nGVL services\n\n\nShutting your machine down\n\n\n\n\n\n\nBackground\n\n\nWhat is the GVL?\n\n\nThe Genomics Virtual Laboratory (GVL) is a research platform that can be\ndeployed on the cloud.\n\n\n\n\nA private GVL server is a virtual machine running on the cloud and contains a\npre-installed suite of tools for performing bioinformatics analyses. It differs\nfrom public GVL servers (such as the\n\nGalaxy Tutorial Server\n,\n\nGalaxy Melbourne\n, and\n\nGalaxy Queensland\n) by providing full\nadministrative access to the server, as well as the full suite of GVL services,\nwhereas public GVL servers provide restricted access for security reasons.\nFor example, public GVL servers do not provide access to the Ubuntu desktop,\nthe Linux command line or JupyterHub at present.\n\n\nAccessing the GVL server is completely free on the Australian NeCTAR Research\nCloud, provided that you have a account with NeCTAR with allocated resources.\n\n\nWhat is NeCTAR?\n\n\nThe \nNational eResearch Collaboration Tools and Resources project\n\n(NeCTAR) is an Australian programme that provides computing infrastructure\nand services to Australian researchers. The NeCTAR Cloud allows us to\ndeploy virtual machines as a platform for research.\n\n\nWhile it is possible to launch the GVL on \nAmazon\n,\nyou may have to pay Amazon usage charges (the GVL software itself is free).\n\n\n\n\nSection 1: Access the NeCTAR dashboard\n\n\n\n\n\n\nLogin into the NeCTAR dashboard at\n    \ndashboard.rc.nectar.org.au\n.\n\n    Only members of the Australian Access Federation (AAF) can access Australian\n    Research Cloud resources. Most Australian universities are members of the\n    AAF, however, if you belong to an institution that is not a member of AAF,\n    \nGVL Help\n may be able to provide you\n    with credentials. You also have the option of using a commercial cloud\n    provider such as \nAWS\n to host your server.\n\n\n\n\n\n\nChoose your organisation from the list and login using your credentials.\n    If you are from the University of Melbourne, choose 'The University of\n    Melbourne' and not 'The University of Melbourne (with ECP)'.\n\n\n\n\n\n\nLogin with your institutional username and password. If this is the first\n    time you have accessed the Australian Research Cloud, you must agree to some\n    terms and conditions.\n\n\nWhen you log in for the first time, you are automatically allocated a trial\nproject which lasts for 3 months. This trial project allows you to launch a\nmedium instance (with 2 cores) which is sufficient for launching a GVL\ninstance.\n\n\n\n\nIf you have projects which require more compute resources, you can apply\nfor more allocation \nhere\n.\n\n\n\n\n\n\n\n\nSection 2: Get your cloud credentials\n\n\nLaunching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher\nto create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process.\n\n\n\n\n\n\n\n\nFrom the NeCTAR dashboard, on the left sidebar, navigate to\n\n    Project > Compute > Access & Security\n\n\n\n\n\n\nClick on the top \n'API Access'\n tab.\n\n\n\n\n\n\nClick on the \n'Download OpenStack RC File'\n button on the top right.\n    A file containing your credentials will be saved to your downloads folder.\n    This file will be needed later in the launch process.\n\n\n\n\n\n\nNext you must obtain your OpenStack password and record it securely for future use.\n    If you have ever done this step before, you should reuse your previously saved\n    password.\n\n\nTo obtain your OpenStack password, click on your username on the top right hand corner\nand go to settings as shown below.\n\n\n\n\nClick on the reset password link.\n\n\n\n\n\n\n\n\nOnce reset, your password will be displayed. Record this securely for all future\n    GVL launches.\n\n\n\n\n\n\n\n\n\n\nSection 3: Launch your personal GVL instance\n\n\n\n\n\n\nIn a new browser tab, go to \nlaunch.usegalaxy.org\n\n\n\n\n\n\nYou will see the screen below. Select the first option from the list - \"Genomics Virtual Lab\".\n\n\n \n\n\n\n\n\n\nYou will be asked to login with your preferred social network account.\n\n\n \n\n\n\n\n\n\nOnce logged in, perform the following steps.\n\n\n\n\n\n\n\n\n\n\nSelect NeCTAR for the question \"On which cloud would you like to launch your appliance\"\n\n\n\n\n\n\n\n\n\n\nClick \"load credentials from file\" and provide file you downloaded in Section 2.3\n\n\n\n\n\n\n\n\n\n\nProvide the OpenStack password you obtained in Section 2.5.\n\n\n\n\n\n\n\n\n\n\nClick \"Test and Use these Credentials\". The Next button will now be activated.\n\n\n\n\n\n\n\n\n\n\n\n\nClick the next button, and provide the following options.\n\n\n \n\n\n\n\nPassword:\n Choose a strong password and remember it. This is the\n  password you will use later to log into your instance.\n\n\n\n\n\n\n\n\nOptional advanced options\n\n    Toggle the 'Advanced cloudlaunch options' option to see more options. For\n    this tutorial, it is not necessary to modify any of the advanced options.\n\n\n\n\nDeployment name:\n You can override the name with a name of your choice.\n  It is recommended you choose a unique name if you launch multiple instances.\n\n\nInstance type:\n Keep the default: Medium (2 vcpu / 8GB RAM)\n\n\nRoot Volume Storage:\n Keep the default: Instance storage\n\n\nPlacement Zone:\n You can also choose the region of where your server is\n  hosted. If you are doing lots of data transfer, it may be beneficial to\n  pick a location close to your physical location. More information about\n  zones can be found \nhere\n.\n\n\nKey pair:\n Key pairs are SSH credentials that can be used to access\n  your instance. You can create and import key pairs in the NeCTAR dashboard\n  by navigating to Project > Compute > Access & Security > Key Pairs and\n  creating or importing a key pair.\n\n\n\n\n\n\n\n\nClick \n'Launch'\n to launch a GVL.\n\n    The launch process takes 2-5 minutes to start the machine and another 5\n    minutes to start and configure Galaxy.\n\n\n\n\nIf an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try\nselecting a different availability zone under the advanced cloudlaunch options -> \nPlacement Zone\n field.\n\n\n\n\n\n\n\n\nSection 4: Access your GVL instance\n\n\n\n\n\n\nOnce your instance has finished launching, click on the Access address\n    to access your GVL dashboard.\n\n\n\n\nIf you accidentally closed the launch page, you can find your access address at any time\nby logging back into \nlaunch.usegalaxy.org\n and navigating\nto the \"My Appliances\" section through the menu bar.\n\n\n\n\n\n\nExplore the GVL dashboard.\n    Have a read through of the services provided by the GVL.\n\n\n\n\n\n\n\n\n\n\nSection 5: GVL services\n\n\nListed below are short descriptions of the services the GVL provides.\n\n\nGalaxy\n\n\nGalaxy\n is a web-based platform for computational\nbiomedical research. The GVL has a number of Galaxy tutorials available\n\nhere\n.\n\n\n\n\nTo begin using Galaxy, register as a user by navigating to User > Register\non the top Galaxy bar.\n\n\nCloudMan\n\n\nCloudMan is a cloud manager that manages services on your GVL instance. Use\nCloudman to start and manage your Galaxy service and to add additional nodes\nto your compute cluster (if you have enough resources).\n\n\nYou can log into CloudMan by using the username 'ubuntu' and your cluster\npassword.\n\n\n\n\nYou can also shut down your instance (permanently) with CloudMan.\n\n\nLubuntu Desktop\n\n\nLubuntu is a lightweight desktop environment through which you can run desktop\napplications on your virtual machine through your web browser. You can also\naccess the GVL command line utilities through the desktop.\n\n\nYou can log into Lubuntu Desktop using the username 'ubuntu' and your cluster\npassword.\n\n\n\n\nSSH\n\n\nSecure Shell (SSH) is a network protocol that allows us to connect to a remotely\nmachine. You can login to your virtual machine remotely through an SSH client.\n\n\nIf you are using Windows, you will need to download an SSH client such as\n\nPuTTY\n. If you are using\nOSX, open up a Terminal window.\n\n\nIf you are unfamiliar with the command line and UNIX, many\n\ntutorials\n\n\non UNIX\n\n\ncan be\n\n\nfound online\n.\n\n\nYou can ssh into your machine using the either the username 'ubuntu' or the\nusername 'researcher' and using your cluster password. It is recommended to use\nthe researcher account when you are doing your computational research and use\nthe ubuntu account when you need administrative powers (such as installing\nsoftware).\n\n\n\n\nJupyterHub\n\n\nJupyterHub\n is a web-based interactive computational\nenvironment where you can combine code execution, text, mathematics, plots and\nrich media into a single document. Currently, JupyterHub can connect to Python2\nand Python3 kernals.\n\n\nIf you are unfamiliar with Python, there are many\n\ntutorials\n\n\navailable\n\n\nonline\n.\n\n\nYou can log into JupyterHub with the username 'researcher' and your cluster\npassword.\n\n\nYou may need to install Python packages you intend to use via the command line\nbeforehand.\n\n\n\n\nRStudio\n\n\nRStudio Server gives browser-based access to RStudio, the popular programming\nand analysis environment for the R programming language. You can find out more\nabout RStudio \nhere\n, and the R programming\nlanguage \nhere\n.\n\n\nYou can log into RStudio with the username 'researcher' and your cluster\npassword.\n\n\n\n\nPublic HTML\n\n\nThis is a shared web-accessible folder. Any files you place in the directory\n\n/home/researcher/public_html\n will be publicly accessible.\n\n\nPacBio SMRT Portal\n\n\nPacBio's SMRT Portal\n is an open source software suite for the analysis of single molecule, real-time sequencing data.\n\n\nBefore you use SMRT Portal, you need to firstly install it through the Admin console. \nPlease note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package.\n\n\nTo install SMRT Portal from the GVL dashboard:\n\n\n\n\n\n\nClick on 'Admin' in the top navigation bar.\n\n\n\n\n\n\nLog in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed.\n\n\n\n\n\n\nScroll down to 'SMRT Analysis', and click 'install'.\n\n\n\n\n\n\nThe install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed.\n\n\n\n\n\n\nWhen launching SMRT Portal for the first time, you will need to register yourself as a new user.\n\n\n\n\nSection 6: Shutting your machine down\n\n\nThere are two ways to terminate your instance. Terminating your instance is\npermanent and all data will be deleted (unless you have persistent\n\nvolume storage\n which you will\nneed to apply for).\n\n\n\n\n\n\nVia Cloudman\n\n    Log into CloudMan by using the username 'ubuntu' and your cluster password.\n    Click the \nShut down...\n button under Cluster Controls.\n\n\n\n\n\n\nVia the NeCTAR dashboard\n\n    Navigate to the \nInstances\n\n    page by navigating to Project > Compute > Instances on the left panel. Find\n    the instance you want to terminate, and on the right-most column (Actions),\n    click on the arrow button, and select \nTerminate Instance\n.",
            "title": "Launching a Personal GVL Server"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#launching-a-personal-gvl-server-on-the-nectar-research-cloud",
            "text": "",
            "title": "Launching a Personal GVL Server on the NeCTAR Research Cloud"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#tutorial-overview",
            "text": "This document guides you to launch your own GVL Analysis platform, with Galaxy,\nusing your default NeCTAR allocation.  The tutorial will go over:   Accessing the NeCTAR dashboard using Australian Access Federation (AAF)\n   credentials  Getting your NeCTAR Research Cloud credentials  Launching the GVL image  Accessing your GVL instance  GVL services  Shutting your machine down",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#background",
            "text": "",
            "title": "Background"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#what-is-the-gvl",
            "text": "The Genomics Virtual Laboratory (GVL) is a research platform that can be\ndeployed on the cloud.   A private GVL server is a virtual machine running on the cloud and contains a\npre-installed suite of tools for performing bioinformatics analyses. It differs\nfrom public GVL servers (such as the Galaxy Tutorial Server , Galaxy Melbourne , and Galaxy Queensland ) by providing full\nadministrative access to the server, as well as the full suite of GVL services,\nwhereas public GVL servers provide restricted access for security reasons.\nFor example, public GVL servers do not provide access to the Ubuntu desktop,\nthe Linux command line or JupyterHub at present.  Accessing the GVL server is completely free on the Australian NeCTAR Research\nCloud, provided that you have a account with NeCTAR with allocated resources.",
            "title": "What is the GVL?"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#what-is-nectar",
            "text": "The  National eResearch Collaboration Tools and Resources project \n(NeCTAR) is an Australian programme that provides computing infrastructure\nand services to Australian researchers. The NeCTAR Cloud allows us to\ndeploy virtual machines as a platform for research.  While it is possible to launch the GVL on  Amazon ,\nyou may have to pay Amazon usage charges (the GVL software itself is free).",
            "title": "What is NeCTAR?"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-1-access-the-nectar-dashboard",
            "text": "Login into the NeCTAR dashboard at\n     dashboard.rc.nectar.org.au . \n    Only members of the Australian Access Federation (AAF) can access Australian\n    Research Cloud resources. Most Australian universities are members of the\n    AAF, however, if you belong to an institution that is not a member of AAF,\n     GVL Help  may be able to provide you\n    with credentials. You also have the option of using a commercial cloud\n    provider such as  AWS  to host your server.    Choose your organisation from the list and login using your credentials.\n    If you are from the University of Melbourne, choose 'The University of\n    Melbourne' and not 'The University of Melbourne (with ECP)'.    Login with your institutional username and password. If this is the first\n    time you have accessed the Australian Research Cloud, you must agree to some\n    terms and conditions.  When you log in for the first time, you are automatically allocated a trial\nproject which lasts for 3 months. This trial project allows you to launch a\nmedium instance (with 2 cores) which is sufficient for launching a GVL\ninstance.   If you have projects which require more compute resources, you can apply\nfor more allocation  here .",
            "title": "Section 1: Access the NeCTAR dashboard"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-2-get-your-cloud-credentials",
            "text": "Launching a GVL instance requires OpenStack API credentials from NeCTAR. These credentials allows the GVL launcher\nto create a new GVL Virtual Machine on your behalf. Obtaining these credentials is a 5 step process.     From the NeCTAR dashboard, on the left sidebar, navigate to \n    Project > Compute > Access & Security    Click on the top  'API Access'  tab.    Click on the  'Download OpenStack RC File'  button on the top right.\n    A file containing your credentials will be saved to your downloads folder.\n    This file will be needed later in the launch process.    Next you must obtain your OpenStack password and record it securely for future use.\n    If you have ever done this step before, you should reuse your previously saved\n    password.  To obtain your OpenStack password, click on your username on the top right hand corner\nand go to settings as shown below.   Click on the reset password link.     Once reset, your password will be displayed. Record this securely for all future\n    GVL launches.",
            "title": "Section 2: Get your cloud credentials"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-3-launch-your-personal-gvl-instance",
            "text": "In a new browser tab, go to  launch.usegalaxy.org    You will see the screen below. Select the first option from the list - \"Genomics Virtual Lab\".       You will be asked to login with your preferred social network account.       Once logged in, perform the following steps.      Select NeCTAR for the question \"On which cloud would you like to launch your appliance\"      Click \"load credentials from file\" and provide file you downloaded in Section 2.3      Provide the OpenStack password you obtained in Section 2.5.      Click \"Test and Use these Credentials\". The Next button will now be activated.       Click the next button, and provide the following options.      Password:  Choose a strong password and remember it. This is the\n  password you will use later to log into your instance.     Optional advanced options \n    Toggle the 'Advanced cloudlaunch options' option to see more options. For\n    this tutorial, it is not necessary to modify any of the advanced options.   Deployment name:  You can override the name with a name of your choice.\n  It is recommended you choose a unique name if you launch multiple instances.  Instance type:  Keep the default: Medium (2 vcpu / 8GB RAM)  Root Volume Storage:  Keep the default: Instance storage  Placement Zone:  You can also choose the region of where your server is\n  hosted. If you are doing lots of data transfer, it may be beneficial to\n  pick a location close to your physical location. More information about\n  zones can be found  here .  Key pair:  Key pairs are SSH credentials that can be used to access\n  your instance. You can create and import key pairs in the NeCTAR dashboard\n  by navigating to Project > Compute > Access & Security > Key Pairs and\n  creating or importing a key pair.     Click  'Launch'  to launch a GVL. \n    The launch process takes 2-5 minutes to start the machine and another 5\n    minutes to start and configure Galaxy.   If an error occurs, or the launch does not complete in 10 minutes, navigate back to the launch page and try\nselecting a different availability zone under the advanced cloudlaunch options ->  Placement Zone  field.",
            "title": "Section 3: Launch your personal GVL instance"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-4-access-your-gvl-instance",
            "text": "Once your instance has finished launching, click on the Access address\n    to access your GVL dashboard.   If you accidentally closed the launch page, you can find your access address at any time\nby logging back into  launch.usegalaxy.org  and navigating\nto the \"My Appliances\" section through the menu bar.    Explore the GVL dashboard.\n    Have a read through of the services provided by the GVL.",
            "title": "Section 4: Access your GVL instance"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-5-gvl-services",
            "text": "Listed below are short descriptions of the services the GVL provides.",
            "title": "Section 5: GVL services"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#galaxy",
            "text": "Galaxy  is a web-based platform for computational\nbiomedical research. The GVL has a number of Galaxy tutorials available here .   To begin using Galaxy, register as a user by navigating to User > Register\non the top Galaxy bar.",
            "title": "Galaxy"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#cloudman",
            "text": "CloudMan is a cloud manager that manages services on your GVL instance. Use\nCloudman to start and manage your Galaxy service and to add additional nodes\nto your compute cluster (if you have enough resources).  You can log into CloudMan by using the username 'ubuntu' and your cluster\npassword.   You can also shut down your instance (permanently) with CloudMan.",
            "title": "CloudMan"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#lubuntu-desktop",
            "text": "Lubuntu is a lightweight desktop environment through which you can run desktop\napplications on your virtual machine through your web browser. You can also\naccess the GVL command line utilities through the desktop.  You can log into Lubuntu Desktop using the username 'ubuntu' and your cluster\npassword.",
            "title": "Lubuntu Desktop"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#ssh",
            "text": "Secure Shell (SSH) is a network protocol that allows us to connect to a remotely\nmachine. You can login to your virtual machine remotely through an SSH client.  If you are using Windows, you will need to download an SSH client such as PuTTY . If you are using\nOSX, open up a Terminal window.  If you are unfamiliar with the command line and UNIX, many tutorials  on UNIX  can be  found online .  You can ssh into your machine using the either the username 'ubuntu' or the\nusername 'researcher' and using your cluster password. It is recommended to use\nthe researcher account when you are doing your computational research and use\nthe ubuntu account when you need administrative powers (such as installing\nsoftware).",
            "title": "SSH"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#jupyterhub",
            "text": "JupyterHub  is a web-based interactive computational\nenvironment where you can combine code execution, text, mathematics, plots and\nrich media into a single document. Currently, JupyterHub can connect to Python2\nand Python3 kernals.  If you are unfamiliar with Python, there are many tutorials  available  online .  You can log into JupyterHub with the username 'researcher' and your cluster\npassword.  You may need to install Python packages you intend to use via the command line\nbeforehand.",
            "title": "JupyterHub"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#rstudio",
            "text": "RStudio Server gives browser-based access to RStudio, the popular programming\nand analysis environment for the R programming language. You can find out more\nabout RStudio  here , and the R programming\nlanguage  here .  You can log into RStudio with the username 'researcher' and your cluster\npassword.",
            "title": "RStudio"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#public-html",
            "text": "This is a shared web-accessible folder. Any files you place in the directory /home/researcher/public_html  will be publicly accessible.",
            "title": "Public HTML"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#pacbio-smrt-portal",
            "text": "PacBio's SMRT Portal  is an open source software suite for the analysis of single molecule, real-time sequencing data.  Before you use SMRT Portal, you need to firstly install it through the Admin console.  Please note PacBio recommends the use of a 16 core instance with 64GB of RAM (or higher) for this package.  To install SMRT Portal from the GVL dashboard:    Click on 'Admin' in the top navigation bar.    Log in with the username 'ubuntu' and your cluster password. The screen will now show all the tools available to be installed.    Scroll down to 'SMRT Analysis', and click 'install'.    The install is complete when SMRT Portal is available as a tool on the GVL dashboard, and a green tick is displayed.    When launching SMRT Portal for the first time, you will need to register yourself as a new user.",
            "title": "PacBio SMRT Portal"
        },
        {
            "location": "/tutorials/gvl_launch/gvl_launch/#section-6-shutting-your-machine-down",
            "text": "There are two ways to terminate your instance. Terminating your instance is\npermanent and all data will be deleted (unless you have persistent volume storage  which you will\nneed to apply for).    Via Cloudman \n    Log into CloudMan by using the username 'ubuntu' and your cluster password.\n    Click the  Shut down...  button under Cluster Controls.    Via the NeCTAR dashboard \n    Navigate to the  Instances \n    page by navigating to Project > Compute > Instances on the left panel. Find\n    the instance you want to terminate, and on the right-most column (Actions),\n    click on the arrow button, and select  Terminate Instance .",
            "title": "Section 6: Shutting your machine down"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/",
            "text": ".image-header-gvl {\n    -webkit-column-count: 3;\n    -moz-column-count: 3;\n    column-count: 3;\n\n    -webkit-column-gap: 140px;\n    -moz-column-gap: 140px;\n    column-gap: 140px;\n  }\n\n\n\n\n\n    \n\n    \n\n    \n\n\n\n\n\nIntroduction to Galaxy\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nBackground\n\n\nGalaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc.\n\n\nThere are some introductory slides available \nhere\n.\n\n\nBasically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial.\n\n\n\n\nThis workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics:\n\n\n\n\nLogging in to the server\n\n\nGetting data into galaxy\n\n\nHow to access the tools\n\n\nUsing to use some common tools\n\n\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe able to register on and login to a Galaxy server.\n\n\nBe able to upload data to a Galaxy server from:\n\n\nA file on your local computer\n\n\nA file on a remote datastore with an accessible URL.\n\n\n\n\n\n\nBe able use tools in Galaxy by:\n\n\nAccessing the tool via the tool menu\n\n\nUsing the tool interface to run the particular tool\n\n\nViewing/accessing the tool output.\n\n\n\n\n\n\n\n\n\n\nSection 1: Preparation.\n\n\nThe purpose of this section is to get you to log in to the server.\n\n\n\n\n\n\nOpen your browser. We recommend Firefox or Chrome (please don't use Internet Explorer or Safari). \n\n\n\n\nGo to the \nGalaxy-Mel\n server. \n\n\nAlternatively, you can use a different Galaxy server - a list of available servers is \nhere\n.\n\n\n\n\n\n\n\n\nIf you have previously registered on this server just log in:\n\n\n\n\nOn the top menu select: \nUser -> Login\n\n\nEnter your password\n\n\nClick \nSubmit\n\n\n\n\n\n\n\n\nIf you haven\u2019t registered on this server, you\u2019ll need to now.\n\n\n\n\nOn the top menu select: \nUser -> Register\n\n\nEnter your email, choose a password, repeat it and add a (all lower case) one word name\n\n\nClick \nSubmit\n\n\n\n\n\n\n\n\n\n\nSection 2: Getting data into Galaxy\n\n\nThere are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop.\n\n\n\n\n\n\nStart a new history for this workshop. To do this:\n\n\n\n\nClick on the history menu button (the \n icon) at the top of the Histories panel.\n\n\nSelect \nCreate New\n\n\n\n\n\n\n\n\nIt is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc.\n\n\nWe will tell Galaxy what type of file each one is as we upload it.\n\n\nMethod 1: Upload a file from your own computer\n\n\nWith this method you can get most of the files on your own computer into Galaxy. (there is a size limit)\n\n\n\n\n\n\nDownload the following file to your computer: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz\n\n\n\n\n(To download this file, copy the link into a new browser tab, and press enter. The file should now download.)\n\n\nFrom the Galaxy tool panel, click on \nGet Data -> Upload File\n\n\nClick the \nChoose File\n button\n\n\nFind and select the \nContig_stats.txt.gz\n file you downloaded and click \nOpen\n\n\nSet the \"Type\" (= file format) to \ntabular\n\n\nClick the \nStart\n button\n\n\nOnce the progress bar reaches 100%, click the \nClose\n button\n\n\n\n\n\n\n\n\nThe file will now upload to your current history.\n\n\nMethod 2: Upload a file from a URL\n\n\nIf a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy.\n\n\n\n\n\n\nFrom the tool panel, click on \nGet Data -> Upload File\n\n\n\n\nClick on the \nPaste/Fetch Data\n button\n\n\nCopy and paste the following web address into the URL/Text box: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz\n\n\nSet the file format to \nfastqsanger\n (not fastqcsanger)\n\n\nClick \nStart\n\n\nOnce the progress bar has reached 100%, click \nClose\n\n\n\n\n\n\n\n\nNote that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.\n\n\nMethod 2 (again): Get data from a public URL\n\n\nNow we are going to upload another file from the remote data source.\n\n\nRepeat the above for: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna\n\n\nNote that this file is a \nfasta\n file and not a \nfastqsanger\n file.\n\n\n\n\nReveal detailed instructions\n\n\n\n\nFrom the tool panel, click on \nGet Data -> Upload File\n\n\nClick on the \nPaste/Fetch Data\n button\n\n\nCopy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna\n\n\nSet the file format to \nfasta\n.\n\n\nClick \nStart\n\n\nOnce the progress bar has reached 100%, click \nClose\n\n\n\n\n\n\nThe DNA sequence of \nStaphlococcus aureus MRSA252\n will be loaded into your history as a fasta file.\n\n\nYour history should now look like this.\n\n\n\n\nThe data\n\n\nThough we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one.\n\n\n\n\n\n\nContigs_stats.txt\n\n\n\n\nthis file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. )\n\n\nThe columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbacterial_std_err_1.fastq.gz\n\n\n\n\nThis file contains sequence reads as they would come off an Illumina sequencing machine. They are in \nfastq\n format.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMRSA0252.fna\n\n\n\n\nThis file contains the genome sequence of \nStaphylococcus aureus MRSA252\n. It is in \nfasta\n format.\n\n\n\n\n\n\n\n\n\n\nSection 3: Play with the tools\n\n\nThe purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools.\n\n\nFirstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So:\n\n\n\n\nClick on the \n icon (edit) next to the file in the history called: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq\n\n\nIn the \"Name\" text box, give it a new name. Call it: \nTypical Fastq File\n\n\nClick the \nSave\n button.\n\n\n\n\nRepeat the process for the MRSA252 fasta file. Rename it to \nMRSA252.fna\n\n\nNow that\u2019s better. There was a lot of other functionality hidden behind that edit (\n) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date.\n\n\nOk, back to the tools.\n\n\nExample 1: Histogram and summary statistics\n\n\nThe first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools.\n\n\nClick on the \n icon of the \nContig_stats.txt\n file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this:\n\n\n1. Cut out column 1 and column 6.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -> Cut\n and set the following:\n\n\nSet \"Cut Columns\" to: \nc1,c6\n\n\n\"Delimited by\": \nTab\n\n\n\"Cut from\": \nContig_stats.txt\n\n\nClick \nExecute\n\n\n\n\nExamine the new file by clicking on its \n icon. We now have 2 columns instead of the 18 in the original file.\n\n\n2. Remove the Header lines of the new file.\n\n\n\n\nFrom the tool panel, click on \nText Manipulation -> Remove beginning\n and set the following:\n\n\n\"Remove First\": \n1\n\n\n\"from\": \nCut on data1\n\n\nclick \nExecute\n\n\n\n\nNote the the new file is the same as the previous one without the header line.\n\n\n3. Make a histogram.\n\n\n\n\nFrom the tool panel, click on \nGraph/Display Data -> Histogram\n and set the following:\n\n\n\"Dataset\": \nRemove beginning on Data X\n\n\n\"Numerical column for X axis\": \nc2\n\n\n\"Number of breaks\": \n25\n\n\n\"Plot title\": \nHistogram of Contig Coverage\n\n\n\"Label for X axis\": \nCoverage depth\n\n\nClick \nExecute\n\n\n\n\nClick on the \n icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs.\n\n\n4. Calculate summary statistics for contig coverage depth.\n\n\n\n\nFrom the tool panel, click on \nStatistics -> Summary Statisitics\n and set the following:\n\n\n\"Summary statistics on\": \nRemove beginning on Data X\n\n\n\"Column or expression\": \nc2\n\n\nClick \nExecute\n\n\n\n\nYou\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug \n symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator.\n\n\nExample 2: Convert Fastq to Fasta\n\n\nThis shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data.\n\n\nConverter tool\n\n\n\n\nFrom the tool panel, click on \nConvert Formats -> FASTQ to FASTA\n and set the following:\n\n\n\"FASTQ file to convert\": \nTypical Fastq File\n\n\nClick \nExecute\n\n\n\n\nThis will have created a new Fasta file called FASTQ to FASTA on data 2.\n\n\nExample 3: Find Ribosomal RNA Features in a DNA Sequence\n\n\nThis example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence.\n\n\n1. Find all of the ribosomal RNA's in a sequence\n\n\n\n\nFrom the tool panel, click on \nNGS: Annotation -> barrnap\n and set the following:\n\n\n\"Fasta file\": MRSA252.fna\n\n\nClick \nExecute\n\n\n\n\nA new file called \nbarrnap on data 3\n will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the \n icon.) \n\n\nNow let's say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool.\n\n\n2. Filter the annotations to get the 23S RNAs\n\n\n\n\nFrom the tool panel, click on \nFilter and Sort -> Select\n and set the following:\n\n\n\"Select lines from\":  (whatever you called the barrnap gff3 output)\n\n\n\"that\": \nMatching\n\n\n\"the pattern\": \n23S\n     (this will look for all the lines in the file that contain \u201c23S\u201d)\n\n\nClick \nExecute\n\n\n\n\nNow you have a gff3 file with just the 23S annotations!\n\n\nWhat now?\n\n\nRemember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button \n at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server.\n\n\nThat's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see \nhttp://genome.edu.au/learn",
            "title": "Introduction to Galaxy"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#introduction-to-galaxy",
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)",
            "title": "Introduction to Galaxy"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#background",
            "text": "Galaxy is a web based analysis and workflow platform designed for biologists to analyse their own data. It comes with most of the popular bioinformatics tools already installed and ready for use. There are many Galaxy servers around the world and some are tailored with specific toolsets and reference data for analysis of human genomics, microbial genomics, proteomics etc.  There are some introductory slides available  here .  Basically, the Galaxy interface is separated into 3 parts. The tool list on the left, the viewing pane in the middle and the analysis and data history on the right. We will be looking at all 3 parts in this tutorial.   This workshop/tutorial will familiarize you with the Galaxy interface. It will cover the following topics:   Logging in to the server  Getting data into galaxy  How to access the tools  Using to use some common tools",
            "title": "Background"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#learning-objectives",
            "text": "At the end of this tutorial you should:   Be able to register on and login to a Galaxy server.  Be able to upload data to a Galaxy server from:  A file on your local computer  A file on a remote datastore with an accessible URL.    Be able use tools in Galaxy by:  Accessing the tool via the tool menu  Using the tool interface to run the particular tool  Viewing/accessing the tool output.",
            "title": "Learning Objectives"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-1-preparation",
            "text": "The purpose of this section is to get you to log in to the server.    Open your browser. We recommend Firefox or Chrome (please don't use Internet Explorer or Safari).    Go to the  Galaxy-Mel  server.   Alternatively, you can use a different Galaxy server - a list of available servers is  here .     If you have previously registered on this server just log in:   On the top menu select:  User -> Login  Enter your password  Click  Submit     If you haven\u2019t registered on this server, you\u2019ll need to now.   On the top menu select:  User -> Register  Enter your email, choose a password, repeat it and add a (all lower case) one word name  Click  Submit",
            "title": "Section 1: Preparation."
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-2-getting-data-into-galaxy",
            "text": "There are 2 main ways to get your data into Galaxy. We will use each of these methods for 3 files and then use those 3 files for the rest of the workshop.    Start a new history for this workshop. To do this:   Click on the history menu button (the   icon) at the top of the Histories panel.  Select  Create New     It is important to note that Galaxy has the concept of \"File Type\" built in. This means that each file stored needs to have its type described to Galaxy as it is being made available. Examples of file types are: text, fasta, fastq, vcf, GFF, Genbank, tabular etc.  We will tell Galaxy what type of file each one is as we upload it.",
            "title": "Section 2: Getting data into Galaxy"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-1-upload-a-file-from-your-own-computer",
            "text": "With this method you can get most of the files on your own computer into Galaxy. (there is a size limit)    Download the following file to your computer:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/galaxy101/Contig_stats.txt.gz   (To download this file, copy the link into a new browser tab, and press enter. The file should now download.)  From the Galaxy tool panel, click on  Get Data -> Upload File  Click the  Choose File  button  Find and select the  Contig_stats.txt.gz  file you downloaded and click  Open  Set the \"Type\" (= file format) to  tabular  Click the  Start  button  Once the progress bar reaches 100%, click the  Close  button     The file will now upload to your current history.",
            "title": "Method 1: Upload a file from your own computer"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-2-upload-a-file-from-a-url",
            "text": "If a file exists on a web resource somewhere and you know its URL (Unique resource location - a web address) you can directly load it into Galaxy.    From the tool panel, click on  Get Data -> Upload File   Click on the  Paste/Fetch Data  button  Copy and paste the following web address into the URL/Text box:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz  Set the file format to  fastqsanger  (not fastqcsanger)  Click  Start  Once the progress bar has reached 100%, click  Close     Note that Galaxy is smart enough to recognize that this is a compressed file and so it will uncompress it as it loads it.",
            "title": "Method 2: Upload a file from a URL"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#method-2-again-get-data-from-a-public-url",
            "text": "Now we are going to upload another file from the remote data source.  Repeat the above for:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna  Note that this file is a  fasta  file and not a  fastqsanger  file.   Reveal detailed instructions   From the tool panel, click on  Get Data -> Upload File  Click on the  Paste/Fetch Data  button  Copy and paste the following web address into the URL/Text box: https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/MRSA0252.fna  Set the file format to  fasta .  Click  Start  Once the progress bar has reached 100%, click  Close    The DNA sequence of  Staphlococcus aureus MRSA252  will be loaded into your history as a fasta file.  Your history should now look like this.",
            "title": "Method 2 (again): Get data from a public URL"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#the-data",
            "text": "Though we aren't going to focus on the contents of these files and what they mean from a bioinformatics standpoint, here is a brief description of each one.    Contigs_stats.txt   this file contains a table of summary data from a de novo genome assembly (the process of attempting to recover the full genome of an organism from the short read sequences produced by most DNA sequencing machines. )  The columns contain a lot of information but the ones we will be using indicate the amount of data (or coverage) that went into making up each piece of the final assembly.          bacterial_std_err_1.fastq.gz   This file contains sequence reads as they would come off an Illumina sequencing machine. They are in  fastq  format.          MRSA0252.fna   This file contains the genome sequence of  Staphylococcus aureus MRSA252 . It is in  fasta  format.",
            "title": "The data"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#section-3-play-with-the-tools",
            "text": "The purpose of this section is to get you used to using the available tools in Galaxy and point out some of the more basic manipulation tools.  Firstly however, you\u2019ll notice that two of the files have very long and confusing names. So we might want to change them. To do this we need to \u201cedit\u201d the file. So:   Click on the   icon (edit) next to the file in the history called:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq  In the \"Name\" text box, give it a new name. Call it:  Typical Fastq File  Click the  Save  button.   Repeat the process for the MRSA252 fasta file. Rename it to  MRSA252.fna  Now that\u2019s better. There was a lot of other functionality hidden behind that edit ( ) icon. You can change a file\u2019s data type, convert its format and many other things. Feel free to play around with them at a later date.  Ok, back to the tools.",
            "title": "Section 3: Play with the tools"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-1-histogram-and-summary-statistics",
            "text": "The first thing we are going to do is produce a histogram of contig read coverage depths and calculate the summary statistics from the Contig_stats.txt file. To do this we need to cut out a couple of columns, remove a line and then produce a histogram. This will introduce some of the text manipulation tools.  Click on the   icon of the  Contig_stats.txt  file to have a look at it. Note that there are 18 columns in this file. We want column 1 and column 6. To do this:  1. Cut out column 1 and column 6.   From the tool panel, click on  Text Manipulation -> Cut  and set the following:  Set \"Cut Columns\" to:  c1,c6  \"Delimited by\":  Tab  \"Cut from\":  Contig_stats.txt  Click  Execute   Examine the new file by clicking on its   icon. We now have 2 columns instead of the 18 in the original file.  2. Remove the Header lines of the new file.   From the tool panel, click on  Text Manipulation -> Remove beginning  and set the following:  \"Remove First\":  1  \"from\":  Cut on data1  click  Execute   Note the the new file is the same as the previous one without the header line.  3. Make a histogram.   From the tool panel, click on  Graph/Display Data -> Histogram  and set the following:  \"Dataset\":  Remove beginning on Data X  \"Numerical column for X axis\":  c2  \"Number of breaks\":  25  \"Plot title\":  Histogram of Contig Coverage  \"Label for X axis\":  Coverage depth  Click  Execute   Click on the   icon of the histogram to have a look at it. Note there are a few peaks. Maybe these correspond to single, double and triple copy number of these contigs.  4. Calculate summary statistics for contig coverage depth.   From the tool panel, click on  Statistics -> Summary Statisitics  and set the following:  \"Summary statistics on\":  Remove beginning on Data X  \"Column or expression\":  c2  Click  Execute   You\u2019ll note that the summary statistics tool failed and is red in the history. There was an error! If you click on the filename, and then the bug   symbol, it will tell you what went wrong. (There is a missing python library.) At this point, you would normally contact your Galaxy server administrator.",
            "title": "Example 1: Histogram and summary statistics"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-2-convert-fastq-to-fasta",
            "text": "This shows how to convert a fastq file to a fasta file. The tool creates a new file with the converted data.  Converter tool   From the tool panel, click on  Convert Formats -> FASTQ to FASTA  and set the following:  \"FASTQ file to convert\":  Typical Fastq File  Click  Execute   This will have created a new Fasta file called FASTQ to FASTA on data 2.",
            "title": "Example 2: Convert Fastq to Fasta"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#example-3-find-ribosomal-rna-features-in-a-dna-sequence",
            "text": "This example shows how to use a tool called \u201cbarrnap\u201d to search for rRNAs in a DNA sequence.  1. Find all of the ribosomal RNA's in a sequence   From the tool panel, click on  NGS: Annotation -> barrnap  and set the following:  \"Fasta file\": MRSA252.fna  Click  Execute   A new file called  barrnap on data 3  will be produced. It is a gff3 file. (This stands for genome feature format - version 3. It is a file format for describing features contained by a DNA sequence.) Change its name to something more appropriate (click on the   icon.)   Now let's say you only want the lines of the file for the 23S rRNA annotations. We can do this using a Filter tool.  2. Filter the annotations to get the 23S RNAs   From the tool panel, click on  Filter and Sort -> Select  and set the following:  \"Select lines from\":  (whatever you called the barrnap gff3 output)  \"that\":  Matching  \"the pattern\":  23S      (this will look for all the lines in the file that contain \u201c23S\u201d)  Click  Execute   Now you have a gff3 file with just the 23S annotations!",
            "title": "Example 3: Find Ribosomal RNA Features in a DNA Sequence"
        },
        {
            "location": "/tutorials/galaxy_101/galaxy_101/#what-now",
            "text": "Remember how we started a new history at the beginning? If you want to see any of your old histories, click on the history menu button   at the top of the histories panel and then select \u201cSaved Histories.\u201d This will give you a list of all the histories you have worked on in this Galaxy server.  That's it. You now know a bit about the Galaxy interface and how to load data, run tools and view their outputs. For more tutorials, see  http://genome.edu.au/learn",
            "title": "What now?"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/",
            "text": ".image-header-gvl {\n    -webkit-column-count: 3;\n    -moz-column-count: 3;\n    column-count: 3;\n\n    -webkit-column-gap: 140px;\n    -moz-column-gap: 140px;\n    column-gap: 140px;\n  }\n\n\n\n\n\n    \n\n    \n\n    \n\n\n\n\n\nGalaxy Workflows\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nBackground\n\n\nThis workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics:\n\n\n\n\nLogging in to the server\n\n\nHow to construct and use a workflow by various methods\n\n\nHow to share a workflow\n\n\n\n\n\n\nSection 1: Preparation.\n\n\nThe purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in: \nLaunch a GVL Galaxy Instance\n or you can use the public \nGalaxy-mel\n\n\nGo to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.)\n\n\n\n\nIf you have previously registered on this server just log in:\n\n\nOn the top menu select: \nUser -> Login\n\n\nEnter your password\n\n\nClick the \nSubmit\n button\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you haven\u2019t registered on this server, you\u2019ll need to now.\n\n\nOn the top menu select: \nUser -> Register\n\n\nEnter your email, choose a password, repeat it and add a (all lower case) one word name\n\n\nClick on the \nSubmit\n button.\n\n\n\n\n\n\n\n\n\n\nSection 2: Create and run a workflow.\n\n\nThis section will show you two different methods to create a workflow and then how to run one.\n\n\nImport the workflow history\n\n\nIn this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it \nhere\n. Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\"\n\n\n\n\nFrom the menu at the top of the Galaxy window, click \nShared Data -> Histories\n\n\nFind the history called \"\nworkflow_finished\n\" and click on it.\n\n\nThen click on \nImport history\n at the top right of the screen.\n\n\nChange the name if you wish and then click \nImport\n\n\n\n\nThis history should now be in your history pane on the right.\n\n\n\n\n\nWorkflow creation: Method 1\n\n\nWe will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow.\n\n\nMake sure your current history is the one you imported in Section 2 - Step 1 above (\nimported: workflow_finished.\n) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history.\n\n\nNow we will create the workflow.\n\n\n\n\nClick on the histories menu button \n at the top of the history pane.\n\n\nClick \nExtract Workflow\n\n\n\n\nYou will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and:\n\n\n\n\nChange the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d\n\n\nClick \nCreate Workflow\n\n\n\n\nThe workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.\n\n\nSome discussion\n\n\nHave a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence.\n\n\nMore interesting though is to:\n\n\n\n\nClick on the  \nWorkflows\n link in the top menu\n\n\nClick on the down arrow on your workflow\u2019s button.\n\n\nClick \nEdit\n\n\n\n\nA visualisation of your workflow will appear. Note the connections and the steps.\n\n\nNext we\u2019ll go through how to create this workflow using the editor..\n\n\nWorkflow Creation: Method 2\n\n\nWe will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.\n\n\nStep 1: Create a workflow name and edit space.\n\n\n\n\nClick on \nWorkflow\n in Galaxy\u2019s menu.\n\n\nClick on the \nCreate New Workflow\n button.\n\n\nIn the \"Workflow Name\" text box type: \nVariants from scratch\n\n\nClick the \nCreate\n button.\n\n\n\n\nYou should now be presented with a blank workflow editing grid.\n\n\nStep 2: Open the editor and place component tools\n\n\nAdd three input datafiles.\n\n\n\n\nIn the Workflow control section of the tool pane, click on \nInputs -> Input dataset\n three times.\n\n\nSpread them out towards the left hand side of the workflow grid by clicking and dragging them around.\n\n\nFor each one, change their name.\n\n\nClick on each input box in turn\n\n\nIn the right hand pane (where the history usually is), change the name to:\n\n\nReference data\n\n\nReads 1\n\n\nReads 2 - respectively.\n\n\n\n\n\n\nIf you click on \nInput dataset\n in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press \nEnter\n to make the change.\n\n\n\n\n\n\n\n\nAdd in the BWA mapping step.\n\n\n\n\nClick on \nNGS: Mapping -> Map with BWA\n in the tool pane. BWA will be added to the workflow grid.\n\n\nIn the right hand pane (where the history usually is), change the following parameters.\n\n\nChange \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to \nUse a genome from history.\n\n\nNote that the BWA box on the grid changes to match these settings.\n\n\n\n\n\n\n\n\nConnect the tools together.\n\n\n\n\nClick and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.)\n\n\n\"Reference data\" output to reference to \"Use the following dataset as the reference sequence\"\n\n\n\"Reads 1\" output to \"Select first set of reads\"\n\n\n\"Reads 2\" output to \"Select second set of reads\"\n\n\n\n\n\n\n\n\nAdd in the Freebayes (Variant Calling step.)\n\n\n\n\nClick on \nNGS: Variant Calling -> Freebayes\n\n\nIn the right hand pane, change the following:\n\n\n\"Choose the source for the reference list:\"  to \nHistory\n\n\nConnect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.\n\n\nConnect the \"Reference data\" output to \"Freebayes\u2019\" \nUse the following dataset as the reference sequence\n input.\n\n\n\n\n\n\n\n\nIf you\u2019re keen - \nNote: this is optional.\n Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes.\n\n\n\n\n\"Choose parameter selection level\": \nComplete list of all options\n\n\n\"Population model options\": \nSet population model options\n\n\n\"Set ploidy for the analysis\": \n1\n\n\n\"Input filters\": \nSet input filters\n\n\n\"Exclude alignments from analysis if they have a mapping quality less than\": \n20\n\n\n\"Exclude alleles from analysis if their supporting base quality is less than\": \n20\n\n\n\"Require at least this fraction of observations \u2026 to evaluate the position\": \n0.9\n\n\n\"Require at least this count of observations .. to evaluate the position\": \n10\n\n\n\"Population and mappability priors\": \nSet population and mappability priors\n\n\n\"Disable incorporation of prior expectations about observations\": \nYes\n\n\n\n\nAdd in the Filter step.\n\n\n\n\nClick on \nFilter and Sort - > Filter\n\n\nConnect \"Freebayes\u2019\" output_vcf to the \"filter\" input.\n\n\nIn the right hand pane, change the following:\n\n\n\"With the following condition\": \nc6 > 500\n\n\n\"Number of header lines to skip\": \n56\n\n\n\n\n\n\n\n\nPhew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore:\n\n\nSelect workflow outputs.\n\n\n\n\nClick on the star next to \"Map with BWA\u2019s\" bam file output.\n\n\nClick on the star next to \"Filter\u2019s\" output vcf.\n\n\n\n\nStep 3: Save it!\n\n\nClick on the \n at the top of the workflow grid and select \nSave\n.\n\n\nCongratulations. You\u2019ve just created a Galaxy workflow.\n\n\nNow to run it!\n\n\nRunning the workflow\n\n\nWe will now make a new history called \"Test\" and run the workflow on it\u2019s data.\n\n\nCreate the new history\n\n\n\n\nFrom the Histories Menu, select \nCopy Datasets\n\n\nSelect the 2 x fastq files and the Ecoli .fna file.\n\n\nUnder destination history, enter \nTest\n into \"New History Named:\"\n\n\nClick \nCopy History Items\n button\n\n\nClick on the link to the new history in the green bar at the top of the screen\n\n\n\n\nRun the workflow\n\n\n\n\nOn the tools pane, click \nAll Workflows\n\n\nSelect the \nVariants from scratch\n workflow (or whatever you called it.)\n\n\nGive it the correct files.\n\n\nEcoli ... .fna\n for Reference data\n\n\nbacterial_std_err_1.fastq\n for Reads 1\n\n\nbacterial_std_err_2.fastq\n for Reads 2\n\n\nClick \nRun Workflow\n\n\n\n\nYour workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.\n\n\nWhat now?\n\n\nWhere to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#galaxy-workflows",
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)",
            "title": "Galaxy Workflows"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#background",
            "text": "This workshop/tutorial will familiarise you with the Galaxy workflow engine. It will cover the following topics:   Logging in to the server  How to construct and use a workflow by various methods  How to share a workflow",
            "title": "Background"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#section-1-preparation",
            "text": "The purpose of this section is to get you to log in to the server.. For this workshop, you can use a Galaxy server that you have created by following the steps outlined in:  Launch a GVL Galaxy Instance  or you can use the public  Galaxy-mel  Go to Galaxy URL of your server in Firefox or Chrome (your choice, please don't use IE or Safari.)   If you have previously registered on this server just log in:  On the top menu select:  User -> Login  Enter your password  Click the  Submit  button       If you haven\u2019t registered on this server, you\u2019ll need to now.  On the top menu select:  User -> Register  Enter your email, choose a password, repeat it and add a (all lower case) one word name  Click on the  Submit  button.",
            "title": "Section 1: Preparation."
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#section-2-create-and-run-a-workflow",
            "text": "This section will show you two different methods to create a workflow and then how to run one.",
            "title": "Section 2: Create and run a workflow."
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#import-the-workflow-history",
            "text": "In this step we will import a shared history to our workspace so we can extract a workflow from it. This will only work on Galaxy servers which have the history available on it. If yours doesn't have the appropriate history, there are instructions to create it  here . Alternatively, you can extract a workflow from any history you have in your \"Saved Histories.\"   From the menu at the top of the Galaxy window, click  Shared Data -> Histories  Find the history called \" workflow_finished \" and click on it.  Then click on  Import history  at the top right of the screen.  Change the name if you wish and then click  Import   This history should now be in your history pane on the right.",
            "title": "Import the workflow history"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-1",
            "text": "We will create a workflow from an existing history. You can use this method to make a re-useable analysis from one you\u2019ve already done. i.e. You can perform the analysis once and then create a workflow out of it to re-use it on more/new data. We will create a workflow from the history you imported in step 1 above. The footnote below explains the steps that created this history. These are the steps we will mimic in the workflow.  Make sure your current history is the one you imported in Section 2 - Step 1 above ( imported: workflow_finished. ) If not, switch to it. If you couldn't import it, you should be able to complete this step with any suitable history.  Now we will create the workflow.   Click on the histories menu button   at the top of the history pane.  Click  Extract Workflow   You will now be shown a page which contains the steps used to create the history you are extracting from. We use this page to say what to include in the workflow. We want everything here so we\u2019ll just accept the defaults and:   Change the Workflow name to something sensible like \u201cBasic Variant Calling Workflow\u201d  Click  Create Workflow   The workflow is now accessible via the bottom of the tool pane by clicking on All Workflows.",
            "title": "Workflow creation: Method 1"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#some-discussion",
            "text": "Have a look at your workflow. Click on its button in the workflow list. It\u2019s tool interface will appear. You can now run this workflow any time you like with different input datasets. NOTE: The input data sets must be of the same types as the original ones. i.e. In our case, two fastq reads files and one fasta reference sequence.  More interesting though is to:   Click on the   Workflows  link in the top menu  Click on the down arrow on your workflow\u2019s button.  Click  Edit   A visualisation of your workflow will appear. Note the connections and the steps.  Next we\u2019ll go through how to create this workflow using the editor..",
            "title": "Some discussion"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#workflow-creation-method-2",
            "text": "We will now create the same read mapping/variant calling workflow using the editor directly. The workflow needs to take in some reads and a reference, map the reads to the reference using BWA, run Freebayes on the BAM output from BWA to call the variants, and finally filter the resulting vcf file.",
            "title": "Workflow Creation: Method 2"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-1-create-a-workflow-name-and-edit-space",
            "text": "Click on  Workflow  in Galaxy\u2019s menu.  Click on the  Create New Workflow  button.  In the \"Workflow Name\" text box type:  Variants from scratch  Click the  Create  button.   You should now be presented with a blank workflow editing grid.",
            "title": "Step 1: Create a workflow name and edit space."
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-2-open-the-editor-and-place-component-tools",
            "text": "Add three input datafiles.   In the Workflow control section of the tool pane, click on  Inputs -> Input dataset  three times.  Spread them out towards the left hand side of the workflow grid by clicking and dragging them around.  For each one, change their name.  Click on each input box in turn  In the right hand pane (where the history usually is), change the name to:  Reference data  Reads 1  Reads 2 - respectively.    If you click on  Input dataset  in the tan box at the top of the right hand panel on the screen, you can change the name of the box as it appears on the editing layout. You need to give each one a more sensible name and press  Enter  to make the change.     Add in the BWA mapping step.   Click on  NGS: Mapping -> Map with BWA  in the tool pane. BWA will be added to the workflow grid.  In the right hand pane (where the history usually is), change the following parameters.  Change \u201cWill you select a reference genome from your history or use a built-in index?:\u201d to  Use a genome from history.  Note that the BWA box on the grid changes to match these settings.     Connect the tools together.   Click and drag on the output of one of the input dataset tools to each of the input spots in the BWA tool. (Make connections.)  \"Reference data\" output to reference to \"Use the following dataset as the reference sequence\"  \"Reads 1\" output to \"Select first set of reads\"  \"Reads 2\" output to \"Select second set of reads\"     Add in the Freebayes (Variant Calling step.)   Click on  NGS: Variant Calling -> Freebayes  In the right hand pane, change the following:  \"Choose the source for the reference list:\"  to  History  Connect \"Map with BWA\" bam output to \"Freebayes\u2019\" bam input.  Connect the \"Reference data\" output to \"Freebayes\u2019\"  Use the following dataset as the reference sequence  input.     If you\u2019re keen -  Note: this is optional.  Also change the following parameters the right hand pane for freebayes to make it a bit more sensible for variant calling in bacterial genomes.   \"Choose parameter selection level\":  Complete list of all options  \"Population model options\":  Set population model options  \"Set ploidy for the analysis\":  1  \"Input filters\":  Set input filters  \"Exclude alignments from analysis if they have a mapping quality less than\":  20  \"Exclude alleles from analysis if their supporting base quality is less than\":  20  \"Require at least this fraction of observations \u2026 to evaluate the position\":  0.9  \"Require at least this count of observations .. to evaluate the position\":  10  \"Population and mappability priors\":  Set population and mappability priors  \"Disable incorporation of prior expectations about observations\":  Yes   Add in the Filter step.   Click on  Filter and Sort - > Filter  Connect \"Freebayes\u2019\" output_vcf to the \"filter\" input.  In the right hand pane, change the following:  \"With the following condition\":  c6 > 500  \"Number of header lines to skip\":  56     Phew! We\u2019re nearly done! The only thing left is to select which workflow outputs we want to keep in our history. Next to each output for every tool is a star. Clicking on the stars will select those files as workflow outputs, everything else will be hidden in the history. In this case we only really want the BAM file and the final variants (vcf file.) Therefore:  Select workflow outputs.   Click on the star next to \"Map with BWA\u2019s\" bam file output.  Click on the star next to \"Filter\u2019s\" output vcf.",
            "title": "Step 2: Open the editor and place component tools"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#step-3-save-it",
            "text": "Click on the   at the top of the workflow grid and select  Save .  Congratulations. You\u2019ve just created a Galaxy workflow.  Now to run it!",
            "title": "Step 3: Save it!"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#running-the-workflow",
            "text": "We will now make a new history called \"Test\" and run the workflow on it\u2019s data.",
            "title": "Running the workflow"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#create-the-new-history",
            "text": "From the Histories Menu, select  Copy Datasets  Select the 2 x fastq files and the Ecoli .fna file.  Under destination history, enter  Test  into \"New History Named:\"  Click  Copy History Items  button  Click on the link to the new history in the green bar at the top of the screen",
            "title": "Create the new history"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#run-the-workflow",
            "text": "On the tools pane, click  All Workflows  Select the  Variants from scratch  workflow (or whatever you called it.)  Give it the correct files.  Ecoli ... .fna  for Reference data  bacterial_std_err_1.fastq  for Reads 1  bacterial_std_err_2.fastq  for Reads 2  Click  Run Workflow   Your workflow will now run. It will send the right files to the right tools at the right time to the cluster (compute engine on your machine) and wait for them to finish. Watch as they turn yellow then green in turn.",
            "title": "Run the workflow"
        },
        {
            "location": "/tutorials/galaxy-workflows/galaxy-workflows/#what-now",
            "text": "Where to start? There\u2019s so much you can do with Workflows. You can even run them on multiple file sets from the one setup.",
            "title": "What now?"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/",
            "text": "History creation instructions for Workflow tutorial\n\n\nUse this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.\n\n\nStep 1: Import the raw datafiles\n\n\n\n\nCreate a new blank history by clicking on the history menu \n, then \nCreate New\n\n\nUse the upload data tool to upload the data files from a remote repository..\n\n\nClick \nGet Data -> Upload File\n\n\nClick \nPaste/Fetch data\n\n\nIn the box paste the following two url's (one per line): \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz\n and \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz\n\n\nChange the \nType\n to \nfastqsanger\n\n\nClick the \nPaste/Fetch data\n button again.\n\n\nPaste \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\nChange the \nType\n on this file to \nfasta\n\n\nClick the \nStart\n button.\n\n\nClick the \nClose\n button.\n\n\n\n\n\n\n\n\nAfter the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.\n\n\nStep 2: Run BWA\n\n\nNow we will run BWA on these files to map the reads to the reference.\n\n\n\n\nIn the tools menu, click \nNGS: Mapping -> Map with BWA\n\n\nSet the following in the tool interface:\n\n\n\"Will you select a reference genome from your history or use a built-in index?\": \nUse a genome from history and build index\n\n\n\"Use the following dataset as the reference sequence\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\n\"Select first set of reads\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq\n\n\n\"Select second set of reads\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.\n\n\nStep 3: Run Freebayes\n\n\nNow we will run Freebayes to call variants in out reads compared with the reference.\n\n\n\n\nIn the tools menu, click \nNGS: Variant Analysis -> Freebayes\n\n\nSet the following in the tool interface:\n\n\n\"Choose the source for the reference genome\": \nHistory\n\n\n\"BAM file\": \nMap with BWA on data 2, data 1, and data 3 (mapped reads in BAM format)\n\n\n\"Use the following dataset as the reference sequence\": \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will run Freebayes on your BAM file and will result in a variant calling format file (vcf). \n\n\nStep 4: Filter the VCF file.\n\n\nNow we will filter the vcf file to something sensible.\n\n\n\n\nIn the tools menu, click \nFilter and Sort -> Filter\n\n\nSet the following in the tool interface:\n\n\n\"Filter\": \nFreeBayes on data 3 and data 4 (variants)\n\n\n\"With following condition\": \nc6 > 500\n\n\n\"Number of header lines to skip\": \n56\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThis will filter out VCF file.\n\n\nYou should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial.\n\n\nReturn to it \nhere",
            "title": "History Creation"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#history-creation-instructions-for-workflow-tutorial",
            "text": "Use this set of instructions to create the base history for the \"Extract Workflow\" section of the workflows tutorial.",
            "title": "History creation instructions for Workflow tutorial"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-1-import-the-raw-datafiles",
            "text": "Create a new blank history by clicking on the history menu  , then  Create New  Use the upload data tool to upload the data files from a remote repository..  Click  Get Data -> Upload File  Click  Paste/Fetch data  In the box paste the following two url's (one per line):  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq.gz  and  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq.gz  Change the  Type  to  fastqsanger  Click the  Paste/Fetch data  button again.  Paste  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna  Change the  Type  on this file to  fasta  Click the  Start  button.  Click the  Close  button.     After the import is complete, you should have 3 files in your history. 2 fastq files and a fasta file.",
            "title": "Step 1: Import the raw datafiles"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-2-run-bwa",
            "text": "Now we will run BWA on these files to map the reads to the reference.   In the tools menu, click  NGS: Mapping -> Map with BWA  Set the following in the tool interface:  \"Will you select a reference genome from your history or use a built-in index?\":  Use a genome from history and build index  \"Use the following dataset as the reference sequence\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna  \"Select first set of reads\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_1.fastq  \"Select second set of reads\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/bacterial_std_err_2.fastq    Click  Execute   This will run BWA and result in an output compressed BAM file containing the mapping information for all of the reads versus the reference.",
            "title": "Step 2: Run BWA"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-3-run-freebayes",
            "text": "Now we will run Freebayes to call variants in out reads compared with the reference.   In the tools menu, click  NGS: Variant Analysis -> Freebayes  Set the following in the tool interface:  \"Choose the source for the reference genome\":  History  \"BAM file\":  Map with BWA on data 2, data 1, and data 3 (mapped reads in BAM format)  \"Use the following dataset as the reference sequence\":  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/COMP90014/Assignment1/Ecoli-O157_H7-Sakai-chr.fna    Click  Execute   This will run Freebayes on your BAM file and will result in a variant calling format file (vcf).",
            "title": "Step 3: Run Freebayes"
        },
        {
            "location": "/tutorials/galaxy-workflows/history_creation/#step-4-filter-the-vcf-file",
            "text": "Now we will filter the vcf file to something sensible.   In the tools menu, click  Filter and Sort -> Filter  Set the following in the tool interface:  \"Filter\":  FreeBayes on data 3 and data 4 (variants)  \"With following condition\":  c6 > 500  \"Number of header lines to skip\":  56    Click  Execute   This will filter out VCF file.  You should now have the requisite history to enable you to complete the workflow extraction section of the workflows tutorial.  Return to it  here",
            "title": "Step 4: Filter the VCF file."
        },
        {
            "location": "/tutorials/genomespace/genomespace/",
            "text": "What is GenomeSpace?\n\n\nGenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer.\n\n\nGenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next.\n\n\nThe GVL GenomeSpace can be found at \nhttps://genomespace.genome.edu.au\n.\n\n\nPrerequisites\n\n\nGenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.\n\n\nRegistering a GenomeSpace account\n\n\nTo register a GenomeSpace account you need to have a valid email address and do the following:\n\n\n\n\n\n\nGo to \nhttps://genomespace.genome.edu.au/\n and click on the \"Register new GenomeSpace user\" link:\n    \n\n\n\n\n\n\nEnter your preferred username, password and the valid email address and click the Sign up button.\n\n\nNote:\n You will receive an error if the username has already been taken.\n\n\n\nIf everything goes right you will see the following page.\n\n\n\n\n\n\n\nActivate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active.\n\n\n\n\n\n\nGo to \nhttps://genomespace.genome.edu.au\n and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace.\n\n\nIn a few seconds you will be redirected to your Home page.\n\n\n\nOn this page you can find the following items:\n\n\n\n\nYour username in the top right corner\n\n\nThe menu bar\n\n\nThe application bar\n\n\nYour home directory\n\n\nThe directory under the name \nShared to \u201cyour username\u201d\n contains any folders that have been shared to you through the GenomeSpace website.\n\n\nThe public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.\n\n\n\n\n\n\n\n\nMaking a swift container\n\n\n(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.)\n\n\nNeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably.\n\n\nIf you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial: \nhttp://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/\n.\n\n\nGo to the NeCTAR dashboard at \nhttps://dashboard.rc.nectar.org.au\n.\n\n\n\n\n\n\nOn the left hand side of the dashboard click on \"Object Store\" and then \"Containers\".\n\n\n\n\n\n\nTo make a container, click \"Create Container\".\n\n\n\n\n\n\n\n\nMounting a swift container\n\n\nContainers can be found under the Object Store link in NeCTAR\u2019s dashboard.\n\n\nTo mount an available container go to Connect menu bar in GenomeSpace and select Swift Container.\n\n\n\nYou will see a new page as follows:\n\n\n\n\nTo fill out this form you need the following parameters:\n\n\n\n\nOpenStack EndPoint:\n The default value should be correct for NeCTAR: \nhttps://keystone.rc.nectar.org.au:5000/v2.0/tokens\n\n\nUser Name:\n This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below:\n  \n\n\nPassword:\n This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key:\n\n\nLogin to your NeCTAR dashboard account.\n\n\nOn the top right hand side of your Home click on the setting link\n    \n\n\nPress the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.)\n    \n\n\n\n\n\n\nTenancy name:\n Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process. \n\n\nContainer name:\n The name of the container that you want to connect to. \n\n\n\n\nBasic file manipulation\n\n\nUnder the containers directory you can perform basic file manipulation as follows:\n\n\n\n\nCreating a directory:\n To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created.\n\n\nUploading a file into a directory:\n Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green.\n\n\nDeleting a file:\n To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds.\n\n\nPreviewing a file:\n Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file.\n\n\nDownloading a file:\n To download a file simply right click on the file and select download. Your download will be started in a few seconds.\n\n\nCreating a public link:\n Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.)\n\n\nCreating a private link:\n Right click on the file and select the \"view file\".\n\n\n\n\nAdding a Galaxy service to your account:\n\n\nPREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first.\n\n\nThe latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows:\n\n\n\n\n\n\nFrom the Menu bar go to the manage menu and select Private Tool.\n    \n\n\n\n\n\n\nFrom the opened window press the \"Add new\" button.      \n\n\n\n\n\n\n\n\nIn the new window fill out the form as follows:\n\n\n\n\nGive a name to your Galaxy\n\n\nGive a description (Optional)\n\n\nTool provider GVL (Optional)\n\n\nBase URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer\n\n\nParameter name: URL\n\n\nRequired: Ticked\n\n\nAllow multiple files: Ticked\n\n\nMultiple file Delimiter: ,\n\n\nSelect the files\u2019 types that you want your galaxy to work on\n\n\nUpload an image as an Icon for Galaxy (Optional)\n\n\nPress the save button. In a few seconds your Galaxy instance will be added to the Application bar.\n\n\n\n\nA sample page can be seen in the following image:\n\n\n\n\n\n\n\nLaunching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch.\n\n\nGalaxy will be opened in a new window.\n\n\n\n\n(Note: Your browser may block the pop-up. Allow the pop-up accordingly).\n\n\n\n\n\n\nFrom the opened Galaxy login and under your username go to the preferences options\n    \n\n\nand select the Manage OpenIDs links:\n\n\n\n\n\n\n\nFrom the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL.\n\n\n\n\n\n\n\n\nFrom now on your Galaxy can talk to GenomeSpace under your UserName.\n\n\nFile transfer to/from Galaxy\n\n\nPREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first (\nHow to\n).\n\n\n\n\n\n\nSending a file:\n\n\n\n\nFrom your Galaxy instance:\n Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy.\n\n\nFrom GenomeSpace:\n You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have \nconnected Galaxy and GenomeSpace\n).    \n\n\n\n\n\n\n\n\nReceiving a file:\n\n\nUnder each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.",
            "title": "Introduction to GenomeSpace"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#what-is-genomespace",
            "text": "GenomeSpace is a cloud-based interoperability framework to support integrative genomics analysis through an easy-to-use Web interface. GenomeSpace provides access to a diverse range of bioinformatics tools, and bridges the gaps between the tools, making it easy to leverage the available analyses and visualizations in each of them. The tools retain their native look and feel, with GenomeSpace providing frictionless conduits between them through a lightweight interoperability layer.  GenomeSpace does not perform any analyses itself; these are done within the member tools wherever they live \u2013 desktop, Web service, cloud, in-house server, etc. Rather, GenomeSpace provides tool selection and launch capabilities, and acts as a data highway automatically reformatting data as required when results move from the output of one tool to input for the next.  The GVL GenomeSpace can be found at  https://genomespace.genome.edu.au .",
            "title": "What is GenomeSpace?"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#prerequisites",
            "text": "GenomeSpace uses a few dialogue boxes to communicate with the NeCTAR cloud. If you have an AdBlocker installed, the dialogues will not be shown properly. If you have an AdBlocker on your system please disable it for the GenomeSpace.genome.edu.au domain.",
            "title": "Prerequisites"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#registering-a-genomespace-account",
            "text": "To register a GenomeSpace account you need to have a valid email address and do the following:    Go to  https://genomespace.genome.edu.au/  and click on the \"Register new GenomeSpace user\" link:\n        Enter your preferred username, password and the valid email address and click the Sign up button.  Note:  You will receive an error if the username has already been taken.  \nIf everything goes right you will see the following page.    Activate your account by following the link in the email from GenomeSpace titled \u201cGenomeSpace user registration\u201d. Your account is now active.    Go to  https://genomespace.genome.edu.au  and enter the username and password you created in the previous steps. You will be logged in into GenomeSpace.  In a few seconds you will be redirected to your Home page.  \nOn this page you can find the following items:   Your username in the top right corner  The menu bar  The application bar  Your home directory  The directory under the name  Shared to \u201cyour username\u201d  contains any folders that have been shared to you through the GenomeSpace website.  The public directory is the directory which contains anything that has been made public to the cloud through the GenomeSpace website.",
            "title": "Registering a GenomeSpace account"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#making-a-swift-container",
            "text": "(These instructions are for the NeCTAR Australian Research Cloud. For any other OpenStack-based cloud storage please change the parameters as necessary.)  NeCTAR object storage is a place that people with NeCTAR credentials can store their data reliably.  If you haven't used the NeCTAR cloud before, follow the steps in sections 1 and 2 of this tutorial:  http://melbournebioinformatics.github.io/MelBioInf_docs/gvl_launch/gvl_launch/ .  Go to the NeCTAR dashboard at  https://dashboard.rc.nectar.org.au .    On the left hand side of the dashboard click on \"Object Store\" and then \"Containers\".    To make a container, click \"Create Container\".",
            "title": "Making a swift container"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#mounting-a-swift-container",
            "text": "Containers can be found under the Object Store link in NeCTAR\u2019s dashboard.  To mount an available container go to Connect menu bar in GenomeSpace and select Swift Container.  You will see a new page as follows:   To fill out this form you need the following parameters:   OpenStack EndPoint:  The default value should be correct for NeCTAR:  https://keystone.rc.nectar.org.au:5000/v2.0/tokens  User Name:  This is your NeCTAR user name. Your user name can be found at the top right corner of the NeCTAR dashboard as shown below:\n    Password:  This is your NeCTAR API key. The API key is the key that applications can use to connect to NeCTAR on your behalf. To find your API key:  Login to your NeCTAR dashboard account.  On the top right hand side of your Home click on the setting link\n      Press the Reset Password button. (Warning: This process will reset your API key. If you have already done this process for any other application you can instead just use your old key.)\n        Tenancy name:  Your NeCTAR Tenancy name (project name). The tenancy name has been assigned to your project by the NeCTAR administration process.   Container name:  The name of the container that you want to connect to.",
            "title": "Mounting a swift container"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#basic-file-manipulation",
            "text": "Under the containers directory you can perform basic file manipulation as follows:   Creating a directory:  To create a directory under another directory, right click on the source directory and select \"Create Subdirectory\". You will be asked for a name and in a few seconds your target directory will be created.  Uploading a file into a directory:  Uploading a file can be done using drag and drop. Go to the directory you want to upload the file into and drag and drop the file you want to upload into the open area on the right-hand side of the Home directory. The effective area will turn green.  Deleting a file:  To delete a file, right click on the selected file and select the \u201cDelete\u201d. The file will be deleted in a few seconds.  Previewing a file:  Under the right click menu you will find the \"preview\" option, which will show the first 5000 bytes of a file.  Downloading a file:  To download a file simply right click on the file and select download. Your download will be started in a few seconds.  Creating a public link:  Right click on the file you want to get the public link for and select the public link. The public link will be shown to you in a few seconds. (Warning: The public link is available for 4 days.)  Creating a private link:  Right click on the file and select the \"view file\".",
            "title": "Basic file manipulation"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#adding-a-galaxy-service-to-your-account",
            "text": "PREREQUISITE: Please make sure you have an account on the Galaxy server you want to add to GenomeSpace. If you have launched a GVL instance, this is a new instance of Galaxy and you will need to register a Galaxy account in your new Galaxy server first.  The latest GVL image is fully compatible with GenomeSpace. Galaxy launched as part of GVL instances can be connected to GenomeSpace as follows:    From the Menu bar go to the manage menu and select Private Tool.\n        From the opened window press the \"Add new\" button.           In the new window fill out the form as follows:   Give a name to your Galaxy  Give a description (Optional)  Tool provider GVL (Optional)  Base URL: http://[Glaxy-ip or DNS]/galaxy/tool_runner?tool_id=genomespace_importer  Parameter name: URL  Required: Ticked  Allow multiple files: Ticked  Multiple file Delimiter: ,  Select the files\u2019 types that you want your galaxy to work on  Upload an image as an Icon for Galaxy (Optional)  Press the save button. In a few seconds your Galaxy instance will be added to the Application bar.   A sample page can be seen in the following image:    Launching the added Galaxy from GenomeSpace: From GenomeSpace click on the Arrow on the right side of your Galaxy application in the Application bar and select launch.  Galaxy will be opened in a new window.   (Note: Your browser may block the pop-up. Allow the pop-up accordingly).    From the opened Galaxy login and under your username go to the preferences options\n      and select the Manage OpenIDs links:    From the associate more OpenID select GenomeSpace and press login. GenomeSpace will be appear as link on the top as a URL.     From now on your Galaxy can talk to GenomeSpace under your UserName.",
            "title": "Adding a Galaxy service to your account:"
        },
        {
            "location": "/tutorials/genomespace/genomespace/#file-transfer-tofrom-galaxy",
            "text": "PREREQUISITE: Please make sure you have connected your Galaxy to GenomeSpace first ( How to ).    Sending a file:   From your Galaxy instance:  Go to Get Data and select GenomeSpace Importer (Please make sure you are logged into GenomeSpace). You will see your GenomeSpace home page in a few seconds. Select the file you want to send to Galaxy and press the Send to Galaxy button. A new job will be created in Galaxy and when it completes, your file will be in Galaxy.  From GenomeSpace:  You can also send a file into Galaxy from the GenomeSpace home page. Simply drag and drop the file into Galaxy (Please make sure you have  connected Galaxy and GenomeSpace ).         Receiving a file:  Under each file click on the store icon and choose \"Send to GenomeSpace\". A dialog will be opened. Choose the directory to store the file and enter the name of the file to store. If you do not provide the file name the file will be stored as \u201cdisplay\u201d. By clicking the Send button the file will be sent to GenomeSpace. The dialog box will close on success.",
            "title": "File transfer to/from Galaxy"
        },
        {
            "location": "/tutorials/docker/docker/",
            "text": "Workshop Slides\n (use the arrow keys to navigate)\n\n\n\n\nPart 1: Docker and Containers\n\n\nPart 2: Running Containers\n\n\nPart 3: Making your Own Image\n\n\nPart 4: Docker on HPC",
            "title": "Containerized Bioinformatics"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/",
            "text": "Introduction to Variant Calling using Galaxy\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.\n\n\nNote: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with the FASTQ format and base quality scores\n\n\nBe able to align reads to generate a BAM file and subsequently generate a pileup file\n\n\nBe able to run the FreeBayes variant caller to find SNVs and indels\n\n\nBe able to visualise BAM files using the \nIntegrative Genomics Viewer (IGV)\n and identify likely SNVs and indels by eye\n\n\n\n\n\n\nBackground\n\n\nSome background reading material - \nbackground\n\n\nWhere is the data in this tutorial from?\n\n\nThe workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the \n1000 genomes\n Genomes project.\n\n\n\n\n1. Preparation\n\n\n\n\nMake sure you have an instance of Galaxy ready to go.\n\n\nIf you are not using your own Galaxy instance, you can use our \nGalaxy Tutorial server\n or \nGalaxy Melbourne server\n.\n\n\n\n\n\n\n\n\nImport data for the tutorial.\n\n\n\n\nIn this case, we are uploading a \nFASTQ\n file.\n\n\nMethod 1\n\n\nPaste/Fetch data from a URL to Galaxy.\n\n\nIn the Galaxy tools panel (left), click on \nGet Data\n and choose \nUpload File\n.\n\n\nClick \nPaste/Fetch data\n and paste the following URL into the box\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq\n\n\n\n\n\nSelect \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nOnce the upload status turns \ngreen\n, it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).\n\n\nThe dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the \nName\n box and give the file a shorter name by removing the URL. Then click \nSave\n.\n\n\n\n\n\n\n\n\nAlternatively, if you have a local file to upload (\nFor the purpose of this tutorial we can stick with the option above\n):\n\n\n\n\nMethod 2\n\n\nUpload data to Galaxy.\n\n\nIn the Galaxy tools panel (left), click on \nGet Data\n and choose \nUpload File\n.\n\n\nFrom \nChoose local file\n select the downloaded FASTQ file. Select \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nOnce the upload status turns \ngreen\n, it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).\n\n\n\n\n\n\n\n\n\n\n\n\nSummary:\n\nSo far, we have started a Galaxy instance, got hold of our data and uploaded it to\nthe Galaxy instance.\n\nNow we are ready to perform our analysis.\n\n\n\n\n2. Quality Control\n\n\nThe first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!\n\n\n1. Take a look at the FASTQ file\n\n\n\n\nClick on the eye icon to the top right of the fastq file to view the a snippet of the file.\n\n\nNote that each read is represented by 4 lines:\n\n\nread identifier\n\n\nshort read sequence\n\n\nseparator\n\n\nshort read sequence quality scores\n\n\n\n\n\n\n\n\ne.g.\nidentifier:    @61CC3AAXX100125:7:72:14903:20386/1\nread sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT\nseparator:     +\nquality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6\n\n\n\n\nFor more details see \nFASTQ\n.\n\n\n2. Assessing read quality from the FASTQ files\n\n\n\n\nFrom the Galaxy tools panel, select \nNGS: QC and manipulation > FastQC: Read Quality reports\n.\n\nThe input FASTQ file will be selected by default. Keep the other defaults and click execute.\n\n\n\n\n\n\n\n   Tip:\n   Note the batch processing interface of Galaxy:\n\n   \n\n   \n grey = waiting in queue\n\n   \n yellow = running\n\n   \n green = finished\n\n   \n red = tried to run and failed\n   \n\n\n\n\nWhen the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data).\nLook at the various quality scores. The data looks pretty good - \nhigh Per base sequence quality\n (avg. above 30).\n\n\nNote that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step.\n\n\n\n\n3. Alignment to the reference - (FASTQ to BAM)\n\n\nThe basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.\n\n\n1. Align the reads with BWA\n\n\n\n\n\n\nMap/align the reads with the \nBWA\n tool to Human reference genome 19 (hg19) \nUCSC hg19\n.\n    From the Galaxy tools panel, select\n\n\n\nNGS: Mapping > Map with BWA-MEM [3-5mins]\n\n\n\nFrom the options:\n\nUsing reference genome: set to hg19\n\nSingle or Paired-end reads: set to Single\n\n\n\nMake sure your fastq file is the input file.\n\n\nKeep other options as default and click execute.\n\n\n\n\nNote: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings\n\n\n\n\n\n\nSort the BAM file.\n  From the Galaxy tools panel, select\n\n\n\nNGS: SAM Tools > Sort BAM dataset\n\n\n\nFrom the options:\n\n\nBAM File: set to the output from the alignment BAM file\n\n\nSort by: Chromosomal coordinates\n\n\nKeep other options as default and click execute\n\n\n\n\n\n\n\n\n\n2. Examine the alignment\n\n\n\n\n\n\nTo examine the output sorted BAM file, we need to first convert it into readable \nSAM\n format.\n  From the Galaxy tools panel, select\n\n\n\nNGS: SAM Tools > BAM-to-SAM\n\n\n\nFrom the options:\n\n\nBAM File to Convert: set to the output to the sorted BAM file\n\n\nKeep other options as default and click execute\n\n\n\n\n\n\n\n\n\nExamine the generated Sequence Alignment Map (SAM) file.\n\n\n\n\nClick the eye icon next to the newly generated file\n\n\nFamiliarise yourself with the \nSAM\n format\n\n\nNote that some reads have mapped to non-chr22 chromosomes (see column 3).\n\n\n\n\nThis is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant?\n\n\n\n\n\nTip:\nGalaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs.\n\n\nThis can be done as follows:\n\n\nClick on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.\n\n\n\n\n\n\n\n\n3. Assess the alignment data\n\n\nWe can generate some mapping statistics from the BAM file to assess the quality of our alignment.\n\n\n\n\n\n\nRun IdxStats. From the Galaxy tools panel, select\n    \n\n    \nNGS: SAM Tools > IdxStats\n\n    \n\n    Select the sorted BAM file as input. Keep other options as default and click execute.\n    \n\n    \n\n\nIdxStats generates a tab-delimited output with four columns.\nEach line consists of a reference sequence name (e.g. a chromosome),\nreference sequence length, number of mapped reads and number of placed but\nunmapped reads.\n\n\nWe can see that most of the reads are aligning to chromosome 22 as expected.\n\n\n\n\n\n\nRun Flagstat. From the Galaxy tools panel, select\n    \n\n    \nNGS: Sam Tools > Flagstat\n\n    \n\n    From the options:\n    \n\n    The BAM: select the sorted BAM file\n    \n\n    Keep other options as default and click execute\n    \n\n    \n\n\nNote that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.\n\n\n\n\n\n\n\n\n4. Visualise the BAM file.\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on the sorted BAM file dataset in the History panel.\n\n\nClick on \"Display with IGV \nweb current\n\". This should download a .jnlp\n    Java Web Start file to your computer. Open this file to run IGV.\n    (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \"\nlocal\n\" instead of \"web current\", and this will open the BAM file in your current IGV session.\n\n\nOnce IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded.\n\n\nOur reads for this tutorial are from chromosome 22, so select \nchr22\n from the second\n    drop box under the toolbar. Zoom in to view alignments of reads to the reference genome.\n\n\n\n\n\n\nTry looking at region \nchr22:36,006,744-36,007,406\n\n\nCan you see a few variants?  \n\n\n\n\nDon't close IGV yet as we'll be using it later.\n\n\n\n\n5. Generate a pileup file\n\n\nA pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view.\n\n\nThe current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.\n\n\n\n\n\n\nGenerate a pileup file:\n\n\nFrom the Galaxy tools panel, select\n\n\n\nNGS: SAMtools > Generate Pileup\n\n\n\nFrom the options:\n\nCall consensus according to MAQ model = Yes\n\nThis generates a called 'consensus base' for each chromosomal position.\n\n\n\nKeep other options as default and click execute\n\n\n\n\nFor each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to \npileup\n.\n\n\n\n\nFirst, rename the output to something more meaningful by clicking on the pencil icon\n\n\nTo change the datatype, click on the Datatype link from the top tab while you're editing attributes.\n\n\nFor downstream processing we want to tell Galaxy that this is a \npileup\n file. From the drop-down, select Pileup and click save.\n\n\n\n\n\n\n\n Tip:\nThe pileup file we generated has 10 columns:\n\n\n 1. chromosome\n\n\n 2. position\n\n\n 3. current reference base\n\n\n 4. consensus base from the mapped reads\n\n\n 5. consensus quality\n\n\n 6. SNV quality\n\n\n 7. maximum mapping quality\n\n\n 8. coverage\n\n\n 9. bases within reads\n\n\n 10. quality values\n\n\nFurther information on (9):\n\nEach character represents one of the following (the longer this string, higher the coverage):\n  \n\n  \n . = match on forward strand for that base\n  \n , = match on reverse strand\n  \n ACGTN = mismatch on forward\n  \n acgtn = mismatch on reverse\n  \n +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next\n  \n -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next\n  \n ^ = start of read\n  \n $ = end of read\n  \n BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores\n  \n\n\n\n\n\n\n\n\nFilter the pileup file:\n\n\nIf you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads.\n  \n\n  From the Galaxy tools panel, select:\n  \n\n  \nNGS: SAM Tools > Filter Pileup\n\n  \n\n  From the options:\n\n  which contains = Pileup with ten columns (with consensus)\n\n  Do not report positions with coverage lower than = 10\n\n\nTry this filtering step two ways:\n\n\n\n\nFirst, set the parameter \nOnly report variants?\n to \nNo\n. This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location.\n\n\nThen, repeat the step but set \nOnly report variants?\n to \nYes\n. This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores.\n\n\n\n\nExamine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.\n\n\n\n\n\n\n6. Call variants with FreeBayes\n\n\nFreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the \nAdvanced Variant Calling\n tutorial.\n\n\n\n\n\n\nCall variants with FreeBayes.\n Under \nNGS ANALYSIS\n, select the tool \nNGS: Variant Analysis -> FreeBayes\n.\n\n\n\n\nSelect your sorted BAM file as input, and select the correct reference genome.\n\n\nUnder \nChoose parameter selection level\n, select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on \nGalaxy-specific options\n.\n\n\nExecute\n FreeBayes.\n\n\n\n\n\n\n\n\nCheck the generated list of variants\n.\n\n\n\n\nClick the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns.\n\n\nHow many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)\n\n\nWhat sort of quality scores do your variants have?\n\n\n\n\nFreeBayes, like most variant callers, produces a \nVariant Call Format (VCF)\n file.\n\n\nVCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers.\n\n\nThe data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call.\n\n\nThe columns in more detail are:\n\n\n\n\n\n\n\n\nCol\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCHROM\n\n\nChromosome name\n\n\n\n\n\n\n2\n\n\nPOS\n\n\n1-based position. For an indel, this is the position preceding the indel.\n\n\n\n\n\n\n3\n\n\nID\n\n\nVariant identifier (optional). Usually the dbSNP rsID.\n\n\n\n\n\n\n4\n\n\nREF\n\n\nReference sequence at POS involved in the variant. For a SNP, it is a single base.\n\n\n\n\n\n\n5\n\n\nALT\n\n\nComma delimited list of alternative sequence(s) seen in our reads.\n\n\n\n\n\n\n6\n\n\nQUAL\n\n\nPhred-scaled probability of all samples being homozygous reference.\n\n\n\n\n\n\n7\n\n\nFILTER\n\n\nSemicolon delimited list of filters that the variant fails to pass.\n\n\n\n\n\n\n8\n\n\nINFO\n\n\nSemicolon delimited list of variant information.\n\n\n\n\n\n\n9\n\n\nFORMAT\n\n\nColon delimited list of the format of individual genotypes in the following fields.\n\n\n\n\n\n\n10+\n\n\nSample(s)\n\n\nIndividual genotype information defined by FORMAT.\n\n\n\n\n\n\n\n\nFor even more detail on VCF files, you can look at the \nVCF format specification\n.\n\n\n\n\n\n\nVisualise the variants and compare files\n\n\n\n\nOpen the VCF file in IGV using the dataset's \ndisplay in IGV local\n link (using the \nweb current\n link will open IGV again, and using \nlocal\n should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.\n\n\nTake a look again at the same region as earlier: \nchr22:36,006,744-36,007,406\n\n\nTry comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool \nFilter and Sort > Filter\n. Choose your previously-filtered pileup file as input, and set the filter condition to \nc1==\"chr22\" and c2 > 36006744 and c2 < 36007406\n.\n\n\n\n\n\n\n\n\nOptional: filter variants\n: See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the \nFilter and Sort: Filter\n tool we used above.\n\n\n\n\n\n\n7. Further steps\n\n\nWe've seen how to:\n\n\n\n\nAlign the raw data (sequence reads) to a reference genome\n\n\nGenerate variant calls from aligned reads\n\n\nInterpret the various file formats used in storing reads, alignments and variant calls\n\n\nVisualise the data using IGV\n\n\n\n\nFor real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants.\n\n\nWe look at some further steps in the \nAdvanced Variant Calling\n tutorial.",
            "title": "Introduction to Variant Calling"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#introduction-to-variant-calling-using-galaxy",
            "text": "",
            "title": "Introduction to Variant Calling using Galaxy"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#tutorial-overview",
            "text": "In this tutorial we cover the concepts of detecting small variants (SNVs and indels) in human genomic DNA using a small set of reads from chromosome 22.  Note: The tutorial is designed to introduce the tools, datatypes and workflow of variation detection. We filter the variants manually to understand what is actually happening in variant calling. In practice the datasets would be much larger and you would carry out some extra steps to improve the quality of the called variants.",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#learning-objectives",
            "text": "At the end of this tutorial you should:   Be familiar with the FASTQ format and base quality scores  Be able to align reads to generate a BAM file and subsequently generate a pileup file  Be able to run the FreeBayes variant caller to find SNVs and indels  Be able to visualise BAM files using the  Integrative Genomics Viewer (IGV)  and identify likely SNVs and indels by eye",
            "title": "Learning Objectives"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#background",
            "text": "Some background reading material -  background",
            "title": "Background"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#where-is-the-data-in-this-tutorial-from",
            "text": "The workshop is based on analysis of short read data from the exome of chromosome 22 of a single human individual. There are one million 76bp reads in the dataset, produced on an Illumina GAIIx from exome-enriched DNA. This data was generated as part of the  1000 genomes  Genomes project.",
            "title": "Where is the data in this tutorial from?"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-preparation",
            "text": "Make sure you have an instance of Galaxy ready to go.  If you are not using your own Galaxy instance, you can use our  Galaxy Tutorial server  or  Galaxy Melbourne server .     Import data for the tutorial.   In this case, we are uploading a  FASTQ  file.  Method 1  Paste/Fetch data from a URL to Galaxy.  In the Galaxy tools panel (left), click on  Get Data  and choose  Upload File .  Click  Paste/Fetch data  and paste the following URL into the box \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/VariantDet_BASIC/NA12878.GAIIx.exome_chr22.1E6reads.76bp.fastq   Select  Type  as  fastqsanger  and click  Start .  Once the upload status turns  green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).  The dataset will have a very long name, as it's named after the full URL we got it from. Optionally, once the file is in your History, click the pencil icon in the upper-right corner of the green dataset box, then select the  Name  box and give the file a shorter name by removing the URL. Then click  Save .     Alternatively, if you have a local file to upload ( For the purpose of this tutorial we can stick with the option above ):   Method 2  Upload data to Galaxy.  In the Galaxy tools panel (left), click on  Get Data  and choose  Upload File .  From  Choose local file  select the downloaded FASTQ file. Select  Type  as  fastqsanger  and click  Start .  Once the upload status turns  green , it means the upload is complete. You should now be able to see the file in the Galaxy history panel (right).       Summary: \nSo far, we have started a Galaxy instance, got hold of our data and uploaded it to\nthe Galaxy instance. \nNow we are ready to perform our analysis.",
            "title": "1. Preparation"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-quality-control",
            "text": "The first step is to evaluate the quality of the raw sequence data. If the quality is poor, then adjustments can be made - e.g. trimming the short reads, or adjusting your expectations of the final outcome!",
            "title": "2. Quality Control"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-take-a-look-at-the-fastq-file",
            "text": "Click on the eye icon to the top right of the fastq file to view the a snippet of the file.  Note that each read is represented by 4 lines:  read identifier  short read sequence  separator  short read sequence quality scores     e.g.\nidentifier:    @61CC3AAXX100125:7:72:14903:20386/1\nread sequence: TTCCTCCTGAGGCCCCACCCACTATACATCATCCCTTCATGGTGAGGGAGACTTCAGCCCTCAATGCCACCTTCAT\nseparator:     +\nquality score: ?ACDDEFFHBCHHHHHFHGGCHHDFDIFFIFFIIIIHIGFIIFIEEIIEFEIIHIGFIIIIIGHCIIIFIID?@<6  For more details see  FASTQ .",
            "title": "1. Take a look at the FASTQ file"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-assessing-read-quality-from-the-fastq-files",
            "text": "From the Galaxy tools panel, select  NGS: QC and manipulation > FastQC: Read Quality reports . The input FASTQ file will be selected by default. Keep the other defaults and click execute.    \n   Tip:\n   Note the batch processing interface of Galaxy: \n    \n     grey = waiting in queue \n     yellow = running \n     green = finished \n     red = tried to run and failed\n      When the job has finished, click on the eye icon to view the newly generated data (in this case a set of quality metrics for the FASTQ data).\nLook at the various quality scores. The data looks pretty good -  high Per base sequence quality  (avg. above 30).  Note that the 'Sequence Duplication Levels' are marked as high. Normally we would run another tool to remove duplicates (technical PCR artifacts) but for the sake of brevity we will omit this step.",
            "title": "2. Assessing read quality from the FASTQ files"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-alignment-to-the-reference-fastq-to-bam",
            "text": "The basic process here to map individual reads - from the input sample FASTQ file - to a matching region on the reference genome.",
            "title": "3. Alignment to the reference - (FASTQ to BAM)"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#1-align-the-reads-with-bwa",
            "text": "Map/align the reads with the  BWA  tool to Human reference genome 19 (hg19)  UCSC hg19 .\n    From the Galaxy tools panel, select  NGS: Mapping > Map with BWA-MEM [3-5mins]  \nFrom the options: \nUsing reference genome: set to hg19 \nSingle or Paired-end reads: set to Single  \nMake sure your fastq file is the input file. \nKeep other options as default and click execute.   Note: This is the longest step in the workshop and will take a few minutes, possibly more depending on how many people are also scheduling mappings    Sort the BAM file.\n  From the Galaxy tools panel, select  NGS: SAM Tools > Sort BAM dataset  \nFrom the options: \nBAM File: set to the output from the alignment BAM file \nSort by: Chromosomal coordinates \nKeep other options as default and click execute",
            "title": "1. Align the reads with BWA"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#2-examine-the-alignment",
            "text": "To examine the output sorted BAM file, we need to first convert it into readable  SAM  format.\n  From the Galaxy tools panel, select  NGS: SAM Tools > BAM-to-SAM  \nFrom the options: \nBAM File to Convert: set to the output to the sorted BAM file \nKeep other options as default and click execute     Examine the generated Sequence Alignment Map (SAM) file.   Click the eye icon next to the newly generated file  Familiarise yourself with the  SAM  format  Note that some reads have mapped to non-chr22 chromosomes (see column 3).   This is the essence of alignment algorithms - the aligner does the best it can, but because of compromises in accuracy vs performance and repetitive sequences in the genome, not all the reads will necessarily align to the \u2018correct\u2019 sequence or could this be suggesting the presence of a structural variant?   \nTip:\nGalaxy auto-generates a name for all outputs. Therefore, it is advisable to choose a more meaningful name to these outputs.  This can be done as follows:  Click on the pencil icon (edit attributes) and change Name e.g. Sample.bam or Sample.sam or Sample.sorted.bam etc.",
            "title": "2. Examine the alignment"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#3-assess-the-alignment-data",
            "text": "We can generate some mapping statistics from the BAM file to assess the quality of our alignment.    Run IdxStats. From the Galaxy tools panel, select\n     \n     NGS: SAM Tools > IdxStats \n     \n    Select the sorted BAM file as input. Keep other options as default and click execute.\n     \n      IdxStats generates a tab-delimited output with four columns.\nEach line consists of a reference sequence name (e.g. a chromosome),\nreference sequence length, number of mapped reads and number of placed but\nunmapped reads.  We can see that most of the reads are aligning to chromosome 22 as expected.    Run Flagstat. From the Galaxy tools panel, select\n     \n     NGS: Sam Tools > Flagstat \n     \n    From the options:\n     \n    The BAM: select the sorted BAM file\n     \n    Keep other options as default and click execute\n     \n      Note that in this case the statistics are not very informative. This is because the dataset has been generated for this workshop and much of the noise has been removed (and in fact we just removed a lot more noise in the previous step); also we are using single ended read data rather than paired-end so some of the metrics are not relevant.",
            "title": "3. Assess the alignment data"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#4-visualise-the-bam-file",
            "text": "To visualise the alignment data:   Click on the sorted BAM file dataset in the History panel.  Click on \"Display with IGV  web current \". This should download a .jnlp\n    Java Web Start file to your computer. Open this file to run IGV.\n    (You will need Java installed on your computer to run IGV). NOTE: If IGV is already open on your computer, you can click \" local \" instead of \"web current\", and this will open the BAM file in your current IGV session.  Once IGV opens, it will show you the BAM file. This may take a bit of time as the data is downloaded.  Our reads for this tutorial are from chromosome 22, so select  chr22  from the second\n    drop box under the toolbar. Zoom in to view alignments of reads to the reference genome.    Try looking at region  chr22:36,006,744-36,007,406  Can you see a few variants?     Don't close IGV yet as we'll be using it later.",
            "title": "4. Visualise the BAM file."
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#5-generate-a-pileup-file",
            "text": "A pileup is essentially a column-wise representation of the aligned reads - at the base level - to the reference. The pileup file summarises all data from the reads at each genomic region that is covered by at least one read. Each row of the pileup file gives similar information to a single vertical column of reads in the IGV view.  The current generation of variant calling tools do not output pileup files, and you don't need to do this section in order to use FreeBayes in the next section. However, a pileup file is a good illustration of the evidence the variant caller is looking at internally, and we will produce one to see this evidence.    Generate a pileup file:  From the Galaxy tools panel, select  NGS: SAMtools > Generate Pileup  \nFrom the options: \nCall consensus according to MAQ model = Yes \nThis generates a called 'consensus base' for each chromosomal position.  \nKeep other options as default and click execute   For each output file, Galaxy tries to assign a datatype attribute to every file. In this case, you'll need to manually assign the datatype to  pileup .   First, rename the output to something more meaningful by clicking on the pencil icon  To change the datatype, click on the Datatype link from the top tab while you're editing attributes.  For downstream processing we want to tell Galaxy that this is a  pileup  file. From the drop-down, select Pileup and click save.    \n Tip:\nThe pileup file we generated has 10 columns:   1. chromosome   2. position   3. current reference base   4. consensus base from the mapped reads   5. consensus quality   6. SNV quality   7. maximum mapping quality   8. coverage   9. bases within reads   10. quality values  Further information on (9): \nEach character represents one of the following (the longer this string, higher the coverage):\n   \n    . = match on forward strand for that base\n    , = match on reverse strand\n    ACGTN = mismatch on forward\n    acgtn = mismatch on reverse\n    +[0-9]+[ACGTNacgtn]+' = insertion between this reference position and the next\n    -[0-9]+[ACGTNacgtn]+' = deletion between this reference position and the next\n    ^ = start of read\n    $ = end of read\n    BaseQualities = one character per base in ReadBases, ASCII encoded Phred scores\n       Filter the pileup file:  If you click the eye icon to view the contents of your pileup file, you'll see that the visible rows of the file aren't very interesting as they are outside chromosome 22 and have very low coverage. Let's filter to regions with coverage of at least 10 reads.\n   \n  From the Galaxy tools panel, select:\n   \n   NGS: SAM Tools > Filter Pileup \n   \n  From the options: \n  which contains = Pileup with ten columns (with consensus) \n  Do not report positions with coverage lower than = 10  Try this filtering step two ways:   First, set the parameter  Only report variants?  to  No . This will give you all locations with a coverage of at least 10 and sufficient read quality. This is similar to the information you see when you look at IVG: each pilup row corresponds to a column of aligned bases at one genomic location.  Then, repeat the step but set  Only report variants?  to  Yes . This will effectively do variant calling: it will give you only locations that have some evidence that there might be a variant present. This variant calling is not very stringent, so you will still get lots of rows. We could filter further to, for instance, variants with high quality scores.   Examine your two pileup files and understand the difference between them. Which coordinates are present in each? What do the bases look like in one compared to the other? Compare the variant quality score (in column 6) to the bases listed on each row.",
            "title": "5. Generate a pileup file"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#6-call-variants-with-freebayes",
            "text": "FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of possible variants. We look at it in more detail in the  Advanced Variant Calling  tutorial.    Call variants with FreeBayes.  Under  NGS ANALYSIS , select the tool  NGS: Variant Analysis -> FreeBayes .   Select your sorted BAM file as input, and select the correct reference genome.  Under  Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on  Galaxy-specific options .  Execute  FreeBayes.     Check the generated list of variants .   Click the eye icon to examine the VCF file contents. The VCF format is described below - make sure you can identify the header rows and the data, and understand the important columns.  How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)  What sort of quality scores do your variants have?   FreeBayes, like most variant callers, produces a  Variant Call Format (VCF)  file.  VCF consists of a header section and a data section. The header section has some information about the file and the parameters used to produce it. The header also specifies what information is stored in the INFO, FILTER and FORMAT columns, as this is different for different variant callers.  The data section has several columns. For this tutorial, you should concentrate on CHROM, POS, REF, ALT and QUAL. CHROM and POS describe the variant's genomic location, REF and ALT describe the variant's nucleotides relative to the reference, and QUAL is a quality score giving FreeBayes' confidence in the correctness of this variant call.  The columns in more detail are:     Col  Field  Description      1  CHROM  Chromosome name    2  POS  1-based position. For an indel, this is the position preceding the indel.    3  ID  Variant identifier (optional). Usually the dbSNP rsID.    4  REF  Reference sequence at POS involved in the variant. For a SNP, it is a single base.    5  ALT  Comma delimited list of alternative sequence(s) seen in our reads.    6  QUAL  Phred-scaled probability of all samples being homozygous reference.    7  FILTER  Semicolon delimited list of filters that the variant fails to pass.    8  INFO  Semicolon delimited list of variant information.    9  FORMAT  Colon delimited list of the format of individual genotypes in the following fields.    10+  Sample(s)  Individual genotype information defined by FORMAT.     For even more detail on VCF files, you can look at the  VCF format specification .    Visualise the variants and compare files   Open the VCF file in IGV using the dataset's  display in IGV local  link (using the  web current  link will open IGV again, and using  local  should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.  Take a look again at the same region as earlier:  chr22:36,006,744-36,007,406  Try comparing to the corresponding location in the pileup file. You can filter to the same window as we just opened in IGV with the tool  Filter and Sort > Filter . Choose your previously-filtered pileup file as input, and set the filter condition to  c1==\"chr22\" and c2 > 36006744 and c2 < 36007406 .     Optional: filter variants : See if you can work out how to filter your VCF file to variants with quality scores greater than 50. You can use the  Filter and Sort: Filter  tool we used above.",
            "title": "6. Call variants with FreeBayes"
        },
        {
            "location": "/tutorials/variant_calling_galaxy_1/variant_calling_galaxy_1/#7-further-steps",
            "text": "We've seen how to:   Align the raw data (sequence reads) to a reference genome  Generate variant calls from aligned reads  Interpret the various file formats used in storing reads, alignments and variant calls  Visualise the data using IGV   For real variant calling, you will probably want to carry out clean-up steps on your BAM file to improve the quality of the calls, and do further filtering and selection on the resulting variants.  We look at some further steps in the  Advanced Variant Calling  tutorial.",
            "title": "7. Further steps"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/",
            "text": "Variant Detection - Advanced Workshop\n\n\nTutorial Overview\n\n\nIn this tutorial, we will look further at variant calling from sequence data. We will:\n\n\n\n\nAlign NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop\n\n\nCarry out local realignment on our aligned reads\n\n\nCompare the performance of different variant calling tools\n\n\nAnnotate our called variants with reference information\n\n\n\n\nBackground\n\n\nSome background reading and reference material can be found \nhere\n.\n\n\nThe slides used in this workshop can be found \nhere\n.\n\n\nWhere is the data in this tutorial from?\n\n\nThe data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/\n\n\nPreparation\n\n\n\n\n\n\nMake sure you have an instance of Galaxy ready to go.\n\n\n\n\nIf you don't have your own - go to our \nGalaxy-Tut\n or \nGalaxy-Melbourne\n server.\n\n\nLog in so that your work will be saved.\nIf you don't already have an account on this server, select from the menu \nUser -> Register\n and create one.\n\n\n\n\n\n\n\n\nImport data for the tutorial.\n\n\n\n\n\n\nWe will import a pair of FASTQ files containing paired-end reads, and a\n  VCF file of known human variants to use for variant evaluation.\n\n\n\n\n\n\nMethod 1: Paste/Fetch data from a URL to Galaxy.\n\n\n\n\nIn the Galaxy tools panel (left), under \nBASIC TOOLS\n, click on \nGet Data\n and choose \nUpload File\n.\n\n\nGet the FASTQ files: click \nPaste/Fetch data\n and enter these URLs into the text box.\nIf you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines.\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2\n\n\n\n\n\nSelect \nType\n as \nfastqsanger\n and click \nStart\n. Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which.\n\n\nGet the VCF file: click \nPaste/Fetch data\n again to open a new text box, and paste the following URL into the box\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf\n\n\n\n\n\nThis time, you can leave the \nType\n on Auto-detect. Click \nStart\n.\n\n\n\n\nOnce the upload status for both sets of files turns \ngreen\n, you can click \nClose\n. You should now be able to see all three files in the Galaxy history panel (right).\n\n\n\n\n\n\nMethod 2: Upload local data to Galaxy.\n (In most cases, you won't need this for the tutorial)\n\n\n\n\nUse this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial.\n\n\nIn the Galaxy tools panel (left), under \nBASIC TOOLS\n, click on \nGet Data\n and choose \nUpload File\n.\n\n\nClick \nChoose local file\n and select the downloaded  FASTQ files. Select \nType\n as \nfastqsanger\n and click \nStart\n.\n\n\nClick \nChoose local file\n again and select the downloaded VCF file. Click \nStart\n.\n\n\nOnce the upload status for all files turns \ngreen\n, you can click \nClose\n. You should now be able to see all three files in the Galaxy history panel (right).\n\n\n\n\n\n\n\n\nRename the datasets\n\n\n\n\nYou should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names.\n\n\nClick on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is \nNA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1\n. Click \nSave\n.\n\n\nSimilarly, rename the second dataset to \nNA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2\n.\n\n\nSimilarly, rename the third dataset to \ndbSNP135_excludingsitesafter129.chr20.vcf\n.\n\n\n\n\n\n\n\n\nSection 1: Quality Control\n\n\nThe aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome!\n\n\n\n\nAnalyse the quality of the reads in the FASTQ file.\n\n\nFrom the left hand tool panel in Galaxy, under \nNGS ANALYSIS\n, select \nNGS: QC and manipulation -> FastQC\n\n\nSelect one of the FASTQ files as input and \nExecute\n the tool.\n\n\nWhen the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics.\n\n\n\n\n\n\n\n\nLook at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).\n\n\nSection 2: Alignment and depth of coverage\n\n\nIn this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome.\n\n\nSome of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects.\n\n\nWe will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment.\n\n\n\n\n\n\nMap/align the reads with Bowtie2 to the human reference genome.\n We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under \nNGS ANALYSIS\n in the tools panel, select the tool \nNGS: Mapping -> Bowtie2\n.\n\n\n\n\nWe have paired-end reads in two FASTQ files, so select \npaired-end\n.\n\n\nSelect the two FASTQ files as inputs.\n\n\nUnder \nSelect reference genome\n select the human genome \nhg19\n.\n\n\nNext we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under \nSet read groups information?\n select \nSet read groups (SAM/BAM specification)\n. (Picard-style should also work.)\n\n\nSet the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier.\n\n\nSet the sample name to \"NA12878\"\n\n\nSet the platform to \nILLUMINA\n\n\nSet the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction.\n\n\n\n\n\n\nYou can leave other read group information blank, and use default Bowtie2 settings. \nExecute\n the tool.\n\n\nWhen the alignment has finished, you should rename the BAM file to something more convenient, such as \nNA12878.chr20_2mb.30xPE.bam\n.\n\n\nNote: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents.\n\n\n\n\n\n\n\n\nVisualise the aligned BAM file with IGV.\n The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server).\n\n\n\n\nNote: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps.\n\n\nIn the green dataset box for your BAM file in the history panel, you will see some \ndisplay with IGV\n links. Launch IGV by clicking the \nweb current\n link. If IGV is already running on your computer, instead click the \nlocal\n link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download.\n\n\nIf\n your BAM file was not automatically loaded, download and open it:\n\n\nDownload the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory.\n\n\nIn IGV, select the correct reference genome, \nhg19\n, in the top-left drop-down menu.\n\n\nIn IGV, open the BAM file using \nFile -> Load from File\n.\n\n\n\n\n\n\nSelect chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box).\n\n\nZoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome.\n\n\nScroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...)\n\n\n\n\n\n\n\n\nRestrict the genomic region considered.\n Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region.\n\n\n\n\nUnder \nBASIC TOOLS\n, select the tool \nText manipulation -> Create single interval\n. Enter these values:\n\n\nChromosome: chr20\n\n\nStart position: 0\n\n\nEnd position: 2000000\n\n\nName: chr20_2mb\n\n\nStrand: plus\n\n\n\n\n\n\nExecute\n this tool. This will create a small BED file specifying just one genomic region.\n\n\nWhen the file is created, rename it to \nchr20_2mb.bed\n. Have a look at the contents of this BED file.\n\n\n\n\n\n\n\n\nEvaluate the depth of coverage of the aligned region.\n Under \nNGS COMMON TOOLSETS\n, select the tool \nNGS: GATK Tools 2.8 -> Depth of Coverage\n.\n\n\n\n\nSelect the BAM file you just generated as the input BAM file.\n\n\nMake sure the reference genome we aligned to is selected under \nUsing reference genome\n.\n\n\nSet \nOutput format\n to \ntable\n.\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nThis tool will produce a lot of files. We are most interested in the summaries. Examine the contents of:\n\n\n\u2018Depth of Coverage on data.... (output summary sample)\u2019:\n this file  will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution.\n\n\nYour mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling.\n\n\nAlso have a quick look at the \n(per locus coverage)\n file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome.\n\n\n\n\n\n\nThe other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.\n\n\n\n\n\n\n\n\nSection 3. Local realignment\n\n\nAlignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome.\n\n\nThis is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels.\n\n\nBoth GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels.\n\n\nIf you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2.\nHowever performing local realignment will improve the accuracy of our variant calls.\n\n\n\n\n\n\nGenerate the list of indel regions to be realigned.\n Under \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -> Realigner Target Creator\n. This tool will look for regions containing potential indels.\n\n\n\n\nSelect your BAM file as input.\n\n\nSelect the correct reference genome (the genome used for alignment).\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nIf you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step.\n\n\n\n\n\n\n\n\nRealign the subsets of reads around the target indel areas.\n Under \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -> Indel Realigner\n. This step will realign reads and generate a new BAM file.\n\n\n\n\nSelect your BAM file as input.\n\n\nSelect the correct reference genome.\n\n\nUnder \nRestrict realignment to provided intervals\n, select the intervals file generated in the previous step.\n\n\nExecute\n.\n\n\nRename the file to something easier to recognise, e.g. \nNA12878.chr20_2mb.30xPE.realigned.bam\n\n\nTo keep your history clean, you may want to delete the log files generated by GATK in the last two steps.\n\n\n\n\n\n\n\n\nCompare the realigned BAM file to original BAM around an indel.\n\n\n\n\nOpen the realigned BAM file in IGV. You can use the \nlocal\n link to open it in your already-running IGV session and compare it to the pre-realignment BAM file.\n\n\nGenerally, the new BAM should appear identical to the old BAM except in the realigned regions.\n\n\nFind some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions).\n\n\nIf you can\u2019t find anything obvious, check region chr20:1163914-1163954;\nyou should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.\n\n\n\n\n\n\n\n\nSection 4. Calling variants with FreeBayes\n\n\nFreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It  can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome.\n\n\nYou can \nread more about FreeBayes here\n.\n\n\n\n\n\n\nCall variants with FreeBayes.\n Under \nNGS ANALYSIS\n, select the tool \nNGS: Variant Analysis -> FreeBayes\n.\n\n\n\n\nSelect your realigned BAM file as input, and select the correct reference genome.\n\n\nUnder \nChoose parameter selection level\n, select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on \nGalaxy-specific options\n.\n\n\nExecute\n FreeBayes.\n\n\nWhen it has run, rename the resulting VCF file to something shorter, such as \nNA12878.FreeBayes.chr20_2mb.vcf\n.\n\n\n\n\n\n\n\n\nCheck the generated list of variants\n.\n\n\n\n\nClick the eye icon to examine the VCF file contents.\n\n\nHow many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)\n\n\nWhat sort of quality scores do your variants have?\n\n\nOpen the VCF file in IGV using the dataset's \ndisplay in IGV local\n link (using the \nweb current\n link will open IGV again, and using \nlocal\n should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.\n\n\n\n\n\n\n\n\nSection 5. Calling variants with GATK Unified Genotyper\n\n\nFor comparison, we will call variants with a second variant caller.\n\n\nThe GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step.\n\n\nThe GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants.\n\n\nYou can \nread more about the GATK here\n.\n\n\n\n\n\n\nCall variants using Unified Genotyper.\n Under \nNGS COMMON TOOLSETS\n, select the tool \nNGS: GATK Tools 2.8 -> Unified Genotyper\n.\n\n\n\n\nSelect your realigned BAM file as input, and select the correct reference genome.\n\n\nUnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under \ndbSNP ROD file\n, select the dataset \ndbsnp135_excludingsitesafter129_chr20.vcf\n.\n\n\nSet \nGenotype likelihoods calculation model to employ\n to \nSNP\n.\n\n\nRestrict the analysis to only the region of interest: Set \nBasic or Advanced GATK options\n to \nAdvanced\n. Click \nInsert Operate on Genomic intervals\n to add a new region and select the chr20_2mb.bed file from your history.\n\n\nExecute\n.\n\n\nRename the file to something useful eg \nNA12878.GATK.chr20_2mb.vcf\n.\n\n\nThe output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files.\n\n\n\n\n\n\n\n\nCheck the generated list of variants.\n\n\n\n\nRoughly how many variants are there in your VCF file (how many lines in the dataset?)\n\n\nClick the eye icon to examine the contents of the VCF file. Notice that the \nID\n column (the third column) is populated with known SNP IDs from the VCF file we provided.\n\n\nNotice that the VCF header rows, and the corresponding information in the \nINFO\n column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the \nINFO\n column, so long as it adds header rows to describe the fields it uses.\n\n\nOpen the VCF file in IGV, using the link in the history panel.\n\n\nFind a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.\n\n\n\n\n\n\n\n\nSection 6. Evaluate variants\n\n\nHow can we evaluate our variants?\n\n\nWe've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample:\n\n\n\n\nWe expect to see true variations at the rate of about 1 per 1000bp against the reference genome\n\n\n85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP)\n\n\nA transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions.\n\n\n\n\nYou can \nread more about SNP call set properties here\n.\n\n\nYou may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs).\n\n\nWe will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on?\n\n\n\n\n\n\nEvaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool.\n\n\n\n\nUnder \nNGS COMMON TOOLSETS\n, select \nNGS: GATK Tools 2.8 -> Eval Variants\n.\n\n\nUnder \nInput variant file\n, select the VCF file you generated with FreeBayes.\n\n\nClick \nInsert Variant\n to add a second input file, and under \nInput variant file\n, select the VCF file you generated with UnifiedGenotyper.\n\n\nSet the reference genome to \nhg19\n.\n\n\nProvide our list of known variants: make sure \nProvide a dbSNP Reference-Ordered Data (ROD) file\n is set to \nSet dbSNP\n and under \ndbSNP ROD file\n select the input reference variant file, \ndbSNP135_excludingsitesafter129.chr20.vcf\n.\n\n\nWe will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set \nBasic or Advanced Analysis options\n to \nAdvanced\n. Then set the following:\n\n\nEval modules to apply on the eval track(s)\n:\n\n\nCompOverlap\n\n\nTiTvVariantEvaluator\n\n\n\n\n\n\nDo not use the standard eval modules by default\n: check this option\n\n\n\n\n\n\nExecute\n.\n\n\n\n\n\n\n\n\nInterpret the dbSNP concordance section of the evaluation report.\n\n\n\n\nExamine the contents of the \u2018Eval Variants on data... (report)\u2019\n\n\nThe first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP.\n\n\nThe \nEvalRod\n column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively.\n\n\nThe \nCompRod\n column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp.\n\n\nNovelty\n: whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing \nall\n variants are the most informative summaries.\n\n\nnEvalVariants\n: number of variant sites in EvalRod (i.e. all called variants).\n\n\nnovelSites\n: number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP).\n\n\nCompOverlap\n: number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP).\n\n\ncompRate\n: percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants).\n\n\nThis metric is important, and it\u2019s what people generally refer to as \ndbSNP concordance\n.\n\n\n\n\n\n\nnConcordant\n and \nconcordantRate\n: number and percentage of overlapping variants that have the same genotype.\n\n\n\n\n\n\n\n\n\n\n\n\nInterpret the TiTv section of the evaluation report.\n\n\n\n\nThe second section of the report lists the transition/transversion ratio for different groups of variants from the callsets.\n\n\nIt generally follows the table format above. The most interesting metric is in the column labelled \ntiTvRatio\n.\n\n\nThe expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation.\n\n\nIn this table, it can be useful to compare not just the rows labelled \nall\n, but also those labelled \nknown\n and \nnovel\n. Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual?\n\n\n\n\n\n\n\n\nHow much overlap is there in the call sets?\n\n\n\n\nOne way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates.\n\n\nRemove the header lines from a VCF file: select the tool \nBASIC TOOLS -> Filter and Sort ->Select\n.\n\n\nAs an input file, in \nSelect lines from\n, select the VCF file you generated using FreeBayes.\n\n\nSelect \nNOT Matching\n.\n\n\nAs the pattern, enter \"^#\". \n^\n in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line.\n\n\nExecute\n. This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have!\n\n\n\n\n\n\nRepeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper.\n\n\nCreate a Venn diagram: select the tool \nSTATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn\n.\n\n\nEnter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\".\n\n\nAs \nInput file 1\n, select the first of the filtered files you just generated.\n\n\nAs \nColumn index\n, enter 1. This is the second column, i.e. the column containing the position coordinate.\n\n\nUnder \nas name\n, enter a name for this input file, e.g. just \"FreeBayes\".\n\n\nAs \nInput file 2\n, select the second of the filtered VCF files.\n\n\nAgain as \nColumn index\n, enter 1.\n\n\nUnder \nas name\n, enter a name for this input file, e.g. just \"UnifiedGenotyper\".\n\n\nExecute\n.\n\n\n\n\n\n\nYou should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram:\n\n\n\"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and \nnot\n UnifiedGenotyper.\n\n\n\"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers.\n\n\n\n\n\n\nYou will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to  remove many false positives, but also a few true positives.\n\n\n\n\n\n\n\n\nSection 7. Annotation\n\n\nThe variants we have detected can be annotated with information from known reference data. This can include, for instance\n\n\n\n\nwhether the variant corresponds to a previously-observed variant in other samples, or across the population\n\n\nwhether the variant is inside or near a known gene\n\n\nwhether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc)\n\n\nwhether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated\n\n\n\n\n... and lots of other information!\n\n\nMost human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant.\n\n\nFor this workshop we will annotate our variants with the \nSnpEff\n tool, which has its own prebuilt annotation databases.\n\n\n\n\n\n\nAnnotate the detected variants.\n Select the tool \nNGS ANALYSIS -> NGS: Annotation -> SnpEff\n.\n\n\n\n\nChoose any of your generated lists of variants as the input VCF file in the first field.\n\n\nSelect \nVCF\n as the output format as well. This will add the annotated information to the INFO column of the VCF file.\n\n\nAs \nGenome source\n, select \nNamed on demand\n. Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed.\n\n\nMake sure \nProduce Summary Stats\n is set to \nYes\n.\n\n\nExecute\n SnpEff.\n\n\n\n\n\n\n\n\nExamine the annotated information.\n Click the eye icon on the resulting VCF file to view it.\n\n\n\n\nYou should see more information in the INFO column for each variant.\n\n\nYou should also see a few extra VCF header rows. In particular notice the new \nINFO=<ID=EFF...\n header row, listing the information added on the predicted effects of each variant.\n\n\nHave a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short \nEFF\n field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information.\n\n\nTry filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool \nBASIC TOOLS -> Filter and Sort -> Select\n on your annotated VCF file and filtering for lines matching the string \"missense_variant\".\n\n\n\n\n\n\n\n\nExamine the annotation summary stats.\n The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file.\n\n\n\n\nYou will see summaries of types of variants, impact of variants, functional regions etc.\n Go through these tables and graphs and see if you understand what they represent.\n\n\nWhere are most of the variants found (in exons, introns etc)? Does this match what you'd expect?",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#variant-detection-advanced-workshop",
            "text": "",
            "title": "Variant Detection - Advanced Workshop"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#tutorial-overview",
            "text": "In this tutorial, we will look further at variant calling from sequence data. We will:   Align NGS read data to a reference genome and perform variant calling, using somewhat different tools to those in the Basic workshop  Carry out local realignment on our aligned reads  Compare the performance of different variant calling tools  Annotate our called variants with reference information",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#background",
            "text": "Some background reading and reference material can be found  here .  The slides used in this workshop can be found  here .  Where is the data in this tutorial from?  The data has been produced from human whole genomic DNA. Only reads that have mapped to a part of chromosome 20 have been used, to make the data suitable for an interactive tutorial. There are about one million 100bp reads in the dataset, produced on an Illumina HiSeq2000. This data was generated as part of the 1000 Genomes project: http://www.1000genomes.org/",
            "title": "Background"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#preparation",
            "text": "Make sure you have an instance of Galaxy ready to go.   If you don't have your own - go to our  Galaxy-Tut  or  Galaxy-Melbourne  server.  Log in so that your work will be saved.\nIf you don't already have an account on this server, select from the menu  User -> Register  and create one.     Import data for the tutorial.    We will import a pair of FASTQ files containing paired-end reads, and a\n  VCF file of known human variants to use for variant evaluation.    Method 1: Paste/Fetch data from a URL to Galaxy.   In the Galaxy tools panel (left), under  BASIC TOOLS , click on  Get Data  and choose  Upload File .  Get the FASTQ files: click  Paste/Fetch data  and enter these URLs into the text box.\nIf you put them in the same upload box, make sure there is a newline between the URLs so that they are really on separate lines. \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2   Select  Type  as  fastqsanger  and click  Start . Note that you cannot use Auto-detect for the type here as there are different subtypes of FASTQ and Galaxy can't be sure which is which.  Get the VCF file: click  Paste/Fetch data  again to open a new text box, and paste the following URL into the box \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/variantCalling_ADVNCD/dbSNP135_excludingsitesafter129_chr20.vcf   This time, you can leave the  Type  on Auto-detect. Click  Start .   Once the upload status for both sets of files turns  green , you can click  Close . You should now be able to see all three files in the Galaxy history panel (right).    Method 2: Upload local data to Galaxy.  (In most cases, you won't need this for the tutorial)   Use this method if you have your own files to upload, or if for any reason you find you need to manually download files for the tutorial.  In the Galaxy tools panel (left), under  BASIC TOOLS , click on  Get Data  and choose  Upload File .  Click  Choose local file  and select the downloaded  FASTQ files. Select  Type  as  fastqsanger  and click  Start .  Click  Choose local file  again and select the downloaded VCF file. Click  Start .  Once the upload status for all files turns  green , you can click  Close . You should now be able to see all three files in the Galaxy history panel (right).     Rename the datasets   You should now have three files in your History, shown in the right-hand panel. If you used Method 1, the name of each dataset will be the full URL we got the file from. For convenience, we will give the datasets shorter names.  Click on the pencil icon to the top right of the dataset name (inside the green box) for the first dataset in your History. Note that the first dataset will be at the bottom! Shorten the name (you can just delete the first part) so that it is  NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_1 . Click  Save .  Similarly, rename the second dataset to  NA12878.hiseq.wgs_chr20_2mb.30xPE.fastq_2 .  Similarly, rename the third dataset to  dbSNP135_excludingsitesafter129.chr20.vcf .",
            "title": "Preparation"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-1-quality-control",
            "text": "The aim here is to evaluate the quality of the short data. If the quality is poor, then adjustments can be made - eg trimming the short reads, or adjusting your expectations of the final outcome!   Analyse the quality of the reads in the FASTQ file.  From the left hand tool panel in Galaxy, under  NGS ANALYSIS , select  NGS: QC and manipulation -> FastQC  Select one of the FASTQ files as input and  Execute  the tool.  When the tool finishes running, you should have an HTML file in your History. Click on the eye icon to view the various quality metrics.     Look at the generated FastQC metrics. This data looks pretty good - high per-base quality scores (most above 30).",
            "title": "Section 1: Quality Control"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-2-alignment-and-depth-of-coverage",
            "text": "In this step we map each of the individual reads in the sample FASTQ readsets to a reference genome, so that we will be able to identify the sequence changes with respect to the reference genome.  Some of the variant callers need extra information regarding the source of reads in order to identify the correct error profiles to use in their statistical variant detection model, so we add more information into the alignment step so that that generated BAM file contains the metadata the variant caller expects.  We will also examine the depth of coverage of the aligned reads across the genome, as a quality check on both the sequencing experiment and the alignment.    Map/align the reads with Bowtie2 to the human reference genome.  We will use Bowtie2, which is one of several good alignment tools for DNA-seq data. Under  NGS ANALYSIS  in the tools panel, select the tool  NGS: Mapping -> Bowtie2 .   We have paired-end reads in two FASTQ files, so select  paired-end .  Select the two FASTQ files as inputs.  Under  Select reference genome  select the human genome  hg19 .  Next we will add read group information. Read groups are usually used when we have reads from multiple experiments, libraries or samples, and want to put them into one aligned BAM file while remembering which read came from which group. In our case we only have one group, but the GATK tools need us to specify a read group in order to work correctly. Under  Set read groups information?  select  Set read groups (SAM/BAM specification) . (Picard-style should also work.)  Set the read group identifier to \"Tutorial_readgroup\". This identifier needs to be a unique identifier for this read group. Since we only have one read group, it doesn't matter much what it is, but a common practice is to construct it out of information guaranteed to be unique, such as the library identifier plus Platform Unit (e.g. flowcell) identifier.  Set the sample name to \"NA12878\"  Set the platform to  ILLUMINA  Set the library name to \"Tutorial_library\". Normally we would set this to identify the DNA library from our DNA extraction.    You can leave other read group information blank, and use default Bowtie2 settings.  Execute  the tool.  When the alignment has finished, you should rename the BAM file to something more convenient, such as  NA12878.chr20_2mb.30xPE.bam .  Note: we assume that you have seen BAM and SAM files before. If you have not you may want to try out the Basic Variant Calling workshop, or take the time now to convert your BAM file to a SAM file and examine the contents.     Visualise the aligned BAM file with IGV.  The Integrated Genome Viewer, IGV, is a very popular tool for visualising aligned NGS data. It will run on your computer (not on the server).   Note: if you are already familiar with IGV, you may want to go through this section quickly, but it's still a good idea to launch IGV for use in later steps.  In the green dataset box for your BAM file in the history panel, you will see some  display with IGV  links. Launch IGV by clicking the  web current  link. If IGV is already running on your computer, instead click the  local  link. If you have problems you can instead launch IGV by visiting https://www.broadinstitute.org/software/igv/download.  If  your BAM file was not automatically loaded, download and open it:  Download the BAM file AND the BAM index (BAI file) by clicking the floppy-disk icon in the green dataset window and selecting each file in turn. Make sure these two files are in the same directory.  In IGV, select the correct reference genome,  hg19 , in the top-left drop-down menu.  In IGV, open the BAM file using  File -> Load from File .    Select chr20 in the IGV chromosomal region drop down box (top of IGV, on the left next to the organism drop down box).  Zoom in to the left hand end of chromosome 20 to see the read alignments - remember our reads only cover the first 2mb of the chromosome.  Scroll around and zoom in and out in the IGV genome viewer to get a feel for genomic data. Note that coverage is variable, with some regions getting almost no coverage (e.g. try chr20:1,870,686-1,880,895 - if you zoom right in to base resolution you\u2019ll see that this region is very GC rich, meaning it\u2019s hard to sequence. Unfortunately it also contains the first few exons of a gene...)     Restrict the genomic region considered.  Later steps can be computationally intensive if performed on the entire genome. We will generate a genomic interval (BED) file that we will use to restrict further analyses to the first 2mb of chromosome 20, as we know our data comes from this region.   Under  BASIC TOOLS , select the tool  Text manipulation -> Create single interval . Enter these values:  Chromosome: chr20  Start position: 0  End position: 2000000  Name: chr20_2mb  Strand: plus    Execute  this tool. This will create a small BED file specifying just one genomic region.  When the file is created, rename it to  chr20_2mb.bed . Have a look at the contents of this BED file.     Evaluate the depth of coverage of the aligned region.  Under  NGS COMMON TOOLSETS , select the tool  NGS: GATK Tools 2.8 -> Depth of Coverage .   Select the BAM file you just generated as the input BAM file.  Make sure the reference genome we aligned to is selected under  Using reference genome .  Set  Output format  to  table .  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  This tool will produce a lot of files. We are most interested in the summaries. Examine the contents of:  \u2018Depth of Coverage on data.... (output summary sample)\u2019:  this file  will tell you the total depth of coverage for your sample across the genome (or in our case, across the first 2mb region we specified). It gives the total and mean coverage, plus some quantiles. This will give you an idea if there is something seriously wrong with your coverage distribution.  Your mean depth here should be ~24x. Note that ~89% of reference bases are covered by at least 15x coverage, which is a sort of informal agreed minimum for reasonable variant calling.  Also have a quick look at the  (per locus coverage)  file. It's not practical to go through this by hand, but you'll see that it gives coverage statistics for every site in the genome.    The other tables give you more detailed statistics on the level of coverage, broken down by regions etc. We don\u2019t really need them so to keep our Galaxy history clean you can delete all the outputs of this step except for the \u2018Depth of Coverage on data.... (output summary sample)\u2019 file. Use the \u2018X\u2019 next to a history file to delete it.",
            "title": "Section 2: Alignment and depth of coverage"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-3-local-realignment",
            "text": "Alignment to large genomes is a compromise between speed and accuracy. Since we usually have (at least) millions of reads, it becomes computationally too expensive to compare reads to one another - instead, high-throughput aligners such as Bowtie align each read individually to the reference genome.  This is often a problem where indels are present, as the aligner will be reluctant to align a read over an indel without sufficient evidence. This can lead to misalignment and to false positive SNVs. We can improve the performance of variant callers by carrying out a further step of \u2018realignment\u2019 after the initial alignment, considering all the reads in a particular genomic region collectively. In particular, this can provide enough collective evidence to realign reads correctly over suspected indels.  Both GATK and FreeBayes will benefit from local realignment around indels. Some tools, such as SamTools Mpileup, have alternative methods to deal with possible misalignment around indels.  If you skip this section, you can still carry out the variant calling steps in later sections by simply using the BAM file from Section 2.\nHowever performing local realignment will improve the accuracy of our variant calls.    Generate the list of indel regions to be realigned.  Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -> Realigner Target Creator . This tool will look for regions containing potential indels.   Select your BAM file as input.  Select the correct reference genome (the genome used for alignment).  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  If you examine the contents of the resulting intervals file, you will see a list of genomic regions to be considered in the next step.     Realign the subsets of reads around the target indel areas.  Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -> Indel Realigner . This step will realign reads and generate a new BAM file.   Select your BAM file as input.  Select the correct reference genome.  Under  Restrict realignment to provided intervals , select the intervals file generated in the previous step.  Execute .  Rename the file to something easier to recognise, e.g.  NA12878.chr20_2mb.30xPE.realigned.bam  To keep your history clean, you may want to delete the log files generated by GATK in the last two steps.     Compare the realigned BAM file to original BAM around an indel.   Open the realigned BAM file in IGV. You can use the  local  link to open it in your already-running IGV session and compare it to the pre-realignment BAM file.  Generally, the new BAM should appear identical to the old BAM except in the realigned regions.  Find some regions with indels that have been realigned (use the \u2018Realigner Target Creator on data... (GATK intervals)\u2019 file from the first step of realignment, it has a list of the realigned regions).  If you can\u2019t find anything obvious, check region chr20:1163914-1163954;\nyou should see that realignment has resulted in reads originally providing evidence of a \u2018G/C\u2019 variant at chr20:1163937 to be realigned with a 10bp insertion at chr20:1163835 and no evidence of the variant.",
            "title": "Section 3. Local realignment"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-4-calling-variants-with-freebayes",
            "text": "FreeBayes is a Bayesian variant caller which assesses the likelihood of each possible genotype for each position in the reference genome, given the observed reads at that position, and reports back the list of variants (positions where a genotype different to homozygous reference was called). It  can be set to aggressively call all possible variants, leaving filtering to the user. FreeBayes generates a variant quality score (as do all variant callers) which can be used for filtering. FreeBayes will also give some phasing information, indicating when nearby variants appear to be on the same chromosome.  You can  read more about FreeBayes here .    Call variants with FreeBayes.  Under  NGS ANALYSIS , select the tool  NGS: Variant Analysis -> FreeBayes .   Select your realigned BAM file as input, and select the correct reference genome.  Under  Choose parameter selection level , select \"Simple diploid calling with filtering and coverage\". This will consider only aligned reads with sufficient mapping quality and base quality. You can see the exact parameters this sets by scrolling down in the main Galaxy window to the Galaxy FreeBayes documentation section on  Galaxy-specific options .  Execute  FreeBayes.  When it has run, rename the resulting VCF file to something shorter, such as  NA12878.FreeBayes.chr20_2mb.vcf .     Check the generated list of variants .   Click the eye icon to examine the VCF file contents.  How many variants exactly are in your list? (Hint: you can look at the number of lines in the dataset, listed in the green box in the History, but remember the top lines are header lines.)  What sort of quality scores do your variants have?  Open the VCF file in IGV using the dataset's  display in IGV local  link (using the  web current  link will open IGV again, and using  local  should use your already-running IGV). This will give an annotation track in IGV to visualise where variants have been called. Compare it to your BAM file.",
            "title": "Section 4. Calling variants with FreeBayes"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-5-calling-variants-with-gatk-unified-genotyper",
            "text": "For comparison, we will call variants with a second variant caller.  The GATK (genome analysis toolkit) is a set of tools from the Broad Institute. It includes the tools for local realignment, used in the previous step.  The GATK UnifiedGenotyper is a Bayesian variant caller and genotyper. You can also use the GATK HaplotypeCaller, which should be available on the GVL server you are using. It takes a little longer to run but is a more recent and sophisticated variant caller that takes into account human haplotype information. Both of these tools are intended primarily for calling diploid, germline variants.  You can  read more about the GATK here .    Call variants using Unified Genotyper.  Under  NGS COMMON TOOLSETS , select the tool  NGS: GATK Tools 2.8 -> Unified Genotyper .   Select your realigned BAM file as input, and select the correct reference genome.  UnifiedGenotyper can automatically label called variants that correspond to known human SNPs, if we provide it with a list of these. We have a VCF file of known SNPs which you imported as input data for this workshop. Under  dbSNP ROD file , select the dataset  dbsnp135_excludingsitesafter129_chr20.vcf .  Set  Genotype likelihoods calculation model to employ  to  SNP .  Restrict the analysis to only the region of interest: Set  Basic or Advanced GATK options  to  Advanced . Click  Insert Operate on Genomic intervals  to add a new region and select the chr20_2mb.bed file from your history.  Execute .  Rename the file to something useful eg  NA12878.GATK.chr20_2mb.vcf .  The output file of interest is the VCF file. If you like, clean up your History by deleting the (log) and (metrics) files.     Check the generated list of variants.   Roughly how many variants are there in your VCF file (how many lines in the dataset?)  Click the eye icon to examine the contents of the VCF file. Notice that the  ID  column (the third column) is populated with known SNP IDs from the VCF file we provided.  Notice that the VCF header rows, and the corresponding information in the  INFO  column, is NOT the same as for the VCF file generated by FreeBayes. In general each variant caller gets to decide what information to put in the  INFO  column, so long as it adds header rows to describe the fields it uses.  Open the VCF file in IGV, using the link in the history panel.  Find a region where GATK has called a variant but FreeBayes hasn\u2019t, and vice versa. Try chr20:1,123,714-1,128,378.",
            "title": "Section 5. Calling variants with GATK Unified Genotyper"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-6-evaluate-variants",
            "text": "How can we evaluate our variants?  We've called variants on normal human DNA, so we expect to find variants with the typical characteristics of human germline variants. We know a lot about variation in humans from many empirical studies, including the 1000Genomes project, so we have some expectations on what we should see when we call variants in a new sample:   We expect to see true variations at the rate of about 1 per 1000bp against the reference genome  85% of variations \u2018rediscovered\u2019 - that is, 85% already known and recorded in dbSNP (% dependent on the version of dbSNP)  A transition/transversion (Ti/Tv) rate of >2 if the variants are high quality, even higher if the variants are in coding regions.   You can  read more about SNP call set properties here .  You may find that each of the variant callers has more variants than we would have expected - we would have expected around 2000 in our 2 megabase region but we see between 3000 and 5000. This is normal for variant calling, where most callers err on the side of sensitivity to reduce false negatives (missed SNVs), expecting the user to do further filtering to remove false positives (false SNVs).  We will also compare the output of our variant callers to one another - how many SNVs have they called in common? How many do they disagree on?    Evaluate dbSNP concordance and Ti/Tv ratio using the GATK VariantEval tool.   Under  NGS COMMON TOOLSETS , select  NGS: GATK Tools 2.8 -> Eval Variants .  Under  Input variant file , select the VCF file you generated with FreeBayes.  Click  Insert Variant  to add a second input file, and under  Input variant file , select the VCF file you generated with UnifiedGenotyper.  Set the reference genome to  hg19 .  Provide our list of known variants: make sure  Provide a dbSNP Reference-Ordered Data (ROD) file  is set to  Set dbSNP  and under  dbSNP ROD file  select the input reference variant file,  dbSNP135_excludingsitesafter129.chr20.vcf .  We will avoid carrying out the full suite of comparisons for this tutorial, and look at a couple of metrics. Set  Basic or Advanced Analysis options  to  Advanced . Then set the following:  Eval modules to apply on the eval track(s) :  CompOverlap  TiTvVariantEvaluator    Do not use the standard eval modules by default : check this option    Execute .     Interpret the dbSNP concordance section of the evaluation report.   Examine the contents of the \u2018Eval Variants on data... (report)\u2019  The first section of the report lists the overlap in variants between the generated VCF files and known human variation sites from dbSNP.  The  EvalRod  column specifies which of the input VCFs is analysed (input_0 = first VCF, input_1 = second etc). For us, these should be FreeBayes and GATK respectively.  The  CompRod  column specifies the set of variants against which the input VCFs are being compared; we have used dbsnp.  Novelty : whether the variants in this row have been found in the supplied dbSNP file or not (known = in dbSNP, novel = not in dbSNP). The rows containing  all  variants are the most informative summaries.  nEvalVariants : number of variant sites in EvalRod (i.e. all called variants).  novelSites : number of variant sites found in EvalRod but not CompRod (i.e. called variants not in dbSNP).  CompOverlap : number of variant sites found in both EvalRod and CompRod (i.e. called variants that ARE in dbSNP).  compRate : percentage of variant sites from EvalRod found in CompRod (=CompOverlap/nEvalVariants).  This metric is important, and it\u2019s what people generally refer to as  dbSNP concordance .    nConcordant  and  concordantRate : number and percentage of overlapping variants that have the same genotype.       Interpret the TiTv section of the evaluation report.   The second section of the report lists the transition/transversion ratio for different groups of variants from the callsets.  It generally follows the table format above. The most interesting metric is in the column labelled  tiTvRatio .  The expectation is a Ti/Tv close to the TiTvRatioStandard of 2.38. Generally, the higher the better, as a high Ti/Tv ratio indicates that most variants are likely to reflect our expectations from what we know about human variation.  In this table, it can be useful to compare not just the rows labelled  all , but also those labelled  known  and  novel . Is the Ti/Tv ratio different for known dbSNP variants, as compared to novel variants in this individual?     How much overlap is there in the call sets?   One way to work out how many of the variants are in common between the call sets is to produce a Venn diagram to visualise the overlap. Since all our variants are on chromosome 20, we will do this by simply comparing the positions of the SNVs. So, we will first remove the VCF headers, leaving us just the variant rows, and then we will compare the variant coordinates.  Remove the header lines from a VCF file: select the tool  BASIC TOOLS -> Filter and Sort ->Select .  As an input file, in  Select lines from , select the VCF file you generated using FreeBayes.  Select  NOT Matching .  As the pattern, enter \"^#\".  ^  in regular expressions indicates the start of the line, so this is a regular expression that says we are detecting lines where there is a \"#\" character at the start of the line.  Execute . This should give you a new dataset, containing just the variant rows. Notice that the number of lines in this dataset now tells you how many variant calls you have!    Repeat the above steps to remove the header lines from VCF file that you generated using GATK UnifiedGenotyper.  Create a Venn diagram: select the tool  STATISTICS AND VISUALISATION -> Graph/Display Data -> proportional venn .  Enter any title you like, e.g. \"FreeBayes vs UnifiedGenotyper\".  As  Input file 1 , select the first of the filtered files you just generated.  As  Column index , enter 1. This is the second column, i.e. the column containing the position coordinate.  Under  as name , enter a name for this input file, e.g. just \"FreeBayes\".  As  Input file 2 , select the second of the filtered VCF files.  Again as  Column index , enter 1.  Under  as name , enter a name for this input file, e.g. just \"UnifiedGenotyper\".  Execute .    You should get an HTML file containing a Venn diagram, containing the overlap of the two sets of SNV calls. The counts below show how many variants are in each part of the diagram:  \"FreeBayes \\ UnifiedGenotyper\" indicates the difference, i.e. how many variants were called by FreeBayes and  not  UnifiedGenotyper.  \"FreeBayes \u2229 UnifiedGenotyper\" indicates the intersection, i.e. how many variants were called by both variant callers.    You will probably find that FreeBayes calls variants more aggressively, and that there are more variants called by FreeBayes and not UnifiedGenotyper than vice versa. We could make FreeBayes calls more specific by filtering on e.g. variant quality score. We'd expect this to  remove many false positives, but also a few true positives.",
            "title": "Section 6. Evaluate variants"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced/#section-7-annotation",
            "text": "The variants we have detected can be annotated with information from known reference data. This can include, for instance   whether the variant corresponds to a previously-observed variant in other samples, or across the population  whether the variant is inside or near a known gene  whether the variant is in a particular kind of genomic region (exon of a gene, UTR, etc)  whether a variant is at a site predicted to cause a pathogenic effect on the gene when mutated   ... and lots of other information!  Most human genes have multiple isoforms, i.e. multiple alternative splicings leading to alternative transcripts. The annotation information we get for a variant in the gene can depend on which transcript is used. In some cases, unless you request only one, you may see multiple alternative annotations for one variant.  For this workshop we will annotate our variants with the  SnpEff  tool, which has its own prebuilt annotation databases.    Annotate the detected variants.  Select the tool  NGS ANALYSIS -> NGS: Annotation -> SnpEff .   Choose any of your generated lists of variants as the input VCF file in the first field.  Select  VCF  as the output format as well. This will add the annotated information to the INFO column of the VCF file.  As  Genome source , select  Named on demand . Then as the genome, enter \"hg19\". This should work on any server, even if the SnpEff annotation database for hg19 has not been previously installed.  Make sure  Produce Summary Stats  is set to  Yes .  Execute  SnpEff.     Examine the annotated information.  Click the eye icon on the resulting VCF file to view it.   You should see more information in the INFO column for each variant.  You should also see a few extra VCF header rows. In particular notice the new  INFO=<ID=EFF...  header row, listing the information added on the predicted effects of each variant.  Have a look through a few variants. Variants not inside or near known genes will not have much annotated information, and may simply have a short  EFF  field listing the variant as being in an \"intergenic region\". Variants in known functional regions of the genome will be annotated with much more information.  Try filtering to view only missense variants (i.e. substitutions which cause an amino acid change) by using the Galaxy tool  BASIC TOOLS -> Filter and Sort -> Select  on your annotated VCF file and filtering for lines matching the string \"missense_variant\".     Examine the annotation summary stats.  The other file SnpEff produces is an HTML document with a summary of the annotations applied to the observed variants. Examine the contents of this file.   You will see summaries of types of variants, impact of variants, functional regions etc.\n Go through these tables and graphs and see if you understand what they represent.  Where are most of the variants found (in exons, introns etc)? Does this match what you'd expect?",
            "title": "Section 7. Annotation"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/",
            "text": "Introduction to Variant detection\n\n\nBackground\n\n\nA variant is something that is different from a standard or type.\n\n\nThe aim of variation detection is to detect how many bases out of the total are different to a reference genome.\n\n\nIn Craig Venter\u2019s genome 4.1 million DNA variants were reported.\n\n\nWhat sort of variation could we find in the DNA sequencing?\n\n\n\n\nSingle nucleotide variations (SNVs)\n\n\nSingle nucleotide polymorphisms (SNPs)\n\n\nSmall insertions and deletions (INDELs)\n\n\nLarge Chromosome rearrangements-Structural variations (SV)\n\n\nCopy number variations (CNV)\n\n\n\n\nVariant Calling vs genotyping\n\n\nVariant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms.\n\n\nVariant callers estimate the probability of a particular genotype given the observed data.\n\n\nThe question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position.\n\n\nThe result of variant calling is a list of probable variants.\n\n\nProcess of variant calling\n\n\nSample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list\n\n\nThe number of reads that stack up on each other is called \nread coverage\n. The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.\n\n\nHomozygous or Heterozygous mutations:\n\n\nWhat should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.\n\n\nVariant Calling Software:\n\n\nThere a number of software available for variant calling some of which are as follows:\n\n\n\n\nSAMtools (mpileup and bcftools): Li 2009 Bioinformatics\n\n\nGATK: McKenna et al. 2010 Genome Res\n\n\nFreeBayes: MIT\n\n\nDiBayes: SOLiD software http://www.lifetechnologies.com\n\n\nInGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009. \nBioinformatics\n\n\nMAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009. \nNature Methods\n\n\n\n\nVariant Calling using Samtools (Mpileup + bcftools)\n\n\nSamtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods.\n\n\nMpileup: Input: BAM file Output: Pileuped up reads under the reference\n\n\nbcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes\n\n\nFurther information\n\n\nVariant Calling using GATK-Unified Genotyper\n\n\nGATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form.\n\n\nGATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods.\n\n\nInputs: BAM file\n\n\nOutput: VCF file with sites and genotypes.\n\n\nThe probability of a variant genotype for a given sequence of data is calculated using the \nBayes Theorem\n as follows:\n\n\nP(Genotype | Data) =  (P(Data | Genotype) * P(Genotype)) / P(Data)\n\n\n\n\nP(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype.\n\n\nP(Data | Genotype) is the probability of the data (the reads) given the genotype\n\n\nP(Data) is the probability of seeing the reads.\n\n\nGATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well.\n\n\nFurther information\n\n\nVariant Calling using FreeBayes\n\n\nFreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files).\n\n\nFurther information\n\n\nEvaluation of detected variants using Variant Eval\n\n\nThe identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set.\n\n\nThe results will have:\n\n\n\n\nTrue Positives (TP): The variants called by the software which are also a known variant in the known variants file.\n\n\nFalse Positives (FP): The Variants called by the software which are not known to be variants in the known variants file.\n\n\nTrue Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file.\n\n\nFalse Negatives (FN): The variants not called by the software which are known as variants in the known variants file.\n\n\n\n\nQuality Matrix:\n\n\nTP | FP\n---|----\nTN | FN\n\n\n\n\nSensitivity: TP/(TP+FN)\n\n\nSpecificity: TN/(TN+FP)\n\n\nNote:\n\n\nAlthough software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity.\n\n\nDNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping.\n\n\n\n\n\n\nTransition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions.\n\n\n\n\n\n\nTransversion: a substitute of a purine for a pyrimidine.\n\n\n\n\n\n\nAlthough there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations.\n\n\nFor more details on variant eval visit: \nhttp://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html\n\n\nNotes:\n\n\nAn important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.\n\n\nLocal realignment\n\n\nIn order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to: \nhttp://samtools.sourceforge.net/mpileup.shtml\n\n\nThe Galaxy workflow platform\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nThe Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\nData Format used in the tutorial\n\n\nSequence Alignment Map format\n\n\nSAM format\n\n\nSequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format:\n\n\n\n\n11 mandatory fields (+ variable number of optional fields)\n\n\n1 QNAME: Query name of the read\n\n\n2 FLAG\n\n\n3 RNAME: Reference sequence name\n\n\n4 POS: Position of alignment in reference sequence\n\n\n5 MAPQ: Mapping quality (Phred-scaled)\n\n\n6 CIGAR: String that describes the specifics of the alignment against the reference\n\n\n7 MRNM\n\n\n8 MPOS\n\n\n9 ISIZE\n\n\n10 SEQQuery: Sequence on the same strand as the reference\n\n\n11 QUAL: Query quality (ASCII-33=Phred base quality)\n\n\n\n\nSAM example\n\n\n    SRR017937.312 16 chr20 43108717 37 76M * 0 0\n    TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG\n    ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@\n\n\n\n\nFor this example:\n\n\n\n\nQNAME = SRR017937.312\n - this is the name of this read\n\n\nFLAG = 16\n - see the format description below\n\n\nRNAME = chr20\n - this read aligns to chromosome 20\n\n\nPOS = 43108717\n - this read aligns the sequence on chr20 at position 43108717\n\n\nMAPQ = 37\n - this is quite a high quality score for the alignment (b/w 0 and 90)\n\n\nCIGAR = 76M\n - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field)\n\n\nMRNM = *\n - see the format description below\n\n\nMPOS = 0\n as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*.\n\n\nISIZE = 0\n as there is no mate for this read\n\n\nSEQQuery =\n the 76bp sequence of the reference segment\n\n\nQUAL =\n per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file\n\n\n\n\nSAM\n format is described more fully \nhere\n\n\nNOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself\n\n\nSee\nhttp://www.illumina.com/technology/paired_end_sequencing_assay.ilmn\n for an overview of paired-end sequencing.\n\n\nSAM file in Galaxy\n\n\n\n\nBinary Sequence Alignment Map format\n\n\nBAM format\n\n\nSAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM.\n\n\nData in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file\n\n\nBAM file in IGV\n\n\n\n\nVCF file format\n\n\nWhat is VCF file:\n\n\nThe \nVariant Call Format\n (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations.\n\n\nVCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official \nVCF spec\n for a more rigorous description of the format.\n\n\n\n\n\n\n\n\nCol\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n1\n\n\nCHROM\n\n\nChromosome name\n\n\n\n\n\n\n2\n\n\nPOS\n\n\n1-based position. For an indel, this is the position preceding the indel.\n\n\n\n\n\n\n3\n\n\nID\n\n\nVariant identifier. Usually the dbSNP rsID.\n\n\n\n\n\n\n4\n\n\nREF\n\n\nReference sequence at POS involved in the variant. For a SNP, it is a single base.\n\n\n\n\n\n\n5\n\n\nALT\n\n\nComma delimited list of alternative sequence(s).\n\n\n\n\n\n\n6\n\n\nQUAL\n\n\nPhred-scaled probability of all samples being homozygous reference.\n\n\n\n\n\n\n7\n\n\nFILTER\n\n\nSemicolon delimited list of filters that the variant fails to pass.\n\n\n\n\n\n\n8\n\n\nINFO\n\n\nSemicolon delimited list of variant information.\n\n\n\n\n\n\n9\n\n\nFORMAT\n\n\nColon delimited list of the format of individual genotypes in the following fields.\n\n\n\n\n\n\n10+\n\n\nSample(s)\n\n\nIndividual genotype information defined by FORMAT.\n\n\n\n\n\n\n\n\nVCF format in Galaxy:\n\n\n\n\nBcf file format:\n\n\n\n\nBCF format:\n\n\nBCF, or the binary variant call format, is the binary version of VCF. It keeps the same information in VCF, while much more efficient to process especially for many samples. The relationship between BCF and VCF is similar to that between BAM and SAM.",
            "title": "Background"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#introduction-to-variant-detection",
            "text": "",
            "title": "Introduction to Variant detection"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#background",
            "text": "A variant is something that is different from a standard or type.  The aim of variation detection is to detect how many bases out of the total are different to a reference genome.  In Craig Venter\u2019s genome 4.1 million DNA variants were reported.  What sort of variation could we find in the DNA sequencing?   Single nucleotide variations (SNVs)  Single nucleotide polymorphisms (SNPs)  Small insertions and deletions (INDELs)  Large Chromosome rearrangements-Structural variations (SV)  Copy number variations (CNV)",
            "title": "Background"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-vs-genotyping",
            "text": "Variant calling is concerned with whether there is evidence of variant in a particular locus whereas genotyping talks about what the sets of alleles in that locus are and their frequencies. In haploid organisms variant calling and genotyping are equivalent whereas the same rule does not apply to other organisms.  Variant callers estimate the probability of a particular genotype given the observed data.  The question one would be asking is what possible genotypes would be possible for a sample. The remaining question is, given that our variant calling process calls a variant, does that mean that there is truly a variant in this locus and also given that the variant caller doesn\u2019t detect a variant in a position does that mean there is no variant in that position.  The result of variant calling is a list of probable variants.",
            "title": "Variant Calling vs genotyping"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#process-of-variant-calling",
            "text": "Sample DNA -> Sequencing -> Read alignment -> BAM file of aligned reads against reference genome -> Genotyper -> Variant list  The number of reads that stack up on each other is called  read coverage . The data is converted into positional information of the reference with the read counts that have piled up under each position. Variant calling will look at how many bases out of the total number of bases is different to the reference at any position.",
            "title": "Process of variant calling"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#homozygous-or-heterozygous-mutations",
            "text": "What should be noted about variants is that they are rare events and homozygous variants are even rarer than heterozygous events.",
            "title": "Homozygous or Heterozygous mutations:"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-software",
            "text": "There a number of software available for variant calling some of which are as follows:   SAMtools (mpileup and bcftools): Li 2009 Bioinformatics  GATK: McKenna et al. 2010 Genome Res  FreeBayes: MIT  DiBayes: SOLiD software http://www.lifetechnologies.com  InGAP: Qi J, Zhao F, Buboltz A, Schuster SC.. 2009.  Bioinformatics  MAQGene: Bigelow H, Doitsidou M, Sarin S, Hobert O. 2009.  Nature Methods",
            "title": "Variant Calling Software:"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-samtools-mpileup-bcftools",
            "text": "Samtools calculates the genotype likelihoods. We then pipe the output to bcftools, which does our SNP calling based on those likelihoods.  Mpileup: Input: BAM file Output: Pileuped up reads under the reference  bcftools: Input: Pileup output from Mpileup Output: VCF file with sites and genotypes  Further information",
            "title": "Variant Calling using Samtools (Mpileup + bcftools)"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-gatk-unified-genotyper",
            "text": "GATK is a programming framework based on the philosophy of MapReduce for developing NGS tools in a distributed or shared memory parallelized form.  GATK unified genotyper uses a Bayesian probabilistic model to calculate genotype likelihoods.  Inputs: BAM file  Output: VCF file with sites and genotypes.  The probability of a variant genotype for a given sequence of data is calculated using the  Bayes Theorem  as follows:  P(Genotype | Data) =  (P(Data | Genotype) * P(Genotype)) / P(Data)  P(Genotype) is the overall probability of that genotype being present in a sequence. This is called the prior probability of a Genotype.  P(Data | Genotype) is the probability of the data (the reads) given the genotype  P(Data) is the probability of seeing the reads.  GATK unified genotyper is not very good in dealing with INDELs and thus we would only calculate SNPs throughout this tutorial. GATK is setup to work with diploid genomes but can be used on haploids as well.  Further information",
            "title": "Variant Calling using GATK-Unified Genotyper"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#variant-calling-using-freebayes",
            "text": "FreeBayes is a high performance, flexible variant caller which uses the open source Freebayes tool to detect genetic variations based high throughput sequencing data (BAM files).  Further information",
            "title": "Variant Calling using FreeBayes"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#evaluation-of-detected-variants-using-variant-eval",
            "text": "The identified variation can further be evaluated against known variations such as common dbSNPs. The result can be checked for high concordance to the common SNPs or a known set of SNPs, the truth set.  The results will have:   True Positives (TP): The variants called by the software which are also a known variant in the known variants file.  False Positives (FP): The Variants called by the software which are not known to be variants in the known variants file.  True Negatives (TN): The variants not called by the software which are not known to be variants in the known variants file.  False Negatives (FN): The variants not called by the software which are known as variants in the known variants file.",
            "title": "Evaluation of detected variants using Variant Eval"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#quality-matrix",
            "text": "TP | FP\n---|----\nTN | FN  Sensitivity: TP/(TP+FN)  Specificity: TN/(TN+FP)  Note:  Although software methods available can find variants in unique regions reliably, the short NGS read length prevent them from detecting variations in repetitive regions with comparable sensitivity.  DNA substitution mutations are of two types: Transitions and Transversions. The Ti/Tv ratio (Transitions/Transversions) is also an indicator of how well the model has performed for genotyping.    Transition: a point mutation in which a purine nucleotide is changed to another purine nucleotide. (A\\<->G) or a pyrimidine nucleotide to another pyrimidine. Approximately 2 out of 3 SNPs are Transitions.    Transversion: a substitute of a purine for a pyrimidine.    Although there are twice as many Transversions as there are Transitions because of the molecular mechanisms by which they are generated, Transition mutations occur at the higher rate than the Transversion mutations.  For more details on variant eval visit:  http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_varianteval_VariantEval.html  Notes:  An important thing worth noting is the more data the better the variant calling. In addition multisampling improves performance.",
            "title": "Quality Matrix:"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#local-realignment",
            "text": "In order to call SNPs close by INDELs correctly, local realignment is strongly recommended before variant calling when using both UnifiedGenotyper and FreeBayes. Samtools mpileup output would not however be affected since it works around this by introducing Base Alignment Quality (BAC). For more information on BAC refer to:  http://samtools.sourceforge.net/mpileup.shtml",
            "title": "Local realignment"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#the-galaxy-workflow-platform",
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here  The Galaxy interface. Tools on the left, data in the middle, analysis workflow on the right.",
            "title": "The Galaxy workflow platform"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#data-format-used-in-the-tutorial",
            "text": "",
            "title": "Data Format used in the tutorial"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#sequence-alignment-map-format",
            "text": "SAM format  Sequence Alignment/Map format records all information relevant to how a set of reads aligns to a reference genome. A SAM file has an optional set of header lines describing the context of the alignment, then one line per read, with the following format:   11 mandatory fields (+ variable number of optional fields)  1 QNAME: Query name of the read  2 FLAG  3 RNAME: Reference sequence name  4 POS: Position of alignment in reference sequence  5 MAPQ: Mapping quality (Phred-scaled)  6 CIGAR: String that describes the specifics of the alignment against the reference  7 MRNM  8 MPOS  9 ISIZE  10 SEQQuery: Sequence on the same strand as the reference  11 QUAL: Query quality (ASCII-33=Phred base quality)   SAM example      SRR017937.312 16 chr20 43108717 37 76M * 0 0\n    TGAGCCTCCGGGCTATGTGTGCTCACTGACAGAAGACCTGGTCACCAAAGCCCGGGAAGAGCTGCAGGAAAAGCCG\n    ?,@A=A\\<5=,@==A:BB@=B9(.;A@B;\\>@ABBB@@9BB@:@5\\<BBBB9)\\>BBB2\\<BBB@BBB?;;BABBBBBBB@  For this example:   QNAME = SRR017937.312  - this is the name of this read  FLAG = 16  - see the format description below  RNAME = chr20  - this read aligns to chromosome 20  POS = 43108717  - this read aligns the sequence on chr20 at position 43108717  MAPQ = 37  - this is quite a high quality score for the alignment (b/w 0 and 90)  CIGAR = 76M  - this read aligns to the reference segment across all bases (76 Matches means no deletions or insertions. Note that 'aligns' can mean 'aligns with mismatches' - mismatches that don't affect the alignment are not recorded in this field)  MRNM = *  - see the format description below  MPOS = 0  as there is no mate for this read - the sequenced DNA library was single ended, not mate paired*.  ISIZE = 0  as there is no mate for this read  SEQQuery =  the 76bp sequence of the reference segment  QUAL =  per-base quality scores for each position on the alignment. This is just a copy of what is in the FASTQ file   SAM  format is described more fully  here  NOTE: reads are shown mapped to the \"sense\" strand of the reference, and bases are listed in 5' -> 3' order. This is important because an actual read might be from the other strand of DNA. The alignment tool will try to map the read as it is, and also the reverse compliment. If it was on the other strand then the reverse compliment is shown in the SAM file, rather than the original read itself  See http://www.illumina.com/technology/paired_end_sequencing_assay.ilmn  for an overview of paired-end sequencing.  SAM file in Galaxy",
            "title": "Sequence Alignment Map format"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#binary-sequence-alignment-map-format",
            "text": "BAM format  SAM is a text format which is not space efficient. Binary Sequence Alignment is a compressed version of SAM.  Data in a BAM file is binary and therefore can't be visualised as text. If you try and visualise in Galaxy, it will default to downloading the file  BAM file in IGV",
            "title": "Binary Sequence Alignment Map format"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-file-format",
            "text": "What is VCF file:  The  Variant Call Format  (VCF) is the emerging standard for storing variant data. Originally designed for SNPs and short INDELs, it also works for structural variations.  VCF consists of a header section and a data section. The header must contain a line starting with one '#', showing the name of each field, and then the sample names starting at the 10th column. The data section is TAB delimited with each line consisting of at least 8 mandatory fields (the first 8 fields in the table below). The FORMAT field and sample information are allowed to be absent. We refer to the official  VCF spec  for a more rigorous description of the format.     Col  Field  Description      1  CHROM  Chromosome name    2  POS  1-based position. For an indel, this is the position preceding the indel.    3  ID  Variant identifier. Usually the dbSNP rsID.    4  REF  Reference sequence at POS involved in the variant. For a SNP, it is a single base.    5  ALT  Comma delimited list of alternative sequence(s).    6  QUAL  Phred-scaled probability of all samples being homozygous reference.    7  FILTER  Semicolon delimited list of filters that the variant fails to pass.    8  INFO  Semicolon delimited list of variant information.    9  FORMAT  Colon delimited list of the format of individual genotypes in the following fields.    10+  Sample(s)  Individual genotype information defined by FORMAT.",
            "title": "VCF file format"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#vcf-format-in-galaxy",
            "text": "",
            "title": "VCF format in Galaxy:"
        },
        {
            "location": "/tutorials/var_detect_advanced/var_detect_advanced_background/#bcf-file-format",
            "text": "",
            "title": "Bcf file format:"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/",
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 2px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.extra {\n        color: #444444;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq - Differential Gene Expression\n\n\nAuthors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a simulated dataset from the common fruit fly, Drosophila\nmelanogaster.\n\n\nThe tutorial is designed to introduce the tools, datatypes and workflows of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\ncontain sequencing and alignment errors that make analysis more difficult.\n\n\nIn this tutorial we will:  \n\n\n\n\nintroduce the types of files typically used in RNA-seq analysis\n\n\nalign RNA-seq reads with an aligner (HISAT2)\n\n\nvisualise RNA-seq alignment data with IGV\n\n\nuse a number of different methods to find differentially expressed genes\n\n\nunderstand the importance of replicates for differential expression analysis\n\n\n\n\nThis tutorial does not cover the following steps that might do in a real\nRNA-seq DGE analysis:  \n\n\n\n\nQC (quality control) of the raw sequence data\n\n\nTrimming the reads for quality and for adaptor sequences\n\n\nQC of the RNA-seq alignment data  \n\n\n\n\nThese steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with basic workflow of alignment, quantification, and testing,\n   for RNA-seq differential expression analysis\n\n\nBe able to process raw RNA sequence data into a list of differentially\n   expressed genes\n\n\nBe aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes\n\n\n\n\n\n\nThe data\n\n\nThe sequencing data you will be working with is simulated from Drosophila\nmelanogaster. The experiment has two conditioins, WT (wildtype) and KO\n(knockout), and three samples in each condition. The sequencing data is\npaired-end, so there are two files for each of the six samples. Your aim will\nbe to find differentially expressed genes in WT vs KO.\n\n\n\n\n\n\nSection 1: Preparation\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser > Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser > Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data > Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Basic_2017\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data > Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\n\n\nUpload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the WT flies. Make sure\n    the type is specified as 'fastqsanger' when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq\n    \n\n    \n\n    These six files are three paired-end samples from the KO flies.\n    Make sure the type is specified as 'fastqsanger' when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq\n    \n\n    \n\n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf\n    \n\n    You should now have 13 files in your history.\n\n\nNote:\n If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019\n\n\n\n\n\n\n3.  View and have an understanding of the files involved in RNA-seq analysis.\n\n\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n6 files containing paired-ended reads for the WT samples:\n\n\n\nWT_01_R1.fastq\n\n\nWT_01_R2.fastq\n\n\nWT_02_R1.fastq\n\n\nWT_02_R2.fastq\n\n\nWT_03_R1.fastq\n\n\nWT_03_R2.fastq\n\n\n\n\n6 files containing paired-ended reads for the KO samples:\n\n\n\nKO_01_R1.fastq\n\n\nKO_01_R2.fastq\n\n\nKO_02_R1.fastq\n\n\nKO_02_R2.fastq\n\n\nKO_03_R1.fastq\n\n\nKO_03_R2.fastq\n\n\n\n\nAnd 1 gene annotation file:\n\n\n\nensembl_dm3.chr4.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\n\n\n\n\nThese 12 sequencing files are in FASTQ format and have the file\n    extension: .fastq. If you are not familiar with the FASTQ format, \nclick\n    here for an overview\n.  \n\n\nEach condition has three samples, and each sample has two files (an R1\nfile containing forward reads and an R2 file containing reverse reads).\n\n\nClick on the \neye icon\n to the top right of any FASTQ file to view the\nfirst part of the file.\n\n\nNote:\n Since the reads in this dataset are synthetic, they do not have\nreal quality scores.\n\n\nNote:\n The reads are paired-end, i.e. WT_01_R1.fastq and\nWT_01_R2.fastq are paired reads from one sequencing run. If you're\nunfamiliar with paired-end sequencing, you can read about it\n\nhere\n.\n\n\n\n\n\n\nThe gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file\n    describes where the genes are located in the D. melanogaster reference\n    genome, filtered for genes on chromosome 4.\n    Each feature is defined by a chromosomal start and end point, feature type\n    (CDS, gene, exon etc), and parent gene and transcript.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n    More information on the GTF format can be found\n    \nhere\n.\n\n\n\n\n\n\n\n\nSection 2: Alignment with HISAT2\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. We will use HISAT to\nperform our alignment. HISAT2 is a fast, splice-aware, alignment program that\nis a successor to TopHat2. More information on\nHISAT2 can be found \nhere\n.\n\n\n1.  Align the RNA-seq reads to a reference genome.\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis > HISAT2\n and set the parameters as follows:  \n\n\n\n\nInput data format\n FASTQ\n\n\nSingle end or paired reads?\n Individual paired reads\n\n\n\n\nForward reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)\n\n\n\n\nWT_01_R1.fastq\n\n\nWT_02_R1.fastq\n\n\nWT_03_R1.fastq\n\n\nKO_01_R1.fastq\n\n\nKO_02_R1.fastq\n\n\nKO_03_R1.fastq\n\n\n\n\n\n\n\n\nReverse reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the reverse\nFASTQ files ending in *2.fastq.)  \n\n\n\n\nWT_01_R2.fastq\n\n\nWT_02_R2.fastq\n\n\nWT_03_R2.fastq\n\n\nKO_01_R2.fastq\n\n\nKO_02_R2.fastq\n\n\nKO_03_R2.fastq\n\n\n\n\n\n\nSource for the reference genome to align against:\n Use\nbuilt-in genome\n\n\nSelect a reference genome:\n D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Examine the alignment stats\n\n\nHISAT2 outputs one bam file for each set of paired-end read files. Rename the 6\nfiles into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam')\nby using the \npen icon\n next to the file.\n\n\nThese files are BAM files (short for\n\nBinary Alignment Map\n)\nand like the name suggests, is a binary file. This means we can't use the\neye icon to view the data in Galaxy; we need to use software that can read the\nfile or convert it into it's plain-text equivalent (SAM) to view it as text.\nIn section 3, we'll use a genome viewer to view our alignments.\n\n\nHISAT2 also outputs some information to stderr which we can preview by\nclicking on the dataset name. To view the raw file, click the \"info\" button\n(view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\"\nrow under \"Job Information\" in the table. Click the \"stderr\" link to view\nthe alignment summary output.\n\n\n16046 reads; of these:\n  16046 (100.00%) were paired; of these:\n    104 (0.65%) aligned concordantly 0 times\n    13558 (84.49%) aligned concordantly exactly 1 time\n    2384 (14.86%) aligned concordantly >1 times\n    ----\n    104 pairs aligned concordantly 0 times; of these:\n      1 (0.96%) aligned discordantly 1 time\n    ----\n    103 pairs aligned 0 times concordantly or discordantly; of these:\n      206 mates make up the pairs; of these:\n        106 (51.46%) aligned 0 times\n        91 (44.17%) aligned exactly 1 time\n        9 (4.37%) aligned >1 times\n99.67% overall alignment rate\n\n\n\n\nHere we see we have a very high alignment rate, which is expected since the\ndata we have is simulated and has no contamination.\n\n\n\n\nSection 3: Visualise the aligned reads\n\n\nThe purpose of this step is to :\n\n\n\n\nvisualise the quantitative, exon-based nature of RNA-seq data\n\n\nvisualise the expression differences between samples represented by the\n  quantity of reads, and\n\n\nbecome familiar with the \nIntegrative Genomics Viewer\n  (IGV)\n-- an interactive\n  visualisation tool by the Broad Institute.  \n\n\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on one of the BAM files, for example 'WT_01.bam'.\n\n\nClick on Display with IGV \n'webcurrent'\n (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)\n\n\nOnce IGV opens, it will show you the BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)\n\n\nSelect \nchr4\n from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)\n\n\n\n\nView differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (WT) and condition 2 (KO).\n\n\nSelect 'KO_02.bam' and click on 'display with IGV local'. This time we are\nusing the \n'local'\n link, as we already have an IGV window up and running\nlocally from the last step. Once the file has loaded, try to find some\ngenes that look differentially expressed.\n\n\n\n\nIf you can't find any, try changing the location to\n\nchr4:816349-830862\n using the field on the top toolbar.\nThe 'Sox102F' gene in this area looks like it has many more reads\nmapped in WT than in KO. Hover over the coverage track to view the read\ndepth of the area. But, of course, it may be that there are many more\nreads in the library for WT than KO. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.\n\n\n\n\n\n\n[Optional]\n Visualise the aligned reads in Trackster\n\nIf you have trouble getting IGV to work, you can also use the inbuilt\nGalaxy genome browser, Trackster, to visualise alignments. Trackster has fewer\nfeatures than IGV, but sometimes it may be more convenient to use as it only\nrequires the browser.\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization > New Track Browser\n.\n\n\nName your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select WT_01.bam and\n    KO_01.bam by using the checkboxes on the left.\n\n\nSelect chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.\n\n\nNext to the drop down list, click on the chromosomal position number\n    display and specify the location \nchr4:816349-830862\n.  \n\n\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 4. Quantification\n\n\nHTSeq-count counts the number of the reads from each bam file that map to the\ngenomic features in the provided annotation file. For each feature (a\ngene for example) we will obtain a numerical value associated with the\nexpression of that feature in our sample (i.e. the number of reads that\nwere aligned to that gene).\n\n\n1.  Examine the GTF file\n\n\nClick on the \neye icon\n to display the ensembl_dm3.chr4.gtf file in Galaxy.\n\n\nThis GTF file is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found\n\nhere\n.\n\n\nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster.\n\n\n2.  Run HTSeq-count\n\n\n\n\n\n\nUse HTSeq-count to count the number of reads for each feature.\n\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis > htseq-count\n and set the parameters as follows:  \n\n\n\n\nAligned SAM/BAM File:\n\n  (Select 'Multiple datasets', then select all six bam files using the shift key.)\n\n\nWT_01.bam\n\n\nWT_02.bam\n\n\nWT_03.bam\n\n\nKO_01.bam\n\n\nKO_02.bam\n\n\nKO_03.bam\n\n\n\n\n\n\nGFF File:\n ensembl_dm3.chr4.gtf\n\n\nStranded:\n No\n\n\nID Attribute:\n gene_name\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nIn the previous step, each input BAM file outputted two files. The first\n    file contains the counts for each of our genes. The second file\n    (ending with \"(no feature)\") contains the stats for the reads that weren't\n    able to be uniquely aligned to a gene. We don't need the \"(no feature)\"\n    files so we can remove then with the delete \"X\" button on the top right.\n\n\n\n\n\n\nRename the remaining six files from htseq-count to meaningful names,\n    such as WT_01, WT_02, etc.\n\n\n\n\n\n\n3.  Generate a count matrix\n\n\n\n\nGenerate a combined count matrix by combining our six files.\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis > Generate count matrix\n and set the parameters as follows:  \n\n\nCount files from your history:\n\n    (Select all six count files using the shift key.)\n\n\nWT_01\n\n\nWT_02\n\n\nWT_03\n\n\nKO_01\n\n\nKO_02\n\n\nKO_03\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nExamine the outputted matrix by using the \neye icon\n.\n\nEach column corresponds to a sample and each row corresponds to a gene. By\nsight, see if you can find a gene you think is differentially expressed\njust by looking at the counts.\n\n\nWe now have a count matrix which we will now use to find differentially\nexpressed genes between WT samples and KO samples.\n\n\n\n\nSection 5. Degust\n\n\nDegust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at \ndegust.erc.monash.edu/\n.\n\n\n\n\n1. Load count data into Degust\n\n\n\n\nIn Galaxy, download the count matrix you generated in the last section\n    using the \ndisk icon\n.\n\n\nGo to \ndegust.erc.monash.edu/\n\n    and click on \"Upload your counts file\".\n\n\nClick \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.\n\n\n\n\n2. Configure your uploaded data\n\n\n\n\nGive your visualisation a name.\n\n\nFor the Info column, select \"gene_id\".\n\n\nAdd two conditions: WT and KO. For each condition, select the three\n    samples which correspond with the condition.\n\n\nSet min gene CPM to 1 in at least 3 samples.\n\n\nClick \nSave changes\n and view your data.\n\n\n\n\n\n\nRead through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.\n\n\n\n\nOn the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.\n\n\nOn the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.\n\n\n4. Explore the demo data\n\n\nDegust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.\n\n\n\n\nSection 5. DESeq2\n\n\nIn this section we'll use the \"DESeq2\" tool in Galaxy to do our differential\ngene analysis. This tool uses the separate HTSeq files we generated in section\n4.\n\n\nSimilar to Voom/Limma or edgeR that was used in Degust to statistically test\nour data, DESeq2 will:\n\n\n\n\nstatistically test for expression differences in normalised read counts for\n    each gene, taking into account the variance observed between samples,  \n\n\nfor each gene, calculate the p-value of the gene being differentially\n    expressed-- this is the probability of seeing the data or something more\n    extreme given the null hypothesis (that the gene is not differentially\n    expressed between the two conditions),\n\n\nfor each gene, estimate the fold change in expression between the two\n    conditions.\n\n\n\n\n\n\n\n\n\nUse DESeq2 to find differentially expressed features from the count data.\n\n    In the left tool panel menu, under NGS Analysis, select \nNGS: RNA Analysis\n    > DESeq2\n and set the parameters as follows:\n\n\n1: Factor\n\n\nSpecify a factor name:\n condition\n\n\n1: Factor level:\n\n\nSpecify a factor level:\n WT\n\n(Select the three WT htseq-count files.)\n\n\nWT_01\n\n\nWT_02\n\n\nWT_03\n\n\n\n\n\n\n\n\n\n\n2: Factor level:\n\n\nSpecify a factor level:\n KO\n\n(Select the three KO htseq-count files.)\n\n\nKO_01\n\n\nKO_02\n\n\nKO_03\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHave a look at the outputs of DESeq2. We will now filter significant\n    (adjusted p-value < 0.05) genes from the DESeq2 result file.\n\n    Under Basic Tools, click on \nFilter and Sort > Filter\n:\n\n\nFilter:\n \"DESeq2 result file on data ...\"\n\n\nWith following condition:\n c7 < 0.05\n\n\nExecute\n\n\n\n\n\n\n\n\nHow many differentially expressed genes with adjusted p-value < 0.05 are there?\n\n\n\n\nSection 6. The importance of replicates\n\n\n\n\n\n\nRepeat the previous differential expression analysis with two samples in\n    each group instead of three. How do you expect your results to differ when\n    using fewer samples?\n\n\n\n\n\n\nFilter genes with adjusted-p-value < 0.05. How many genes are significant?\n\n\n\n\n\n\nRun DESeq2 again, using only one sample from each group. How many genes are\n    now significant?\n\n\n\n\n\n\nCan you find genes that were identified as differentially expressed when\n    using three samples in each condition that were not identified as\n    differentially expressed when using two samples? What do you expect these\n    gene's counts or logFC values to look like compared to genes that remained\n    statistically significance? Have a look at the counts or the logFC values\n    of these genes.\n\n\n\n\n\n\nThe identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.\n\n\nMyoglianin is an example of a gene that showed up as differentially expressed\nwhen we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion.\nIf we say that genes like Myoglianin was \ntruly\n differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.\n\n\nIt is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.\n\n\n\n\nOptional extension\n\n\nHave a go at doing another differential expression analysis with the following\nSaccharomyces cerevisiae data from chromosome I. This time, the two conditions\nare called 'batch' and 'chem', and like before, there are three samples per\ncondition.\n\n\nBatch sequence data:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n\n\n\n\nChem sequence data:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n\n\n\n\nGene annotation:\n\n\nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#rna-seq-differential-gene-expression",
            "text": "Authors: Jessica Chung, Mahtab Mirmomeni, Andrew Lonie",
            "title": "RNA-Seq - Differential Gene Expression"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#tutorial-overview",
            "text": "In this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a simulated dataset from the common fruit fly, Drosophila\nmelanogaster.  The tutorial is designed to introduce the tools, datatypes and workflows of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\ncontain sequencing and alignment errors that make analysis more difficult.  In this tutorial we will:     introduce the types of files typically used in RNA-seq analysis  align RNA-seq reads with an aligner (HISAT2)  visualise RNA-seq alignment data with IGV  use a number of different methods to find differentially expressed genes  understand the importance of replicates for differential expression analysis   This tutorial does not cover the following steps that might do in a real\nRNA-seq DGE analysis:     QC (quality control) of the raw sequence data  Trimming the reads for quality and for adaptor sequences  QC of the RNA-seq alignment data     These steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#learning-objectives",
            "text": "At the end of this tutorial you should:   Be familiar with basic workflow of alignment, quantification, and testing,\n   for RNA-seq differential expression analysis  Be able to process raw RNA sequence data into a list of differentially\n   expressed genes  Be aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes",
            "title": "Learning Objectives"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#the-data",
            "text": "The sequencing data you will be working with is simulated from Drosophila\nmelanogaster. The experiment has two conditioins, WT (wildtype) and KO\n(knockout), and three samples in each condition. The sequencing data is\npaired-end, so there are two files for each of the six samples. Your aim will\nbe to find differentially expressed genes in WT vs KO.",
            "title": "The data"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-1-preparation",
            "text": "1.  Register as a new user in Galaxy if you don\u2019t already have an account   Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User > Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User > Login .   2.  Import the RNA-seq data for the workshop.  If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Basic_2017 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data > Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.   Upload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the WT flies. Make sure\n    the type is specified as 'fastqsanger' when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_01_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_02_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/WT_03_R2.fastq\n     \n     \n    These six files are three paired-end samples from the KO flies.\n    Make sure the type is specified as 'fastqsanger' when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_01_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_02_R2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/KO_03_R2.fastq\n     \n     \n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/rna_seq_basic/ensembl_dm3.chr4.gtf\n     \n    You should now have 13 files in your history.  Note:  If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019    3.  View and have an understanding of the files involved in RNA-seq analysis.    You should now have the following files in your Galaxy history:  6 files containing paired-ended reads for the WT samples:  WT_01_R1.fastq  WT_01_R2.fastq  WT_02_R1.fastq  WT_02_R2.fastq  WT_03_R1.fastq  WT_03_R2.fastq   6 files containing paired-ended reads for the KO samples:  KO_01_R1.fastq  KO_01_R2.fastq  KO_02_R1.fastq  KO_02_R2.fastq  KO_03_R1.fastq  KO_03_R2.fastq   And 1 gene annotation file:  ensembl_dm3.chr4.gtf   These files can be renamed by clicking the  pen icon  if you wish.    These 12 sequencing files are in FASTQ format and have the file\n    extension: .fastq. If you are not familiar with the FASTQ format,  click\n    here for an overview .    Each condition has three samples, and each sample has two files (an R1\nfile containing forward reads and an R2 file containing reverse reads).  Click on the  eye icon  to the top right of any FASTQ file to view the\nfirst part of the file.  Note:  Since the reads in this dataset are synthetic, they do not have\nreal quality scores.  Note:  The reads are paired-end, i.e. WT_01_R1.fastq and\nWT_01_R2.fastq are paired reads from one sequencing run. If you're\nunfamiliar with paired-end sequencing, you can read about it here .    The gene annotation file (ensembl_dm3.chr4.gtf) is in GTF format. This file\n    describes where the genes are located in the D. melanogaster reference\n    genome, filtered for genes on chromosome 4.\n    Each feature is defined by a chromosomal start and end point, feature type\n    (CDS, gene, exon etc), and parent gene and transcript.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n    More information on the GTF format can be found\n     here .",
            "title": "Section 1: Preparation"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-2-alignment-with-hisat2",
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. We will use HISAT to\nperform our alignment. HISAT2 is a fast, splice-aware, alignment program that\nis a successor to TopHat2. More information on\nHISAT2 can be found  here .  1.  Align the RNA-seq reads to a reference genome.  In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > HISAT2  and set the parameters as follows:     Input data format  FASTQ  Single end or paired reads?  Individual paired reads   Forward reads: \n(Click on the  multiple datasets icon  and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)   WT_01_R1.fastq  WT_02_R1.fastq  WT_03_R1.fastq  KO_01_R1.fastq  KO_02_R1.fastq  KO_03_R1.fastq     Reverse reads: \n(Click on the  multiple datasets icon  and select all six of the reverse\nFASTQ files ending in *2.fastq.)     WT_01_R2.fastq  WT_02_R2.fastq  WT_03_R2.fastq  KO_01_R2.fastq  KO_02_R2.fastq  KO_03_R2.fastq    Source for the reference genome to align against:  Use\nbuilt-in genome  Select a reference genome:  D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)  Use defaults for the other fields  Execute    Note: This may take a few minutes, depending on how busy the server is.  2.  Examine the alignment stats  HISAT2 outputs one bam file for each set of paired-end read files. Rename the 6\nfiles into a more meaningful name (e.g. 'HISAT on data 2 and data 1' to 'WT_01.bam')\nby using the  pen icon  next to the file.  These files are BAM files (short for Binary Alignment Map )\nand like the name suggests, is a binary file. This means we can't use the\neye icon to view the data in Galaxy; we need to use software that can read the\nfile or convert it into it's plain-text equivalent (SAM) to view it as text.\nIn section 3, we'll use a genome viewer to view our alignments.  HISAT2 also outputs some information to stderr which we can preview by\nclicking on the dataset name. To view the raw file, click the \"info\" button\n(view details) of a dataset, say WT_01.bam, and find the \"Tool Standard Error\"\nrow under \"Job Information\" in the table. Click the \"stderr\" link to view\nthe alignment summary output.  16046 reads; of these:\n  16046 (100.00%) were paired; of these:\n    104 (0.65%) aligned concordantly 0 times\n    13558 (84.49%) aligned concordantly exactly 1 time\n    2384 (14.86%) aligned concordantly >1 times\n    ----\n    104 pairs aligned concordantly 0 times; of these:\n      1 (0.96%) aligned discordantly 1 time\n    ----\n    103 pairs aligned 0 times concordantly or discordantly; of these:\n      206 mates make up the pairs; of these:\n        106 (51.46%) aligned 0 times\n        91 (44.17%) aligned exactly 1 time\n        9 (4.37%) aligned >1 times\n99.67% overall alignment rate  Here we see we have a very high alignment rate, which is expected since the\ndata we have is simulated and has no contamination.",
            "title": "Section 2: Alignment with HISAT2"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-3-visualise-the-aligned-reads",
            "text": "The purpose of this step is to :   visualise the quantitative, exon-based nature of RNA-seq data  visualise the expression differences between samples represented by the\n  quantity of reads, and  become familiar with the  Integrative Genomics Viewer\n  (IGV) -- an interactive\n  visualisation tool by the Broad Institute.     To visualise the alignment data:   Click on one of the BAM files, for example 'WT_01.bam'.  Click on Display with IGV  'webcurrent'  (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)  Once IGV opens, it will show you the BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)  Select  chr4  from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)   View differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (WT) and condition 2 (KO).  Select 'KO_02.bam' and click on 'display with IGV local'. This time we are\nusing the  'local'  link, as we already have an IGV window up and running\nlocally from the last step. Once the file has loaded, try to find some\ngenes that look differentially expressed.   If you can't find any, try changing the location to chr4:816349-830862  using the field on the top toolbar.\nThe 'Sox102F' gene in this area looks like it has many more reads\nmapped in WT than in KO. Hover over the coverage track to view the read\ndepth of the area. But, of course, it may be that there are many more\nreads in the library for WT than KO. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.    [Optional]  Visualise the aligned reads in Trackster \nIf you have trouble getting IGV to work, you can also use the inbuilt\nGalaxy genome browser, Trackster, to visualise alignments. Trackster has fewer\nfeatures than IGV, but sometimes it may be more convenient to use as it only\nrequires the browser.   On the top bar of Galaxy, select  Visualization > New Track Browser .  Name your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select WT_01.bam and\n    KO_01.bam by using the checkboxes on the left.  Select chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.  Next to the drop down list, click on the chromosomal position number\n    display and specify the location  chr4:816349-830862 .     Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.",
            "title": "Section 3: Visualise the aligned reads"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-4-quantification",
            "text": "HTSeq-count counts the number of the reads from each bam file that map to the\ngenomic features in the provided annotation file. For each feature (a\ngene for example) we will obtain a numerical value associated with the\nexpression of that feature in our sample (i.e. the number of reads that\nwere aligned to that gene).  1.  Examine the GTF file  Click on the  eye icon  to display the ensembl_dm3.chr4.gtf file in Galaxy.  This GTF file is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found here . \nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster.  2.  Run HTSeq-count    Use HTSeq-count to count the number of reads for each feature. \n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis > htseq-count  and set the parameters as follows:     Aligned SAM/BAM File: \n  (Select 'Multiple datasets', then select all six bam files using the shift key.)  WT_01.bam  WT_02.bam  WT_03.bam  KO_01.bam  KO_02.bam  KO_03.bam    GFF File:  ensembl_dm3.chr4.gtf  Stranded:  No  ID Attribute:  gene_name  Use defaults for the other fields  Execute     In the previous step, each input BAM file outputted two files. The first\n    file contains the counts for each of our genes. The second file\n    (ending with \"(no feature)\") contains the stats for the reads that weren't\n    able to be uniquely aligned to a gene. We don't need the \"(no feature)\"\n    files so we can remove then with the delete \"X\" button on the top right.    Rename the remaining six files from htseq-count to meaningful names,\n    such as WT_01, WT_02, etc.    3.  Generate a count matrix   Generate a combined count matrix by combining our six files.\n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis > Generate count matrix  and set the parameters as follows:    Count files from your history: \n    (Select all six count files using the shift key.)  WT_01  WT_02  WT_03  KO_01  KO_02  KO_03    Use defaults for the other fields  Execute     Examine the outputted matrix by using the  eye icon . \nEach column corresponds to a sample and each row corresponds to a gene. By\nsight, see if you can find a gene you think is differentially expressed\njust by looking at the counts.  We now have a count matrix which we will now use to find differentially\nexpressed genes between WT samples and KO samples.",
            "title": "Section 4. Quantification"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-degust",
            "text": "Degust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at  degust.erc.monash.edu/ .   1. Load count data into Degust   In Galaxy, download the count matrix you generated in the last section\n    using the  disk icon .  Go to  degust.erc.monash.edu/ \n    and click on \"Upload your counts file\".  Click \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.   2. Configure your uploaded data   Give your visualisation a name.  For the Info column, select \"gene_id\".  Add two conditions: WT and KO. For each condition, select the three\n    samples which correspond with the condition.  Set min gene CPM to 1 in at least 3 samples.  Click  Save changes  and view your data.    Read through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.   On the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.  On the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.  4. Explore the demo data  Degust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.",
            "title": "Section 5. Degust"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-5-deseq2",
            "text": "In this section we'll use the \"DESeq2\" tool in Galaxy to do our differential\ngene analysis. This tool uses the separate HTSeq files we generated in section\n4.  Similar to Voom/Limma or edgeR that was used in Degust to statistically test\nour data, DESeq2 will:   statistically test for expression differences in normalised read counts for\n    each gene, taking into account the variance observed between samples,    for each gene, calculate the p-value of the gene being differentially\n    expressed-- this is the probability of seeing the data or something more\n    extreme given the null hypothesis (that the gene is not differentially\n    expressed between the two conditions),  for each gene, estimate the fold change in expression between the two\n    conditions.     Use DESeq2 to find differentially expressed features from the count data. \n    In the left tool panel menu, under NGS Analysis, select  NGS: RNA Analysis\n    > DESeq2  and set the parameters as follows:  1: Factor  Specify a factor name:  condition  1: Factor level:  Specify a factor level:  WT \n(Select the three WT htseq-count files.)  WT_01  WT_02  WT_03      2: Factor level:  Specify a factor level:  KO \n(Select the three KO htseq-count files.)  KO_01  KO_02  KO_03      Use defaults for the other fields  Execute         Have a look at the outputs of DESeq2. We will now filter significant\n    (adjusted p-value < 0.05) genes from the DESeq2 result file. \n    Under Basic Tools, click on  Filter and Sort > Filter :  Filter:  \"DESeq2 result file on data ...\"  With following condition:  c7 < 0.05  Execute     How many differentially expressed genes with adjusted p-value < 0.05 are there?",
            "title": "Section 5. DESeq2"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#section-6-the-importance-of-replicates",
            "text": "Repeat the previous differential expression analysis with two samples in\n    each group instead of three. How do you expect your results to differ when\n    using fewer samples?    Filter genes with adjusted-p-value < 0.05. How many genes are significant?    Run DESeq2 again, using only one sample from each group. How many genes are\n    now significant?    Can you find genes that were identified as differentially expressed when\n    using three samples in each condition that were not identified as\n    differentially expressed when using two samples? What do you expect these\n    gene's counts or logFC values to look like compared to genes that remained\n    statistically significance? Have a look at the counts or the logFC values\n    of these genes.    The identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.  Myoglianin is an example of a gene that showed up as differentially expressed\nwhen we did a 3 vs 3 comparsion but not with a 2 vs 2 comparsion.\nIf we say that genes like Myoglianin was  truly  differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.  It is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.",
            "title": "Section 6. The importance of replicates"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tutorial/#optional-extension",
            "text": "Have a go at doing another differential expression analysis with the following\nSaccharomyces cerevisiae data from chromosome I. This time, the two conditions\nare called 'batch' and 'chem', and like before, there are three samples per\ncondition.  Batch sequence data: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq  \nChem sequence data: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq  \nGene annotation: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf",
            "title": "Optional extension"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/",
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq Differential Gene Expression: Basic Tutorial\n\n\nAuthors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a small synthetic dataset from the model organism,\nDrosophila melanogaster.\n\n\nThe tutorial is designed to introduce the tools, datatypes and workflow of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\nwould contain sequencing and alignment errors that make analysis more difficult.\n\n\nOur input data for this tutorial will be raw RNA-seq reads from two\nexperimental conditions and we will output a list of differentially\nexpressed genes identified to be statistically significant.\n\n\nIn this tutorial we will:  \n\n\n\n\nintroduce the types of files typically used in RNA-seq analysis\n\n\nalign RNA-seq reads with Tophat\n\n\nvisualise RNA-seq alignment data with IGV\n\n\nfind differentially expressed genes with Cuffdiff\n\n\nunderstand the importance of replicates for differential expression analysis\n\n\n\n\nThis tutorial does not cover the following steps that you would do in a real\nRNA-seq DGE analysis:  \n\n\n\n\nQC (quality control) of the raw sequence data\n\n\nTrimming the reads for quality and for adaptor sequences\n\n\nQC of the RNA-seq alignment data  \n\n\n\n\nThese steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.\n\n\n\n\nLearning Objectives\n\n\nAt the end of this tutorial you should:\n\n\n\n\nBe familiar with the Tuxedo Protocol workflow for RNA-seq differential\n   expression analysis\n\n\nBe able to process raw RNA sequence data into a list of differentially\n   expressed genes\n\n\nBe aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes\n\n\n\n\n\n\nBackground\n\n\nWhere does the data in this tutorial come from?\n\n\nThe data for this tutorial is from an RNA-seq experiment looking for\ndifferentially expressed genes in D. melanogaster (fruit fly) between two\nexperimental conditions. The experiment and analysis protocol we will follow\nis derived from a paper in Nature Protocols by the research group responsible\nfor one of the most widely used set of RNA-seq analysis tools: \n\"Differential\ngene and transcript expression analysis of RNA-seq experiments with TopHat and\nCufflinks\"\n (Trapnell et al 2012).\n\n\nThe sequence datasets are single-end Illumina synthetic short reads,\nfiltered to only include chromosome 4 to facilitate faster mapping (which\nwould otherwise take hours). We\u2019ll use data from three biological replicates\nfrom each of the two experimental conditions.\n\n\nThe Tuxedo Protocol\n\n\nThe workflow this tutorial is based on is the Tuxedo Protocol. Reads are first\nmapped with TopHat and a transcriptome is then assembled using Cufflinks.\nCuffdiff then quantifies the expression in each condition, and tests for\ndifferential expression.\n\n\nIn this tutorial we use a simpler protocol as the D. melanogaster\ntranscriptome is already very well characterised.\n\n\nMore information about the Tuxedo protocol can be found \nhere\n.\n\n\n\n\nSection 1: Preparation [15 min]\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account (\nwhat is Galaxy?\n)\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser > Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser > Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data > Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Basic_Sec_1\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data > Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\nUpload the sequence data by pasting the following links into the text\n    input area:\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq\n    \n\n    \n\n    Select the type as 'fastqsanger' and press \nstart\n to upload the\n    files to Galaxy.\n\n\nUpload the annotated gene list reference by pasting the following link\n    into the text input area:\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf\n    \n\n    You don't need to specify the type for this file as Galaxy will\n    auto-detect the file as a GTF file.\n\n\n\n\n3.  View and have an understanding of the files involved in RNA-seq analysis.\n\n\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n6 files containing single-ended reads:\n\n\n\nC1_R1.chr4.fq\n\nC1_R2.chr4.fq\n\nC1_R3.chr4.fq\n\nC2_R1.chr4.fq\n\nC2_R2.chr4.fq\n\nC2_R3.chr4.fq\n\n\n\n\nAnd 1 gene annotation file:\n\n\n\nensembl_dm3.chr4.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\n\n\n\n\nThese 6 sequencing files are in FASTQ format and have the file\n    extension: .fq. If you are not familiar with the FASTQ format, \nclick\n    here for an overview\n.  \n\n\nClick on the \neye icon\n to the top right of each FASTQ file to view the\nfirst part of the file.\nThe first 3 files are from the first condition (C1) and has 3\nreplicates labelled R1, R2, and R3. The next 3 FASTQ files are from\nthe second condition (C2) and has 3 replicates labelled R1, R2, and R3.\n\n\nIn this tutorial, we aim to find genes which are differentially\nexpressed between condition C1 and condition C2.\n\n\n\n\n\n\nThe gene annotation file is in GTF format. This file describes where\n    the genes are located in the Drosophila reference genome.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.\n\n\n\n\n\n\nNOTE:\n Since the reads in this dataset are synthetic, they do not have\nreal quality scores.\n\n\nNOTE:\n If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019\n\n\n\n\nSection 2: Align reads with Tophat [30 mins]\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found \nhere\n.\n\n\n1.  Align the RNA-seq short reads to a reference genome.\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis > Tophat\n and set the parameters as follows:  \n\n\n\n\nIs this single-end or paired-end data?\n Single-end  \n\n\nRNA-Seq FASTQ file:\n\n  (Click on the multiple datasets icon and select all six of the FASTQ\n  files. This can be done by holding down the shift key to select a range\n  of files, or holding down the ctrl key (Windows) or command key (OSX) and\n  clicking to select multiple files.)\n\n\nC1_R1.chr4.fq\n\n\nC1_R2.chr4.fq\n\n\nC1_R3.chr4.fq\n\n\nC2_R1.chr4.fq\n\n\nC2_R2.chr4.fq\n\n\nC2_R3.chr4.fq  \n\n\n\n\n\n\nUse a built in reference genome or own from your history:\n Use\n  built-in genome\n\n\nSelect a reference genome:\n D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n//<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Examine the output files\n\n\nYou should have 5 output files for each of the FASTQ input files:\n\n\n\n\nTophat on data 1: accepted_hits:\n This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.\n\n\nTophat on data 1: splice junctions:\n This file lists all the places\n  where Tophat had to split a read into two pieces to span an exon\n  junction.\n\n\nTophat on data 1: deletions\n and \nTophat on data 1: insertions:\n\n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.\n\n\nTophat on data 1: align_summary:\n This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.\n\n\n\n\nYou should have a total of 30 Tophat output files in your history.\n\n\n3.  Visualise the aligned reads with IGV\n\n\nThe purpose of this step is to :\n\n\n\n\nvisualise the quantitative, exon-based nature of RNA-seq data\n\n\nvisualise the expression differences between samples represented by the\n  quantity of reads, and\n\n\nbecome familiar with the \nIntegrative Genomics Viewer\n  (IGV)\n-- an interactive\n  visualisation tool by the Broad Institute.  \n\n\n\n\nTo visualise the alignment data:\n\n\n\n\nClick on one of the Tophat accepted hits files, for example 'Tophat on\n    data 1: accepted_hits'.\n\n\nClick on Display with IGV \n'webcurrent'\n (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)\n\n\nOnce IGV opens, it will show you the accepted_hits BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)\n\n\nSelect \nchr4\n from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)\n\n\nView one of the splice function files such as 'TopHat on data 1: splice\n    junctions'. You will need to save this file to your local disk\n    using the \ndisk icon\n under the details of the file. Then open\n    the saved .bed file directly in IGV using the \nFile > Load From File\n\n    option from IGV. This is because IGV doesn\u2019t automatically stream BED\n    files from Galaxy.\n    \n\n    The junctions file is loaded at the bottom of the\n    IGV window and splicing events are represented as coloured arcs. The\n    height and thickness of the arcs are proportional to the read depth.\n\n\n\n\nView differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (C1) and condition 2 (C2).\n\n\nSelect 'TopHat on data 4: accepted_hits' (this is the accepted hits\nalignment file from first replicate of condition C2) and click on\n'display with IGV local'. This time we are using the \n'local'\n link, as\nwe already have an IGV window up and running locally from the last\nstep. One the file has loaded, change the location to\n\nchr4:325197-341887\n using the field on the top toolbar.\n\n\nThe middle gene in this area clearly looks like it has many more reads\nmapped in condition 2 than condition 1, whereas for the surrounding\ngenes the reads look about the same. The middle gene looks like it is\ndifferentially expressed. But, of course, it may be that there are\nmany more reads in the readsets for C1 and C2, and the other genes\nare underexpressed in condition 2. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.\n\n\n\n\n\n\n4.  \n[Optional]\n Visualise the aligned reads in Trackster\n\n\nWe can also use the inbuilt Galaxy genome browser, Trackster, to visualise\nalignments. Trackster has fewer features than IGV, but sometimes it may be\nmore convenient to use as it only requires the browser.\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization > New Track Browser\n.\n\n\nName your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select Tophat on\n    data 1: accepted_hits and Tophat on data 4: accepted_hits by using the\n    checkboxes on the left.\n\n\nSelect chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.\n\n\nNext to the drop down list, click on the chromosomal position number\n    display and specify the location \nchr4:325197-341887\n.  \n\n\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 3: Test differential expression with Cuffdiff [45 min]\n\n\nThe aim in this section is to:\n\n\n\n\ngenerate tables of normalised read counts per gene per condition based on the\n  annotated reference transcriptome,\n\n\nstatistically test for expression differences in normalised read counts for\n  each gene, taking into account the variance observed between samples,  \n\n\nfor each gene, calculate the p-value of the gene being differentially\n  expressed-- this is the probability of seeing the data or something more\n  extreme given the null hypothesis (that the gene is not differentially\n  expressed between the two conditions),\n\n\nfor each gene, estimate the fold change in expression between the two\n  conditions.\n\n\n\n\nAll these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff\nis part of the Cufflinks software suite which takes the aligned reads from\nTophat and generates normalised read counts and a list of differentially\nexpressed genes based on a reference transcriptome - in this case, the curated\nEnsembl list of D. melanogaster genes from chromosome 4 that we supply as a\nGTF (Gene Transfer Format) file.\nA more detailed explanation of Cufflinks DGE testing can be found\n\nhere\n.\n\n\n1.  Examine the reference transcriptome\n\n\nClick on the \neye icon\n to display the ensembl_dm3.chr4.gtf reference\ntranscriptome file in Galaxy.\n\n\nThe reference transcriptome is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found\n\nhere\n.\n\n\nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff\nuses the reference transcriptome to aggregate read counts per gene,\ntranscript, transcription start site and coding sequence (CDS). For this\ntutorial, we\u2019ll only consider differential gene testing, but it is also\npossible to test for differential expression of transcripts or\ntranscription start sites.\n\n\n2.  Run Cuffdiff to identify differentially expressed genes and transcripts\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis > Cuffdiff\n and set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n\n\nTophat on data 1: accepted_hits\n\n\nTophat on data 2: accepted_hits\n\n\nTophat on data 3: accepted_hits\n\n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key for Windows or the command key for OSX.)\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n\n\nTophat on data 4: accepted_hits\n\n\nTophat on data 5: accepted_hits\n\n\nTophat on data 6: accepted_hits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n3.  Explore the Cuffdiff output files\n\n\nThere should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 37, data 32, and others\".\n\n\nFPKM tracking files:\n  \n\n\n\n\ntranscript FPKM tracking\n\n\ngene FPKM tracking\n\n\nTSS groups FPKM tracking\n\n\nCDS FPKM tracking  \n\n\n\n\nThese 4 files contain the FPKM (a unit of normalised expression taking\ninto account the transcript length for each transcript\nand the library size of the sample) for each of the two conditions.\n\n\nDifferential expression testing files:\n  \n\n\n\n\ngene differential expression testing\n\n\ntranscript differential expression testing\n\n\nTSS groups differential expression testing\n\n\nCDS FPKM differential expression testing\n\n\nCDS overloading diffential expression testing\n\n\npromoters differential expression testing\n\n\nsplicing differential expression testing\n\n\n\n\nThese 7 files contain the statistical\nresults from testing the level of expression between condition C1 and condition C2.\n\n\n\n\n\n\nExamine the tables of normalised gene counts\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM\n    tracking\" by clicking on the \neye icon\n. The file consists of one row\n    for each gene from the reference transcriptome, with columns containing the\n    normalised read counts for each of the two conditions.\n    Note:  \n\n\n\n\nCuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates\n  to a gene. Multiple transcription start sites are aggregated under a\n  single tracking_id.\n\n\nA gene encompasses a chromosomal locus which covers all the features\n  that make up that gene (exons, introns, 5\u2019 UTR, etc).\n\n\n\n\n\n\n\n\nInspect the gene differential expression testing file\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene\n    differential expression testing\" by clicking on the \neye icon\n. The\n    columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10),\n    p_value (c12), q_value (c13) and significant (c14).\n\n\n\n\n\n\nFilter based on column 14 (\u2018significant\u2019) - a binary assessment of\n    q_value > 0.05, where q_value is p_value adjusted for multiple testing.\n    Under Basic Tools, click on \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nThis will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed.\n\nWe can rename this file (screenshot) by clicking on the \npencil icon\n of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Significant_DE_Genes\".\n\n\n\n\n\n\nExamine the sorted list of differentially expressed genes.\n    Click on the \neye icon\n next to \"Significant_DE_Genes\" to view the data.\n\n\n\n\n\n\n\n    \nHow many genes are in the Significant_DE_Genes file? What are their names?\n\n    \n\n    Two genes have been identified as differentially expressed between\n    conditions C1 and C2:\n    \n\n        \nAnk\n located at chr4:137014-150378, and\n        \nCG2177\n located at chr4:331557-334534\n    \n\n    Both genes have q-values of 0.00175.\n\n\n\n\n\"CG2177\" located at chr4:331557-334534 was the gene that we intuitively (with IGV)\nsaw to be differentially expressed in the previous section, in the broader\nregion of chr4:325197-341887.\n\n\n\n\nSection 4. Repeat without replicates [20 min]\n\n\nIn this section, we will run Cuffdiff with fewer replicates.\n\n\n\n    \nStop and think:\n\n    Why do we need replicates for an RNA-seq differential gene expression\n    experiment? What do you expect to happen if we only use one sample from\n    each condition for our analysis?\n\n\n\n\n\n\n\n\nRepeat the differential gene expression testing from section 2, but this\n    time only use one replicate from each condition group (C1 and C2).\n\n    From the Galaxy tool panel, select \nNGS: RNA Analysis > Cuffdiff\n and\n    set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n Tophat on data 1: accepted_hits\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n Tophat on data 4: accepted_hits\n\n\n\n\n\n\n\n\n\n\nLibrary normalization method:\n classic-fpkm\n\n\nDispersion estimation method:\n blind\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_C1_R1_vs_C2_R1\"\n\n\n\n\n\n\nClick on the \neye icon\n of Significant_DE_Genes_C1_R1_vs_C2_R1.\n\n    You should get \nno\n differentially expressed genes at statistical\n    significance of 0.05. The \"Ank\" gene and the \"CG1277\", which were found to\n    be significantly differentially expressed in our first analysis, are not\n    identified as differentially expressed when we only use one sample for each\n    condition.\n\n\n\n\n\n\nRepeat this no-replicates analysis, but this time specify a different set\n    of samples.\n    From the Galaxy tool panel, select \nNGS: RNA Analysis > Cuffdiff\n and\n    set the parameters as follows:  \n\n\n\n\nTranscripts:\n ensembl_dm3.chr4.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n Tophat on data 1: accepted_hits\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n Tophat on data 5: accepted_hits\n\n\n\n\n\n\n\n\n\n\nLibrary normalization method:\n classic-fpkm\n\n\nDispersion estimation method:\n blind\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_C1_R1_vs_C2_R2\"\n\n\n\n\n\n\nClick on the \neye icon\n of Significant_DE_Genes_C1_R1_vs_C2_R2.\n\n    We now see \"CG2177\" appear again in the list as significantly\n    differentially expressed, but not \"Ank\".\n\n\n\n\n\n\n\n    \nHow can we interpret the difference in results from using different\n    replicates? \n\n    \n\n    There is a larger absolute difference in CG1277 expression between\n    samples 1 (C1_R1) and 5 (C2_R2) than samples 1 (C1_R1) and 4 (C2_R1), hence\n    Cuffdiff identifies CG1277 as differentially expressed between C1_R1 and\n    C2_R2, but not between C1_R1 and C2_R1.\n    \n\n    On the other hand, differences in level of expression of Ank is much smaller\n    between samples, so we need to see it consistently across multiple replicates\n    for Cuffdiff to be confident it actually exists. One replicate is not enough.\n\n\n\n\nThe identification of differentially expressed genes is based on the size\nof the difference in expression and the variance observed across multiple\nreplicates. This demonstrates how important it is to have biological replicates\nin differential gene expression experiments.\n\n\nIf we say that genes Ank and CG2177 are \ntruly\n differentially expressed, we\ncan call these instances where the true differentially expressed genes are not\nidentified as false negatives. Generally, increasing replicates decreases the\nnumber of false negatives.\n\n\nIt is also more likely to see more false positives when using an insufficient\nnumber of replicates. False positives can be defined as identifiying a gene as\ndifferentially expressed when it is, in reality, not.\n\n\n[Optional step]\n\nRepeat this analysis, specifying groups of two replicates each. What do you\nget? How many replicates do we need to identify Ank as differentially\nexpressed?\n\n\n\n\nSection 5. Optional Extension [20 min]\n\n\nExtension on the Tuxedo Protocol\n\n\nThe full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge,\nand CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by\nidentifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow\nyou to identify new genes and transcripts, and then analyse them for\ndifferential expression. This is critical for organisms in which the\ntranscriptome is not well characterised. CummeRbund helps visualise the data\nproduced from the Cuffdiff using the R statistical programming language.\n\n\nRead more on the full Tuxedo protocol \nhere\n.\n\n\nIf the organism we were working on did not have a well characterized reference\ntranscriptome, we would run Cufflinks and Cuffmerge to create a transcriptome.\n\n\n\n\n\n\nSuppose we didn't have our Drosophila GTF file containing the location of\n    known genes. We can use Cufflinks to assemble transcripts from the\n    alignment data to create GTF files.\n    From the Galaxy tool panel, select \nNGS: RNA Analysis > Cufflinks\n and\n    set the parameters as follows:  \n\n\n\n\nSAM or BAM file of aligned RNA-Seq reads:\n\n  Click on the multiple datasets icon and select all 6 BAM files from\n  Tophat\n\n\nMax Intron Length:\n 50000  \n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nNext, we want to merge the assemblies outputted by Cufflinks by selecting\n    \nNGS: RNA Analysis > Cuffmerge\n and setting the parameters as follows:  \n\n\n\n\nGTF file(s) produced by Cufflinks:\n Select the 6 GTF files ending\n  with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or\n  command key to select multiple files.\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nNote: In cases where you have a reference GTF, but also want to identify\nnovel transcripts with Cufflinks, you would add the reference GTF to the\ncuffmerge inputs with the \nAdditional GTF Inputs (Lists)\n parameter.\n\n\n\n\n\n\nView the Cuffmerge GTF file by clicking the \neye icon\n\n\n\n\n\n\nRun Cuffdiff using the new GTF file\n    In the Galaxy tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis > Cuffdiff\n and set the parameters as follows:  \n\n\n\n\nTranscripts:\n Cuffmerge on data x, data x, and others: merged\n  transcripts\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname:\n C1\n\n\nReplicates:\n\n\nTophat on data 1: accepted_hits\n\n\nTophat on data 2: accepted_hits\n\n\nTophat on data 3: accepted_hits\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname:\n C2\n\n\nReplicates:\n\n\nTophat on data 4: accepted_hits\n\n\nTophat on data 5: accepted_hits\n\n\nTophat on data 6: accepted_hits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nFilter the recently generated gene set for significantly differentially\n    expressed genes by going to \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nRename the output file to something meaningful like\n\"Significant_DE_Genes_using_Cufflinks_Assembly\"\n\n\n\n\n\n\nViewing the significant genes, we see that there are two genes that are\nidentified as differentially expressed by Cuffdiff using the GTF file produced\nfrom Cufflinks and Cuffmerge. The locations of these two genes correspond to\nthe previous result from section 3 (genes Ank and CG2177).\n\n\nTranscript-level differential expression\n\n\nOne can think of a scenario in an experiment aiming to investigate the\ndifferences between two experimental conditions, where a gene had the same\nnumber of read counts in the two conditions but these read counts were derived\nfrom different transcripts; this gene would not be identified in a differential\ngene expression test, but would be in a differential transcript expression test.\nThe choice of what \"unit of aggregation\" to use in differential expression\ntesting is one that should be made by the biological investigator, and will\naffect the bioinformatics analysis done (and probably the data generation too).\n\n\nTake a look at the different differential expression files produced by\nCuffdiff from section 3 which use different units of aggregation.\n\n\nReferences\n\n\nTrapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. \nNature Protocols\n [serial online]. March 1, 2012;7(3):562-578.",
            "title": "Tuxedo Protocol Tutorial"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#rna-seq-differential-gene-expression-basic-tutorial",
            "text": "Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung",
            "title": "RNA-Seq Differential Gene Expression: Basic Tutorial"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#tutorial-overview",
            "text": "In this tutorial we cover the concepts of RNA-seq differential gene expression\n(DGE) analysis using a small synthetic dataset from the model organism,\nDrosophila melanogaster.  The tutorial is designed to introduce the tools, datatypes and workflow of an\nRNA-seq DGE analysis. In practice, real datasets would be much larger and\nwould contain sequencing and alignment errors that make analysis more difficult.  Our input data for this tutorial will be raw RNA-seq reads from two\nexperimental conditions and we will output a list of differentially\nexpressed genes identified to be statistically significant.  In this tutorial we will:     introduce the types of files typically used in RNA-seq analysis  align RNA-seq reads with Tophat  visualise RNA-seq alignment data with IGV  find differentially expressed genes with Cuffdiff  understand the importance of replicates for differential expression analysis   This tutorial does not cover the following steps that you would do in a real\nRNA-seq DGE analysis:     QC (quality control) of the raw sequence data  Trimming the reads for quality and for adaptor sequences  QC of the RNA-seq alignment data     These steps have been omitted because the data we use in this tutorial is\nsynthetic and has no quality issues, unlike real data.",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#learning-objectives",
            "text": "At the end of this tutorial you should:   Be familiar with the Tuxedo Protocol workflow for RNA-seq differential\n   expression analysis  Be able to process raw RNA sequence data into a list of differentially\n   expressed genes  Be aware of how the relationship between the number of biological replicates\n   in an experiment and the statistical power available to detect differentially\n   expressed genes",
            "title": "Learning Objectives"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#background",
            "text": "",
            "title": "Background"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#where-does-the-data-in-this-tutorial-come-from",
            "text": "The data for this tutorial is from an RNA-seq experiment looking for\ndifferentially expressed genes in D. melanogaster (fruit fly) between two\nexperimental conditions. The experiment and analysis protocol we will follow\nis derived from a paper in Nature Protocols by the research group responsible\nfor one of the most widely used set of RNA-seq analysis tools:  \"Differential\ngene and transcript expression analysis of RNA-seq experiments with TopHat and\nCufflinks\"  (Trapnell et al 2012).  The sequence datasets are single-end Illumina synthetic short reads,\nfiltered to only include chromosome 4 to facilitate faster mapping (which\nwould otherwise take hours). We\u2019ll use data from three biological replicates\nfrom each of the two experimental conditions.",
            "title": "Where does the data in this tutorial come from?"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#the-tuxedo-protocol",
            "text": "The workflow this tutorial is based on is the Tuxedo Protocol. Reads are first\nmapped with TopHat and a transcriptome is then assembled using Cufflinks.\nCuffdiff then quantifies the expression in each condition, and tests for\ndifferential expression.  In this tutorial we use a simpler protocol as the D. melanogaster\ntranscriptome is already very well characterised.  More information about the Tuxedo protocol can be found  here .",
            "title": "The Tuxedo Protocol"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-1-preparation-15-min",
            "text": "",
            "title": "Section 1: Preparation [15 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account-what-is-galaxy",
            "text": "Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User > Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User > Login .",
            "title": "1.  Register as a new user in Galaxy if you don\u2019t already have an account (what is Galaxy?)"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-import-the-rna-seq-data-for-the-workshop",
            "text": "If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Basic_Sec_1 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data > Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.  Upload the sequence data by pasting the following links into the text\n    input area:\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R1.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R2.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C1_R3.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R1.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R2.chr4.fq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/C2_R3.chr4.fq\n     \n     \n    Select the type as 'fastqsanger' and press  start  to upload the\n    files to Galaxy.  Upload the annotated gene list reference by pasting the following link\n    into the text input area:\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_BASIC/ensembl_dm3.chr4.gtf\n     \n    You don't need to specify the type for this file as Galaxy will\n    auto-detect the file as a GTF file.",
            "title": "2.  Import the RNA-seq data for the workshop."
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-view-and-have-an-understanding-of-the-files-involved-in-rna-seq-analysis",
            "text": "You should now have the following files in your Galaxy history:  6 files containing single-ended reads:  C1_R1.chr4.fq C1_R2.chr4.fq C1_R3.chr4.fq C2_R1.chr4.fq C2_R2.chr4.fq C2_R3.chr4.fq   And 1 gene annotation file:  ensembl_dm3.chr4.gtf   These files can be renamed by clicking the  pen icon  if you wish.    These 6 sequencing files are in FASTQ format and have the file\n    extension: .fq. If you are not familiar with the FASTQ format,  click\n    here for an overview .    Click on the  eye icon  to the top right of each FASTQ file to view the\nfirst part of the file.\nThe first 3 files are from the first condition (C1) and has 3\nreplicates labelled R1, R2, and R3. The next 3 FASTQ files are from\nthe second condition (C2) and has 3 replicates labelled R1, R2, and R3.  In this tutorial, we aim to find genes which are differentially\nexpressed between condition C1 and condition C2.    The gene annotation file is in GTF format. This file describes where\n    the genes are located in the Drosophila reference genome.\n    We will examine this file more closely later in Section 3 of this\n    tutorial.    NOTE:  Since the reads in this dataset are synthetic, they do not have\nreal quality scores.  NOTE:  If you log out of Galaxy and log back at a later time your data\nand results from previous experiments will be available in the right panel\nof your screen called the \u2018History\u2019",
            "title": "3.  View and have an understanding of the files involved in RNA-seq analysis."
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-2-align-reads-with-tophat-30-mins",
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found  here .",
            "title": "Section 2: Align reads with Tophat [30 mins]"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-align-the-rna-seq-short-reads-to-a-reference-genome",
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat  and set the parameters as follows:     Is this single-end or paired-end data?  Single-end    RNA-Seq FASTQ file: \n  (Click on the multiple datasets icon and select all six of the FASTQ\n  files. This can be done by holding down the shift key to select a range\n  of files, or holding down the ctrl key (Windows) or command key (OSX) and\n  clicking to select multiple files.)  C1_R1.chr4.fq  C1_R2.chr4.fq  C1_R3.chr4.fq  C2_R1.chr4.fq  C2_R2.chr4.fq  C2_R3.chr4.fq      Use a built in reference genome or own from your history:  Use\n  built-in genome  Select a reference genome:  D. melanogaster Apr. 2006 (BDGP R5/dm3)\n  (dm3)  Use defaults for the other fields  Execute     Show screenshot       //<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>  \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n      Note: This may take a few minutes, depending on how busy the server is.",
            "title": "1.  Align the RNA-seq short reads to a reference genome."
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-examine-the-output-files",
            "text": "You should have 5 output files for each of the FASTQ input files:   Tophat on data 1: accepted_hits:  This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.  Tophat on data 1: splice junctions:  This file lists all the places\n  where Tophat had to split a read into two pieces to span an exon\n  junction.  Tophat on data 1: deletions  and  Tophat on data 1: insertions: \n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.  Tophat on data 1: align_summary:  This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.   You should have a total of 30 Tophat output files in your history.",
            "title": "2.  Examine the output files"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-visualise-the-aligned-reads-with-igv",
            "text": "The purpose of this step is to :   visualise the quantitative, exon-based nature of RNA-seq data  visualise the expression differences between samples represented by the\n  quantity of reads, and  become familiar with the  Integrative Genomics Viewer\n  (IGV) -- an interactive\n  visualisation tool by the Broad Institute.     To visualise the alignment data:   Click on one of the Tophat accepted hits files, for example 'Tophat on\n    data 1: accepted_hits'.  Click on Display with IGV  'webcurrent'  (or 'local' if you have IGV\n    installed on your computer. You will need to open IGV before you click on\n    'local'). This should download a .jnlp Java Web Start file to your\n    computer. Open this file to run IGV. (You will need Java installed on your\n    computer to run IGV)  Once IGV opens, it will show you the accepted_hits BAM file. (Note:\n    this may take a bit of time as the data is downloaded to IGV)  Select  chr4  from the second drop box under the toolbar. Zoom in to\n    view alignments of reads to the reference genome.\n    You should see the characteristic distribution of RNA-seq reads across\n    the exons of the genes, with some gaps at intron/exon boundaries.\n    The number of reads aligned to a particular gene is proportional to the\n    abundance of the RNA derived from that gene in the sequenced sample.\n    (Note that IGV already has a list of known genes of most major organisms\n    including Drosophila, which is why you can see the genes in the bottom\n    panel of IGV.)  View one of the splice function files such as 'TopHat on data 1: splice\n    junctions'. You will need to save this file to your local disk\n    using the  disk icon  under the details of the file. Then open\n    the saved .bed file directly in IGV using the  File > Load From File \n    option from IGV. This is because IGV doesn\u2019t automatically stream BED\n    files from Galaxy.\n     \n    The junctions file is loaded at the bottom of the\n    IGV window and splicing events are represented as coloured arcs. The\n    height and thickness of the arcs are proportional to the read depth.   View differentially expressed genes by viewing two alignment files\n    simultaneously. The aim of this tutorial is to statistically test\n    differential expression, but first it\u2019s useful to reassure ourselves\n    that the data looks right at this stage by comparing the aligned reads\n    for condition 1 (C1) and condition 2 (C2).  Select 'TopHat on data 4: accepted_hits' (this is the accepted hits\nalignment file from first replicate of condition C2) and click on\n'display with IGV local'. This time we are using the  'local'  link, as\nwe already have an IGV window up and running locally from the last\nstep. One the file has loaded, change the location to chr4:325197-341887  using the field on the top toolbar.  The middle gene in this area clearly looks like it has many more reads\nmapped in condition 2 than condition 1, whereas for the surrounding\ngenes the reads look about the same. The middle gene looks like it is\ndifferentially expressed. But, of course, it may be that there are\nmany more reads in the readsets for C1 and C2, and the other genes\nare underexpressed in condition 2. So we need to statistically\nnormalise the read counts before we can say anything definitive,\nwhich we will do in the next section.",
            "title": "3.  Visualise the aligned reads with IGV"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#4-optional-visualise-the-aligned-reads-in-trackster",
            "text": "We can also use the inbuilt Galaxy genome browser, Trackster, to visualise\nalignments. Trackster has fewer features than IGV, but sometimes it may be\nmore convenient to use as it only requires the browser.   On the top bar of Galaxy, select  Visualization > New Track Browser .  Name your new visualization and select D. melanogaster (dm3) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select Tophat on\n    data 1: accepted_hits and Tophat on data 4: accepted_hits by using the\n    checkboxes on the left.  Select chr4 from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar. You can also add more tracks using the Add\n    Tracks icon located on the top right.  Next to the drop down list, click on the chromosomal position number\n    display and specify the location  chr4:325197-341887 .     Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.",
            "title": "4.  [Optional] Visualise the aligned reads in Trackster"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-3-test-differential-expression-with-cuffdiff-45-min",
            "text": "The aim in this section is to:   generate tables of normalised read counts per gene per condition based on the\n  annotated reference transcriptome,  statistically test for expression differences in normalised read counts for\n  each gene, taking into account the variance observed between samples,    for each gene, calculate the p-value of the gene being differentially\n  expressed-- this is the probability of seeing the data or something more\n  extreme given the null hypothesis (that the gene is not differentially\n  expressed between the two conditions),  for each gene, estimate the fold change in expression between the two\n  conditions.   All these steps are rolled up into a single tool in Galaxy: Cuffdiff. Cuffdiff\nis part of the Cufflinks software suite which takes the aligned reads from\nTophat and generates normalised read counts and a list of differentially\nexpressed genes based on a reference transcriptome - in this case, the curated\nEnsembl list of D. melanogaster genes from chromosome 4 that we supply as a\nGTF (Gene Transfer Format) file.\nA more detailed explanation of Cufflinks DGE testing can be found here .",
            "title": "Section 3: Test differential expression with Cuffdiff [45 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#1-examine-the-reference-transcriptome",
            "text": "Click on the  eye icon  to display the ensembl_dm3.chr4.gtf reference\ntranscriptome file in Galaxy.  The reference transcriptome is essentially a list of chromosomal features\nwhich together define genes. Each feature is in turn defined by a\nchromosomal start and end point, feature type (CDS, gene, exon etc),\nand parent gene and transcript. Importantly, a gene may have many features,\nbut one feature will belong to only one gene.\nMore information on the GTF format can be found here . \nThe ensembl_dm3.chr4.gtf file contains ~4900 features which together define\nthe 92 known genes on chromosome 4 of Drosophila melanogaster. Cuffdiff\nuses the reference transcriptome to aggregate read counts per gene,\ntranscript, transcription start site and coding sequence (CDS). For this\ntutorial, we\u2019ll only consider differential gene testing, but it is also\npossible to test for differential expression of transcripts or\ntranscription start sites.",
            "title": "1.  Examine the reference transcriptome"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#2-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts",
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff  and set the parameters as follows:     Transcripts:  ensembl_dm3.chr4.gtf  Condition:     1: Condition  name:  C1  Replicates:  Tophat on data 1: accepted_hits  Tophat on data 2: accepted_hits  Tophat on data 3: accepted_hits \n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key for Windows or the command key for OSX.)      2: Condition  name:  C2  Replicates:  Tophat on data 4: accepted_hits  Tophat on data 5: accepted_hits  Tophat on data 6: accepted_hits        Use defaults for the other fields  Execute     Show screenshot       \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "2.  Run Cuffdiff to identify differentially expressed genes and transcripts"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#3-explore-the-cuffdiff-output-files",
            "text": "There should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 37, data 32, and others\".  FPKM tracking files:      transcript FPKM tracking  gene FPKM tracking  TSS groups FPKM tracking  CDS FPKM tracking     These 4 files contain the FPKM (a unit of normalised expression taking\ninto account the transcript length for each transcript\nand the library size of the sample) for each of the two conditions.  Differential expression testing files:      gene differential expression testing  transcript differential expression testing  TSS groups differential expression testing  CDS FPKM differential expression testing  CDS overloading diffential expression testing  promoters differential expression testing  splicing differential expression testing   These 7 files contain the statistical\nresults from testing the level of expression between condition C1 and condition C2.    Examine the tables of normalised gene counts\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene FPKM\n    tracking\" by clicking on the  eye icon . The file consists of one row\n    for each gene from the reference transcriptome, with columns containing the\n    normalised read counts for each of the two conditions.\n    Note:     Cuffdiff gives each gene it\u2019s own \u2018tracking_id\u2019, which equates\n  to a gene. Multiple transcription start sites are aggregated under a\n  single tracking_id.  A gene encompasses a chromosomal locus which covers all the features\n  that make up that gene (exons, introns, 5\u2019 UTR, etc).     Inspect the gene differential expression testing file\n    View the Cuffdiff file \"Cuffdiff on data x, data x, and others: gene\n    differential expression testing\" by clicking on the  eye icon . The\n    columns of interest are: gene (c3), locus (c4), log2(fold_change) (c10),\n    p_value (c12), q_value (c13) and significant (c14).    Filter based on column 14 (\u2018significant\u2019) - a binary assessment of\n    q_value > 0.05, where q_value is p_value adjusted for multiple testing.\n    Under Basic Tools, click on  Filter and Sort > Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   This will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. \nWe can rename this file (screenshot) by clicking on the  pencil icon  of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Significant_DE_Genes\".    Examine the sorted list of differentially expressed genes.\n    Click on the  eye icon  next to \"Significant_DE_Genes\" to view the data.",
            "title": "3.  Explore the Cuffdiff output files"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-4-repeat-without-replicates-20-min",
            "text": "In this section, we will run Cuffdiff with fewer replicates.",
            "title": "Section 4. Repeat without replicates [20 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#section-5-optional-extension-20-min",
            "text": "",
            "title": "Section 5. Optional Extension [20 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#extension-on-the-tuxedo-protocol",
            "text": "The full Tuxedo protocol includes other tools such as Cufflinks, Cuffmerge,\nand CummeRbund. Cufflinks and Cuffmerge can build a new reference transcriptome by\nidentifying novel transcripts and genes in the RNA-seq dataset - i.e. using these tools will allow\nyou to identify new genes and transcripts, and then analyse them for\ndifferential expression. This is critical for organisms in which the\ntranscriptome is not well characterised. CummeRbund helps visualise the data\nproduced from the Cuffdiff using the R statistical programming language.  Read more on the full Tuxedo protocol  here .  If the organism we were working on did not have a well characterized reference\ntranscriptome, we would run Cufflinks and Cuffmerge to create a transcriptome.    Suppose we didn't have our Drosophila GTF file containing the location of\n    known genes. We can use Cufflinks to assemble transcripts from the\n    alignment data to create GTF files.\n    From the Galaxy tool panel, select  NGS: RNA Analysis > Cufflinks  and\n    set the parameters as follows:     SAM or BAM file of aligned RNA-Seq reads: \n  Click on the multiple datasets icon and select all 6 BAM files from\n  Tophat  Max Intron Length:  50000    Use defaults for the other fields  Execute     Next, we want to merge the assemblies outputted by Cufflinks by selecting\n     NGS: RNA Analysis > Cuffmerge  and setting the parameters as follows:     GTF file(s) produced by Cufflinks:  Select the 6 GTF files ending\n  with 'assembled transcripts' produced by Cufflinks. Use the ctrl key or\n  command key to select multiple files.  Use defaults for the other fields  Execute   Note: In cases where you have a reference GTF, but also want to identify\nnovel transcripts with Cufflinks, you would add the reference GTF to the\ncuffmerge inputs with the  Additional GTF Inputs (Lists)  parameter.    View the Cuffmerge GTF file by clicking the  eye icon    Run Cuffdiff using the new GTF file\n    In the Galaxy tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis > Cuffdiff  and set the parameters as follows:     Transcripts:  Cuffmerge on data x, data x, and others: merged\n  transcripts  Condition:     1: Condition  name:  C1  Replicates:  Tophat on data 1: accepted_hits  Tophat on data 2: accepted_hits  Tophat on data 3: accepted_hits      2: Condition  name:  C2  Replicates:  Tophat on data 4: accepted_hits  Tophat on data 5: accepted_hits  Tophat on data 6: accepted_hits        Use defaults for the other fields  Execute     Filter the recently generated gene set for significantly differentially\n    expressed genes by going to  Filter and Sort > Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   Rename the output file to something meaningful like\n\"Significant_DE_Genes_using_Cufflinks_Assembly\"    Viewing the significant genes, we see that there are two genes that are\nidentified as differentially expressed by Cuffdiff using the GTF file produced\nfrom Cufflinks and Cuffmerge. The locations of these two genes correspond to\nthe previous result from section 3 (genes Ank and CG2177).",
            "title": "Extension on the Tuxedo Protocol"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#transcript-level-differential-expression",
            "text": "One can think of a scenario in an experiment aiming to investigate the\ndifferences between two experimental conditions, where a gene had the same\nnumber of read counts in the two conditions but these read counts were derived\nfrom different transcripts; this gene would not be identified in a differential\ngene expression test, but would be in a differential transcript expression test.\nThe choice of what \"unit of aggregation\" to use in differential expression\ntesting is one that should be made by the biological investigator, and will\naffect the bioinformatics analysis done (and probably the data generation too).  Take a look at the different differential expression files produced by\nCuffdiff from section 3 which use different units of aggregation.",
            "title": "Transcript-level differential expression"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_tuxedo/#references",
            "text": "Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks.  Nature Protocols  [serial online]. March 1, 2012;7(3):562-578.",
            "title": "References"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/",
            "text": "Background\n\n\nIntroduction to RNA-seq\n\n\nRNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample.\n\n\nDifferential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples.\n\n\nIn eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites\n\n\nThe Galaxy workflow platform\n\n\nWhat is Galaxy?\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nFigure 1: The Galaxy interface\n\n\nTools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\nDifferential gene expression analysis using Tophat and Cufflinks\n\n\nTwo protocols are described in the paper inspiring this tutorial (Trapnell et al 2012):\n\n\n\n\n\n\nThe \nTuxedo protocol\n: a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes\n\n\n\n\n\n\nThe \nAlternate protocol\n: a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed\n\n\n\n\n\n\nAssembling a transcriptome is advised if no well characterised transcriptome exists, but as \nD. melanogaster\n is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on \nD. melanogaster\n, so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler.\n\n\nIf we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.\n\n\nThe Alternate protocol\n\n\nThe overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.\n\n\nFigure 2: General workflow for testing expression differences between two experimental conditions\n\n\n\n\nTophat\n\n\nReads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.\n\n\n\n\nTopHat input: Fasta or Fastq files\n\n\nTopHat output: BAM file (Compressed SAM file)\n\n\n\n\nCuffdiff\n\n\nThe reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.\n\n\n\n\nCuffdiff input: Reference transcriptome as GTF file\n\n\nCuffdiff output:\n\n\nGene and transcript expression levels as tables of normalised read counts\n\n\nDifferential analysis testing on:\n\n\nGenes\n\n\nTranscripts\n\n\nTranscription Start Site (TSS) groups\n\n\nSplicing: files reporting on splicing\n\n\nPromoter: differentially spliced genes via promoter switching\n\n\nCDS: CoDing Sequences\n\n\n\n\n\n\n\n\n\n\n\n\nThe full Tuxedo Protocol\n\n\nFigure 3: Full Tuxedo protocol workflow\n\n\n\n\n\n\n\n\nTophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.\n\n\n\n\nTopHat input: Fasta or Fastq files\n\n\nTopHat output: BAM file (Compressed SAM file)\n\n\n\n\n\n\n\n\nCufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones\n\n\n\n\nCufflinks input:\n\n\nMapped reads (SAM or BAM), use accepted_hits.bam from Tophat\n\n\nGenome annotation: GTF file\n\n\n\n\n\n\nCufflinks output:\n\n\nassembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular)\n\n\ntranscript_expression (tabular): table of expression levels for each transcript\n\n\ngene_expression (tabular): table of total expression levels for each gene.\n\n\n\n\n\n\n\n\n\n\n\n\nCuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy.\n\n\n\n\nCuffcompare input:\n\n\nAssembled_transcripts for each sample\n\n\nReference Annotation\n\n\n\n\n\n\nCuffcompare output: combined_transcripts.gtf\n\n\n\n\n\n\n\n\nCuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.\n\n\n\n\nCuffdiff input:\n\n\nReference transcriptome as GTF file\n\n\nBAM files of mapped reads from Tophat for all samples\n\n\n\n\n\n\nCuffdiff output:\n\n\nGene and transcript expression levels as tables of normalised read counts\n\n\nDifferential analysis testing on:\n\n\nGenes\n\n\nTranscripts\n\n\nTranscription Start Site (TSS) groups\n\n\nSplicing: files reporting on splicing\n\n\nPromoter: differentially spliced genes via promoter switching\n\n\nCDS: CoDing Sequences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.\n\n\n\n\n\n\nProtocols recommendations\n\n\n\n\nCreate a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended.\n\n\nDo paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads.\n\n\nSequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads.\n\n\nIdentify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.\n\n\n\n\nLimitations of the protocols\n\n\n\n\nBoth Tophat and Cufflinks require a reference genome.\n\n\nThe protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.\n\n\n\n\nReferences\n\n\n\n\nTrapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks. \nNature Protocols\n [serial online]. March 1, 2012;7(3):562-578.\n\n\nJames T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov.\n \nIntegrative Genomics Viewer\n. Nature Biotechnology 29, 24\u201326 (2011)\n,\n\n\n\n\n[^1]: \nThe published protocol has been designed for running on the\n    command line in Linux. This tutorial has been adapted to use on the\n    web-based Galaxy platform.\n\n\n[^2]: \nNote: Cuffmerge is a meta-assembler. It treats the assemblies\n    created by Cufflinks the same way Cufflinks treats reads from\n    Tophat. That is, it produces a parsimonious transcript assembly of\n    the assemblies. The difference between Cuffmerge and Cuffcompare is\n    that if we have two transfrags A and B, Cuffcompare only combines\n    the two where either A or B is \u2018contained\u2019 in the other transfrag or\n    in other words one of them is redundant; whereas Cuffmerge assembles\n    them if they overlap with each other and agree on splicing.\n\n\n[^3]: \nSince CummRbund is currently not installed on Galaxy, the\n    underlying steps are not included in this tutorial; instead we use\n    IGV (Robinson et al 2011) and Trackster for visualizing the output\n    which are accessible from Galaxy.",
            "title": "Background"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#background",
            "text": "",
            "title": "Background"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#introduction-to-rna-seq",
            "text": "RNA-seq as a genomics application is essentially the process of collecting RNA (of any type: mRNA, rRNA, miRNA), converting in some way to DNA, and sequencing on a massively parallel sequencing technology such as Illumina Hiseq. Critically, the number of short reads generated for a particular RNA is assumed to be proportional to the amount of that RNA that was present in the collected sample.  Differential gene expression studies can exploit RNA-seq to quantitate the amount of mRNA in different samples and statistically test the difference in expression per-gene (generally measured as the normalised number of sequence reads per gene/transcript) between the samples.  In eukaryotes, differential gene expression analysis is complicated by the possibility of multiple isoforms for any particular gene through alternative splicing and/or multiple transcription start sites",
            "title": "Introduction to RNA-seq"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-galaxy-workflow-platform",
            "text": "",
            "title": "The Galaxy workflow platform"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#what-is-galaxy",
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here",
            "title": "What is Galaxy?"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-1-the-galaxy-interface",
            "text": "Tools on the left, data in the middle, analysis workflow on the right.",
            "title": "Figure 1: The Galaxy interface"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#differential-gene-expression-analysis-using-tophat-and-cufflinks",
            "text": "Two protocols are described in the paper inspiring this tutorial (Trapnell et al 2012):    The  Tuxedo protocol : a full analysis protocol covering the assembly and characterisation of the expressed genes from the experimental data, and statistical analysis of gene expression changes in those genes    The  Alternate protocol : a shorter approach for experiments in which the set of genes to be analysed is already known. Changes in expression of those genes are analysed    Assembling a transcriptome is advised if no well characterised transcriptome exists, but as  D. melanogaster  is a model organism we have access to well-annotated and comprehensive genomes and transcriptomes from the multitude of previous genomic analyses on  D. melanogaster , so the simpler \u2018Alternate protocol\u2019 is appropriate. It also has the advantage of being simpler.  If we were investigating a novel organism then we would first need to characterise the transcriptome by assembling it from the experimental data, as gene expression is only meaningful in the context of a defined transcriptome.",
            "title": "Differential gene expression analysis using Tophat and Cufflinks"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-alternate-protocol",
            "text": "The overall workflow for this protocol is depicted below[^1]. Briefly, raw reads from each of the sequenced replicates for each experimental condition are aligned against a reference genome; during this process splice sites are identified and reads mapped across introns as required. The mapped reads are then used to derive counts of reads vs genes by cross referencing against a list of known genes (the \u2018reference transcriptome\u2019); these read counts are normalised within and between sample sets by a variety of methods and then statistical tests are used to assess the significance of differences between the normalised read counts of sample sets, producing a ranked list of differentially expressed genes.",
            "title": "The Alternate protocol"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-2-general-workflow-for-testing-expression-differences-between-two-experimental-conditions",
            "text": "",
            "title": "Figure 2: General workflow for testing expression differences between two experimental conditions"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#tophat",
            "text": "Reads from experimental conditions A and B are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine; it breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.   TopHat input: Fasta or Fastq files  TopHat output: BAM file (Compressed SAM file)",
            "title": "Tophat"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#cuffdiff",
            "text": "The reads and the reference transcriptome are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.   Cuffdiff input: Reference transcriptome as GTF file  Cuffdiff output:  Gene and transcript expression levels as tables of normalised read counts  Differential analysis testing on:  Genes  Transcripts  Transcription Start Site (TSS) groups  Splicing: files reporting on splicing  Promoter: differentially spliced genes via promoter switching  CDS: CoDing Sequences",
            "title": "Cuffdiff"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#the-full-tuxedo-protocol",
            "text": "",
            "title": "The full Tuxedo Protocol"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#figure-3-full-tuxedo-protocol-workflow",
            "text": "Tophat: Reads from different experimental conditions are mapped to a reference genome with TopHat. TopHat uses the Bowtie aligner as an alignment engine. It breaks up the reads that Bowtie cannot align on its own into smaller pieces called segments.   TopHat input: Fasta or Fastq files  TopHat output: BAM file (Compressed SAM file)     Cufflinks: Resulting alignment files are provided to the Cufflinks program. Cufflinks uses these alignments to generate a transcriptome assembly for each condition. It reports a parsimonious transcriptome assembly of the data, i.e. all transcript fragments or \u2018transfrags\u2019 needed to \u2018explain\u2019 all the splicing event outcomes in the input data are reported. Cufflinks also quantifies the expression level of each transfrag in the sample to filter out the artificial ones   Cufflinks input:  Mapped reads (SAM or BAM), use accepted_hits.bam from Tophat  Genome annotation: GTF file    Cufflinks output:  assembled transcripts (GTF) including all isoforms with their exon structure and expression levels. (tabular)  transcript_expression (tabular): table of expression levels for each transcript  gene_expression (tabular): table of total expression levels for each gene.       Cuffmerge[^2]/Cuffcompare: Cufflinks produces an assembly for each condition/sample. To perform differential expression we need to combine to assemblies into a single assembly. Assemblies can be merged together using the Cuffmerge or Cuffcompare utilities which are included with the Cufflinks package. This will result in the creation of a meta-transcriptome. Both Cuffcompare and Cuffmerge are available on Galaxy.   Cuffcompare input:  Assembled_transcripts for each sample  Reference Annotation    Cuffcompare output: combined_transcripts.gtf     Cuffdiff:The reads and the combined assembly are fed to Cuffdiff which calculates expression levels and tests the statistical significance of the observed changes.   Cuffdiff input:  Reference transcriptome as GTF file  BAM files of mapped reads from Tophat for all samples    Cuffdiff output:  Gene and transcript expression levels as tables of normalised read counts  Differential analysis testing on:  Genes  Transcripts  Transcription Start Site (TSS) groups  Splicing: files reporting on splicing  Promoter: differentially spliced genes via promoter switching  CDS: CoDing Sequences         CummRbund[^3]: Using an R package called CummRbund, diiferentially expressed genes and transcriptomes can be visually displayed using various expression plots.",
            "title": "Figure 3: Full Tuxedo protocol workflow"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#protocols-recommendations",
            "text": "Create a replicate from each condition to control the batch effects such as variation in culture conditions. With current available kits creating triplicates is feasible and strongly recommended.  Do paired-end sequencing whenever possible. For example, Cufflinks is much more accurate in the presence of paired-end reads.  Sequence with longer reads whenever possible. Tophat is more accurate in presence of longer reads in compare to shorter reads. However, since the cost of sequencing with longer reads is substantially more than shorter reads, some researchers prefer to do more replicates or more samples with shorter reads.  Identify new genes with traditional cloning and PCR-based techniques because transcriptome assembly is difficult.",
            "title": "Protocols recommendations"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#limitations-of-the-protocols",
            "text": "Both Tophat and Cufflinks require a reference genome.  The protocol assumes that RNASeq was done using Illumina or Solid sequencing techniques.",
            "title": "Limitations of the protocols"
        },
        {
            "location": "/tutorials/rna_seq_dge_basic/rna_seq_basic_background/#references",
            "text": "Trapnell C, Roberts A, Pachter L, et al. Differential gene and transcript expression analysis of RNA-seq experiments with TopHat and Cufflinks.  Nature Protocols  [serial online]. March 1, 2012;7(3):562-578.  James T. Robinson, Helga Thorvaldsd\u00f3ttir, Wendy Winckler, Mitchell Guttman, Eric S. Lander, Gad Getz, Jill P. Mesirov.   Integrative Genomics Viewer . Nature Biotechnology 29, 24\u201326 (2011) ,   [^1]:  The published protocol has been designed for running on the\n    command line in Linux. This tutorial has been adapted to use on the\n    web-based Galaxy platform.  [^2]:  Note: Cuffmerge is a meta-assembler. It treats the assemblies\n    created by Cufflinks the same way Cufflinks treats reads from\n    Tophat. That is, it produces a parsimonious transcript assembly of\n    the assemblies. The difference between Cuffmerge and Cuffcompare is\n    that if we have two transfrags A and B, Cuffcompare only combines\n    the two where either A or B is \u2018contained\u2019 in the other transfrag or\n    in other words one of them is redundant; whereas Cuffmerge assembles\n    them if they overlap with each other and agree on splicing.  [^3]:  Since CummRbund is currently not installed on Galaxy, the\n    underlying steps are not included in this tutorial; instead we use\n    IGV (Robinson et al 2011) and Trackster for visualizing the output\n    which are accessible from Galaxy.",
            "title": "References"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/",
            "text": "body{\n        line-height: 2;\n        font-size: 16px;\n    }\n\n    ol li{padding: 4px;}\n    ul li{padding: 0px;}\n    h4 {margin: 30px 0px 15px 0px;}\n\n    div.code {\n        font-family: \"Courier New\";\n        border: 1px solid;\n        border-color: #999999;\n        background-color: #eeeeee;\n        padding: 5px 10px;\n        margin: 10px;\n        border-radius: 5px;\n        overflow: auto;\n    }\n\n    div.question {\n        color: #666666;\n        background-color: #e1eaf9;\n        padding: 15px 25px;\n        margin: 20px;\n        font-size: 15px;\n        border-radius: 20px;\n    }\n\n    div.question h4 {\n        font-style: italic;\n        margin: 10px 0px !important;\n    }\n\n\n\n\n\n\n\n\nRNA-Seq Differential Gene Expression: Advanced Tutorial\n\n\nAuthors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we compare the performance of three statistically-based\nexpression analysis tools:  \n\n\n\n\nCuffDiff\n\n\nEdgeR\n\n\nDESeq2\n\n\n\n\nThis tutorial builds on top of the \nbasic RNA-seq DGE tutorial\n.\nIt is recommended to have some familiarity of RNA-seq before beginning this\ntutorial.\n\n\n\n\nBackground [15 min]\n\n\nWhere does the data in this tutorial come from?\n\n\nThe data for this tutorial is from the paper, \nA comprehensive comparison of\nRNA-Seq-based transcriptome analysis from reads to differential gene expression\nand cross-comparison with microarrays: a case study in Saccharomyces\ncerevisiae\n by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK\n113-7D (yeast) under two different metabolic conditions: glucose-excess (batch)\nor glucose-limited (chemostat).\n\n\nThe RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with\naccession SRS307298. There are 6 samples in total-- two treatments with\nthree biological replicates each. The data is paired-end.  \n\n\nWe have extracted chromosome I reads from the samples to make the\ntutorial a suitable length. This has implications, as discussed in section 8.\n\n\n\n\nSection 1: Preparation [15 min]\n\n\n1.  Register as a new user in Galaxy if you don\u2019t already have an account\n\n\n\n\nOpen a browser and go to a Galaxy server. This can either be your\n    personal GVL server you \nstarted previously\n,\n    the public \nGalaxy Tutorial server\n\n    or the public \nGalaxy Melbourne server\n.\n\n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.\n\n\nRegister as a new user by clicking \nUser > Register\n on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking \nUser > Login\n.\n\n\n\n\n2.  Import the RNA-seq data for the workshop.\n\n\nIf you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to\n\nShared Data > Published Histories\n on the top toolbar, and selecting\nthe history called \nRNA-Seq_Adv_Sec_1\n. Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.\n\n\nAlternatively, if you are using your own personal Galaxy server, you can import\nthe data by:\n\n\n\n\nIn the tool panel located on the left, under Basic Tools select \nGet\n    Data > Upload File\n. Click on the \nPaste/Fetch data\n button on the\n    bottom section of the pop-up window.\n\n\nUpload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the batch condition\n    (glucose-excess). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n    \n\n    \n\n    These six files are three paired-end samples from the chem condition\n    (glucose-limited). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n    \n\n    \n\n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n    \n\n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf\n    \n\n\n\n\nYou should now have these 13 files in your history:\n\n\n\n\nbatch1_chrI_1.fastq\n\n\nbatch1_chrI_2.fastq\n\n\nbatch2_chrI_1.fastq\n\n\nbatch2_chrI_2.fastq\n\n\nbatch3_chrI_1.fastq\n\n\nbatch3_chrI_2.fastq\n\n\nchem1_chrI_1.fastq\n\n\nchem1_chrI_2.fastq\n\n\nchem2_chrI_1.fastq\n\n\nchem2_chrI_2.fastq\n\n\nchem3_chrI_1.fastq\n\n\nchem3_chrI_2.fastq\n\n\ngenes.gtf\n\n\n\n\nThese files can be renamed by clicking the \npen icon\n if you wish.\n\n\nNote:\n The reads are paired end; for example batch1_chrI_1.fastq and\nbatch1_chrI_2.fastq are paired reads from one sequencing run. Low quality\nreads have already been trimmed.\n\n\n\n\n\n\n\n\nSection 2: Alignment [30 mins]\n\n\nIn this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found \nhere\n.\n\n\n1.  Map/align the reads with Tophat to the S. cerevisiae reference\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis > Tophat\n and set the parameters as follows:  \n\n\n\n\nIs this single-end or paired-end data?\n Paired-end (as individual datasets)  \n\n\n\n\nRNA-Seq FASTQ file, forward reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)\n\n\n\n\nbatch1_chrI_1.fastq\n\n\nbatch2_chrI_1.fastq\n\n\nbatch3_chrI_1.fastq\n\n\nchem1_chrI_1.fastq\n\n\nchem2_chrI_1.fastq\n\n\nchem3_chrI_1.fastq\n\n\n\n\n\n\n\n\nRNA-Seq FASTQ file, reverse reads:\n\n(Click on the \nmultiple datasets icon\n and select all six of the reverse\nFASTQ files ending in *2.fastq.)  \n\n\n\n\nbatch1_chrI_2.fastq\n\n\nbatch2_chrI_2.fastq\n\n\nbatch3_chrI_2.fastq\n\n\nchem1_chrI_2.fastq\n\n\nchem2_chrI_2.fastq\n\n\nchem3_chrI_2.fastq\n\n\n\n\n\n\nUse a built in reference genome or own from your history:\n Use\nbuilt-in genome\n\n\nSelect a reference genome:\n S. cerevisiae June 2008 (SGD/SacCer2)\n(sacCer2)\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n//<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nNote: This may take a few minutes, depending on how busy the server is.\n\n\n2.  Rename the output files\n\n\nYou should have 5 output files for each of the FASTQ input files:\n\n\n\n\nTophat on data 2 and data 1: accepted_hits:\n This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.\n\n\nTophat on data 2 and data 1: splice junctions:\n This file lists all the places\n  where TopHat had to split a read into two pieces to span an exon\n  junction.\n\n\nTophat on data 2 and data 1 deletions\n and \nTophat on data 2 and data 1: insertions:\n\n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.\n\n\nTophat on data 2 and data 1: align_summary:\n This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.\n\n\n\n\nYou should have a total of 30 Tophat output files in your history.\n\n\nRename the 6 accepted_hits files into a more meaningful name (e.g.\n'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam')\nby using the \npen icon\n next to the file.\n\n\n3.  Visualise the aligned reads with Trackster\n\n\n\n\nOn the top bar of Galaxy, select \nVisualization > New Track Browser\n.\n\n\nName your new visualization and select S. cerevisiae (sacCer2) as the\n    reference genome build.\n\n\nClick the \nAdd Datasets to Visualization\n button and select\n    'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the\n    checkboxes on the left.\n\n\nSelect chrI from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar.\n\n\nYou can also add more tracks using the \nAdd Tracks icon\n located on the\n    top right. Load one of the splice junction files such as 'Tophat on data 2\n    and data 1: splice junctions'.\n\n\nExplore the data and try to find a splice junction. Next to the\n    drop down list, click on the chromosomal position number\n    display and specify the location \nchrI:86985-87795\n to view an\n    intron junction.  \n\n\n\n\nIdeally we would add a gene model to the visualisation; but the\ngenes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser)\nhas a slightly different naming convention for one of the chromosomes\nthan the reference genome used by Galaxy, which will cause an error to\nbe thrown by Trackster if you try to add it. This is very typical of\ngenomics currently! If you are interested, you can fiddle with the\ngenes.gtf file to rename the chromosome '2-micron' to '2micron', which\nwill fix the problem.\n\n\nBefore starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.\n\n\n\n\nSection 3. Cuffdiff [40 min]\n\n\nThe aim in this section is to statistically test for differential expression\nusing Cuffdiff and obtain a list of significant genes.\n\n\n1.  Run Cuffdiff to identify differentially expressed genes and transcripts\n\n\nIn the left tool panel menu, under NGS Analysis, select\n\nNGS: RNA Analysis > Cuffdiff\n and set the parameters as follows:\n\n\n\n\nTranscripts:\n genes.gtf\n\n\nCondition:\n  \n\n\n1: Condition\n\n\nname\n batch\n\n\nReplicates:\n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key (Windows) or the command key (OSX).)\n\n\n\n\n\n\n\n\n\n\n2: Condition\n\n\nname\n chem\n\n\nReplicates:\n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nNote: This step may take a while, depending on how busy the server is.\n\n\n2.  Explore the Cuffdiff output files\n\n\nThere should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 43, data 38, and others\". We'll\nmostly be interested in the file ending with 'gene differential expression\ntesting' which contains the statistical results from testing the level of\ngene expression between the batch condition and chem condition.\n\n\nFilter based on column 14 (\u2018significant\u2019) - a binary assessment of\nq_value > 0.05, where q_value is p_value adjusted for multiple testing.\nUnder Basic Tools, click on \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Cuffdiff on data....: gene differential expression testing\"\n\n\nWith following condition:\n c14=='yes'\n\n\nExecute\n\n\n\n\nThis will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. There should be 53 differentially\nexpressed genes in this list.\n\n\nWe can rename this file by clicking on the \npencil icon\n of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Cuffdiff_Significant_DE_Genes\".\n\n\n\n\nSection 4. Count reads in features [30 min]\n\n\nHTSeq-count creates a count matrix using the number of the reads from each bam\nfile that map to the genomic features in the genes.gtf. For each feature (a\ngene for example) a count matrix shows how many reads were mapped to this\nfeature.\n\n\n\n\n\n\nUse HTSeq-count to count the number of reads for each feature.\n\n    In the left tool panel menu, under NGS Analysis, select\n    \nNGS: RNA Analysis > SAM/BAM to count matrix\n and set the parameters as follows:  \n\n\n\n\nGene model (GFF) file to count reads over from your current history:\n genes.gtf\n\n\nbam/sam file from your history:\n\n  (Select all six bam files using the shift key.)\n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n\n\n\n\nExamine the outputted matrix by using the \neye icon\n.\n\n    Each column corresponds to a sample and each row corresponds to a gene. By\n    sight, see if you can find a gene you think is differentially expressed\n    from looking at the counts.\n\n\n\n\n\n\nWe now have a count matrix, with a count against each corresponding sample. We\nwill use this matrix in later sections to calculate the differentially\nexpressed genes.\n\n\n\n\nSection 5: edgeR  [30 min]\n\n\nedgeR\n\nis an R package, that is used for analysing differential expression of\nRNA-Seq data and can either use exact statistical methods or generalised\nlinear models.\n\n\n1.  Generate a list of differentially expressed genes using edgeR\n\n\nIn the Galaxy tool panel, under NGS Analysis, select\n\nNGS: RNA > Differential_Count\n and set the parameters as follows:\n\n\n\n\nSelect an input matrix - rows are contigs, columns are counts for each\n  sample:\n bams to DGE count matrix_htseqsams2mx.xls\n\n\nTitle for job outputs:\n Differential_Counts_edgeR\n\n\nTreatment Name:\n Batch\n\n\nSelect columns containing treatment:\n  \n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\n\n\n\n\nControl Name:\n Chem\n\n\nSelect columns containing control:\n  \n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nRun this model using edgeR:\n Run edgeR\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\n2.  Examine the outputs from the previous step\n\n\n\n\nExamine the Differential_Counts_edgeR_topTable_edgeR.xls file by\n    clicking on the \neye icon\n.\n    This file is a list of genes sorted by p-value from using EdgeR to\n    perform differential expression analysis.\n\n\nExamine the Differential_Counts_edgeR.html file. This file has some\n    output logs and plots from running edgeR. If you are familiar with R,\n    you can examine the R code used for analysis by scrolling to the bottom\n    of the file, and clicking Differential_Counts.Rscript to download the\n    Rscript file.\n\n    If you are curious about the statistical methods edgeR uses, you can\n    read the \nedgeR user's guide at\n    Bioconductor\n.\n\n\n\n\n3.  Extract the significant differentially expressed genes.\n\n\nUnder Basic Tools, click on \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Differential_Counts_edgeR_topTable_edgeR.xls\"\n\n\nWith following condition:\n c6 <= 0.05\n\n\nExecute\n\n\n\n\nThis will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 55 genes in this file.\nRename this file by clicking on the \npencil icon\n of and change the name\nfrom \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".\n\n\n\n\nSection 6. DESeq2 [30 min]\n\n\nDESeq2\n is an\nR package that uses a negative binomial statistical model to find differentially\nexpressed genes. It can work without replicates (unlike edgeR) but the author\nstrongly advises against this for reasons of statistical validity.\n\n\n1.  Generate a list of differentially expressed genes using DESeq2\n\n\nIn the Galaxy tool panel, under NGS Analysis, select\n\nNGS: RNA Analysis > Differential_Count\n and set the parameters as follows:  \n\n\n\n\nSelect an input matrix - rows are contigs, columns are counts for each\n  sample:\n bams to DGE count matrix_htseqsams2mx.xls\n\n\nTitle for job outputs:\n Differential_Counts_DESeq2\n\n\nTreatment Name:\n Batch\n\n\nSelect columns containing treatment:\n  \n\n\nbatch1-accepted_hits.bam\n\n\nbatch2-accepted_hits.bam\n\n\nbatch3-accepted_hits.bam\n\n\n\n\n\n\nControl Name:\n Chem\n\n\nSelect columns containing control:\n  \n\n\nchem1-accepted_hits.bam\n\n\nchem2-accepted_hits.bam\n\n\nchem3-accepted_hits.bam\n\n\n\n\n\n\nRun this model using edgeR:\n Do not run edgeR\n\n\nRun the same model with DESeq2 and compare findings:\n Run DESeq2\n\n\n\n\n2.  Examine the outputs the previous step\n\n\n\n\nExamine the Differential_Counts_DESeq2_topTable_DESeq2.xls file.\n    This file is a list of genes sorted by p-value from using DESeq2 to\n    perform differential expression analysis.\n\n\nExamine the Differential_Counts_DESeq2.html file. This file has some\n    output logs and plots from running DESeq2. Take a look at the PCA plot.\n\n\n\n\n\n\n\n\nMore info on PCA plots\n\n\n\n\nPCA plots are useful for exploratory data analysis. Samples which are more\nsimilar to each other are expected to cluster together. A count matrix often\nhas thousands of dimensions (one for each feature) and our PCA plot generated in\nthe previous step transforms the data so the most variability is represented in principal\ncomponents 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively).\n\n\n\n\nTake note of the scales on the x-axis and the y-axis. The x-axis representing\nthe first principal component accounts for 96% of the variance and ranges from\napproximately -6 to +6, while the y-axis ranges from approximately -1 to +1.\n\n\nFor both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition.\n\n\nAdditionally, within conditions, the lower glucose (chem) condition shows more\nvariability between replicates than the higher glucose (batch) condition.\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n3.  Filter out the significant differentially expressed genes.\n\n\nUnder Basic Tools, click on \nFilter and Sort > Filter\n:\n\n\n\n\nFilter:\n \"Differential_Counts_DESeq2_topTable_DESeq2.xls\"\n\n\nWith following condition:\n c7 <= 0.05\n\n\nExecute\n\n\n\n\nThis will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 53 genes in this file.\nRename this file by clicking on the \npencil icon\n of and change the name\nfrom \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see\nthe first few differentially expressed genes are similar to the ones\nidentified by EdgeR.\n\n\n\n\nSection 7: How much concordance is there between methods?\n\n\nWe are interested in how similar the identified genes are between the different\nstatistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a\nVenn diagram to visualise the amount of overlap.\n\n\n\n\n\n\nGenerate a Venn diagram of the output of the 3 differential expression tools.\n\n    Note that column index 2 (or c3) contains the gene name in the CuffDiff output.\n    Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names.\n\n    In the Galaxy tool panel, under Statistics and Visualisation, select\n    \nGraph/Display Data > proportional venn\n and set the parameters as follows:  \n\n\n\n\ntitle:\n Common genes\n\n\ninput file 1:\n Cuffdiff_Significant_DE_Genes\n\n\ncolumn index:\n 2\n\n\nas name:\n Cuffdiff\n\n\ninput file 2:\n edgeR_Significant_DE_Genes\n\n\ncolumn index file 2:\n 0\n\n\nas name file 2:\n edgeR\n\n\ntwo or three:\n three\n\n\ninput file 3:\n DESeq2_Significant_DE_Genes\n\n\ncolumn index file 3:\n 0\n\n\nas name file 3:\n DESeq2\n\n\nExecute\n\n\n\n\n\n\n\n\nView the generated Venn diagram.\n    Agreement between the tools is good: there are 49 differentially expressed\n    genes that all three tools agree upon, and only a handful that are\n    exclusive to each tool.\n\n\n\n\n\n\n\n\nGenerate the common list of significantly expressed genes identified by the\n    three mentioned tools by extracting the respective gene list columns and\n    intersecting:\n\n\n\n\n\n\nUnder Basic Tools in the Galaxy tool panel, select\n    \nText Manipulation > cut\n\n\n\n\nCut columns:\n c3\n\n\nDelimited by:\n Tab\n\n\nFrom:\n Cuffdiff_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_gene_list'\n\n\n\n\n\n\nSelect \nText Manipulation > cut\n\n\n\n\nCut columns:\n c1\n\n\nDelimited by:\n Tab\n\n\nFrom:\n edgeR_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'edgeR_gene_list'\n\n\n\n\n\n\nSelect \nText Manipulation > cut\n\n\n\n\nCut columns:\n c1\n\n\nDelimited by:\n Tab\n\n\nFrom:\n DESeq2_Significant_DE_Genes\n\n\nExecute\n\n\n\n\nRename the output to something like 'DESeq2_gene_list'\n\n\n\n\n\n\nUnder Basic Tools in the Galaxy tool panel, select\n    \nJoin, Subtract and Group > Compare two Datasets\n\n\n\n\nCompare:\n Cuffdiff_gene_list\n\n\nagainst:\n edgeR_gene_list\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_edgeR_common_gene_list'\n\n\n\n\n\n\nSelect \nJoin, Subtract and Group > Compare two Datasets\n\n\n\n\nCompare:\n Cuffdiff_edgeR_common_gene_list\n\n\nagainst:\n DESeq2_gene_list\n\n\nUse defaults for the other fields\n\n\nExecute\n\n\n\n\nRename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list'\n\n\n\n\n\n\n\n\n\n\nWe now have a list of 49 genes that have been identified as significantly\ndifferentially expressed by all three tools.\n\n\n\n\nSection 8: Gene set enrichment analysis\n\n\nThe biological question being asked in the original paper is essentially:\n\n\n\"What is the global response of the yeast transcriptome in the shift from\ngrowth at glucose excess conditions (batch) to glucose-limited conditions\n(chemostat)?\"\n  \n\n\nWe can address this question by attempting to interpret our differentially\nexpressed gene list at a higher level, perhaps by examining the categories of\ngene and protein networks that change in response to glucose.\n\n\nFor example, we can input our list of differentially expressed genes to a Gene\nOntology (GO) enrichment analysis tool such as GOrilla to find out the GO\nenriched terms.\n\n\nNOTE: Because of time-constraints in this tutorial, the analysis was confined to\na single chromosome (chromosome I) and as a consequence we don\u2019t have\nsufficient information to look for groups of differentially expressed genes\n(simply because we don\u2019t have enough genes identified from the one chromosome to\nlook for statistically convincing over-representation of any particular gene\ngroup).\n\n\n\n\n\n\nDownload the list of genes \nhere in a plain-text file\n\n    to your local computer by right clicking on the link and selecting Save Link As...\n\n\nNote that there are ~2500 significantly differentially expressed genes\nidentified in the full analysis. Also note that the genes are ranked in\norder of statistical significance. This is critical for the next step.\n\n\n\n\n\n\nExplore the data using gene set enrichment analysis (GSEA) using the online\n    tool GOrilla\n\n\n\n\nGo to \ncbl-gorilla.cs.technion.ac.il\n\n\nChoose Organism:\n Saccharomyces cerevisiae\n\n\nChoose running mode:\n Single ranked list of genes\n\n\nOpen the gene list you downloaded in the previous step in a text editor.\n  Select the full list, then copy and paste the list into the text box.\n\n\nChoose an Ontology:\n Process\n\n\nSearch Enriched GO terms\n\n\n\n\n\n\n\n\nOnce the analysis has finished running, you will be redirected to a\n    page depicting the GO enriched biological processes and its significance\n    (indicated by colour), based on the genes you listed.\n\n\n\nScroll down to view a table of GO terms and their significance scores.\nIn the last column, you can toggle the \n[+] Show genes\n to see the\nlist of associated genes.\n\n\n\n\n\n\nExperiment with different ontology categories (Function, Component) in GOrilla.\n\n\n\n\n\n\n\n\n\n\nAt this stage you are interpreting the experiment in different ways, potentially\ndiscovering information that will lead you to further lab experiments. This\nis driven by your biological knowledge of the problem space. There are an\nunlimited number of methods for further interpretation of which GSEA is just one.\n\n\n\n\nOptional extension: Degust\n\n\nDegust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at \nvicbioinformatics.com/degust/\n.\n\n\n\n\n1. Load count data into Degust\n\n\n\n\nIn Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\"\n    generated in Section 4 using the \ndisk icon\n.\n\n\nGo to \nvicbioinformatics.com/degust/\n\n    and click on \"Upload your counts file\".\n\n\nClick \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.\n\n\n\n\n2. Configure your uploaded data\n\n\n\n\nGive your visualisation a name.\n\n\nFor the Info column, select Contig.\n\n\nAdd two conditions: batch and chem. For each condition, select the three\n    samples which correspond with the condition.\n\n\nClick \nSave changes\n and view your data.\n\n\n\n\n\n\n\n\nShow screenshot\n\n\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n3. Explore your data\n\n\nRead through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.\n\n\n\n\nOn the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.\n\n\nOn the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.\n\n\n4. Explore the demo data\n\n\nDegust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.\n\n\n\n\nReferences\n\n\n[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10",
            "title": "RNA-seq DGE Advanced"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#rna-seq-differential-gene-expression-advanced-tutorial",
            "text": "Authors: Mahtab Mirmomeni, Andrew Lonie, Jessica Chung",
            "title": "RNA-Seq Differential Gene Expression: Advanced Tutorial"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#tutorial-overview",
            "text": "In this tutorial we compare the performance of three statistically-based\nexpression analysis tools:     CuffDiff  EdgeR  DESeq2   This tutorial builds on top of the  basic RNA-seq DGE tutorial .\nIt is recommended to have some familiarity of RNA-seq before beginning this\ntutorial.",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#background-15-min",
            "text": "",
            "title": "Background [15 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#where-does-the-data-in-this-tutorial-come-from",
            "text": "The data for this tutorial is from the paper,  A comprehensive comparison of\nRNA-Seq-based transcriptome analysis from reads to differential gene expression\nand cross-comparison with microarrays: a case study in Saccharomyces\ncerevisiae  by Nookaew et al. [1] which studies S.cerevisiae strain CEN.PK\n113-7D (yeast) under two different metabolic conditions: glucose-excess (batch)\nor glucose-limited (chemostat).  The RNA-Seq data has been uploaded in NCBI, short read archive (SRA), with\naccession SRS307298. There are 6 samples in total-- two treatments with\nthree biological replicates each. The data is paired-end.    We have extracted chromosome I reads from the samples to make the\ntutorial a suitable length. This has implications, as discussed in section 8.",
            "title": "Where does the data in this tutorial come from?"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-1-preparation-15-min",
            "text": "",
            "title": "Section 1: Preparation [15 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-register-as-a-new-user-in-galaxy-if-you-dont-already-have-an-account",
            "text": "Open a browser and go to a Galaxy server. This can either be your\n    personal GVL server you  started previously ,\n    the public  Galaxy Tutorial server \n    or the public  Galaxy Melbourne server . \n    Recommended browsers include Firefox and Chrome. Internet Explorer\n    is not supported.  Register as a new user by clicking  User > Register  on the top\n    dark-grey bar. Alternatively, if you already have an account, login by\n    clicking  User > Login .",
            "title": "1.  Register as a new user in Galaxy if you don\u2019t already have an account"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-import-the-rna-seq-data-for-the-workshop",
            "text": "If you are using the public Galaxy Tutorial server or Galaxy Melbourne server,\nyou can import the data directly from Galaxy. You can do this by going to Shared Data > Published Histories  on the top toolbar, and selecting\nthe history called  RNA-Seq_Adv_Sec_1 . Then click on \"Import History\" on\nthe top right and \"start using this history\" to switch to the newly imported\nhistory.  Alternatively, if you are using your own personal Galaxy server, you can import\nthe data by:   In the tool panel located on the left, under Basic Tools select  Get\n    Data > Upload File . Click on the  Paste/Fetch data  button on the\n    bottom section of the pop-up window.  Upload the sequence data by pasting the following links into the text\n    input area.\n    These six files are three paired-end samples from the batch condition\n    (glucose-excess). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch1_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch2_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/batch3_chrI_2.fastq\n     \n     \n    These six files are three paired-end samples from the chem condition\n    (glucose-limited). Make sure the type is specified as 'fastqsanger'\n    when uploading.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem1_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem2_chrI_2.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_1.fastq\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/chem3_chrI_2.fastq\n     \n     \n    Then, upload this file of gene definitions. You don't need to specify\n    the type for this file as Galaxy will auto-detect the file as a GTF\n    file.\n     \n    https://swift.rc.nectar.org.au:8888/v1/AUTH_a3929895f9e94089ad042c9900e1ee82/RNAseqDGE_ADVNCD/genes.gtf\n       You should now have these 13 files in your history:   batch1_chrI_1.fastq  batch1_chrI_2.fastq  batch2_chrI_1.fastq  batch2_chrI_2.fastq  batch3_chrI_1.fastq  batch3_chrI_2.fastq  chem1_chrI_1.fastq  chem1_chrI_2.fastq  chem2_chrI_1.fastq  chem2_chrI_2.fastq  chem3_chrI_1.fastq  chem3_chrI_2.fastq  genes.gtf   These files can be renamed by clicking the  pen icon  if you wish.  Note:  The reads are paired end; for example batch1_chrI_1.fastq and\nbatch1_chrI_2.fastq are paired reads from one sequencing run. Low quality\nreads have already been trimmed.",
            "title": "2.  Import the RNA-seq data for the workshop."
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-2-alignment-30-mins",
            "text": "In this section we map the reads in our FASTQ files to a reference genome. As\nthese reads originate from mRNA, we expect some of them will cross exon/intron\nboundaries when we align them to the reference genome. Tophat is a splice-aware\nmapper for RNA-seq reads that is based on Bowtie. It uses the mapping results\nfrom Bowtie to identify splice junctions between exons. More information on\nTophat can be found  here .",
            "title": "Section 2: Alignment [30 mins]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-mapalign-the-reads-with-tophat-to-the-s-cerevisiae-reference",
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Tophat  and set the parameters as follows:     Is this single-end or paired-end data?  Paired-end (as individual datasets)     RNA-Seq FASTQ file, forward reads: \n(Click on the  multiple datasets icon  and select all six of the forward\nFASTQ files ending in *1.fastq. This should be correspond to every\nsecond file (1,3,5,7,9,11). This can be done by holding down the\nctrl key (Windows) or the command key (OSX) to select multiple files.)   batch1_chrI_1.fastq  batch2_chrI_1.fastq  batch3_chrI_1.fastq  chem1_chrI_1.fastq  chem2_chrI_1.fastq  chem3_chrI_1.fastq     RNA-Seq FASTQ file, reverse reads: \n(Click on the  multiple datasets icon  and select all six of the reverse\nFASTQ files ending in *2.fastq.)     batch1_chrI_2.fastq  batch2_chrI_2.fastq  batch3_chrI_2.fastq  chem1_chrI_2.fastq  chem2_chrI_2.fastq  chem3_chrI_2.fastq    Use a built in reference genome or own from your history:  Use\nbuilt-in genome  Select a reference genome:  S. cerevisiae June 2008 (SGD/SacCer2)\n(sacCer2)  Use defaults for the other fields  Execute     Show screenshot       //<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>  \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink0\").click(function(e){\n            e.preventDefault();\n            $(\"#showable0\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n      Note: This may take a few minutes, depending on how busy the server is.",
            "title": "1.  Map/align the reads with Tophat to the S. cerevisiae reference"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-rename-the-output-files",
            "text": "You should have 5 output files for each of the FASTQ input files:   Tophat on data 2 and data 1: accepted_hits:  This is a BAM file containing\n  sequence alignment data of the reads. This file contains the location\n  of where the reads mapped to in the reference genome. We will examine\n  this file more closely in the next step.  Tophat on data 2 and data 1: splice junctions:  This file lists all the places\n  where TopHat had to split a read into two pieces to span an exon\n  junction.  Tophat on data 2 and data 1 deletions  and  Tophat on data 2 and data 1: insertions: \n  These files list small insertions or deletions found in the reads.\n  Since we are working with synthetic reads we can ignore Tophat for\n  Illumina data 1:insertions Tophat for Illumina data 1:deletions for now.  Tophat on data 2 and data 1: align_summary:  This file gives some mapping\n  statistics including the number of reads mapped and the mapping rate.   You should have a total of 30 Tophat output files in your history.  Rename the 6 accepted_hits files into a more meaningful name (e.g.\n'Tophat on data 2 and data 1: accepted_hits' to 'batch1-accepted_hits.bam')\nby using the  pen icon  next to the file.",
            "title": "2.  Rename the output files"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-visualise-the-aligned-reads-with-trackster",
            "text": "On the top bar of Galaxy, select  Visualization > New Track Browser .  Name your new visualization and select S. cerevisiae (sacCer2) as the\n    reference genome build.  Click the  Add Datasets to Visualization  button and select\n    'batch1-accepted_hits.bam' and 'chem1-accepted_hits.bam' by using the\n    checkboxes on the left.  Select chrI from the dropdown box. You can zoom in and out using the\n    buttons on the top toolbar.  You can also add more tracks using the  Add Tracks icon  located on the\n    top right. Load one of the splice junction files such as 'Tophat on data 2\n    and data 1: splice junctions'.  Explore the data and try to find a splice junction. Next to the\n    drop down list, click on the chromosomal position number\n    display and specify the location  chrI:86985-87795  to view an\n    intron junction.     Ideally we would add a gene model to the visualisation; but the\ngenes.gtf file for S. cerevisae (as downloaded from UCSC Table Browser)\nhas a slightly different naming convention for one of the chromosomes\nthan the reference genome used by Galaxy, which will cause an error to\nbe thrown by Trackster if you try to add it. This is very typical of\ngenomics currently! If you are interested, you can fiddle with the\ngenes.gtf file to rename the chromosome '2-micron' to '2micron', which\nwill fix the problem.  Before starting the next section, leave the Trackster interface and return\nto the analysis view of Galaxy by clicking 'Analyze Data' on the top\nGalaxy toolbar.",
            "title": "3.  Visualise the aligned reads with Trackster"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-3-cuffdiff-40-min",
            "text": "The aim in this section is to statistically test for differential expression\nusing Cuffdiff and obtain a list of significant genes.",
            "title": "Section 3. Cuffdiff [40 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-run-cuffdiff-to-identify-differentially-expressed-genes-and-transcripts",
            "text": "In the left tool panel menu, under NGS Analysis, select NGS: RNA Analysis > Cuffdiff  and set the parameters as follows:   Transcripts:  genes.gtf  Condition:     1: Condition  name  batch  Replicates:  batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam \n(Multiple datasets can be selected by holding down the shift key or\nthe ctrl key (Windows) or the command key (OSX).)      2: Condition  name  chem  Replicates:  chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam          Use defaults for the other fields  Execute     Show screenshot       \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n      Note: This step may take a while, depending on how busy the server is.",
            "title": "1.  Run Cuffdiff to identify differentially expressed genes and transcripts"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-explore-the-cuffdiff-output-files",
            "text": "There should be 11 output files from Cuffdiff. These files should all begin\nwith something like \"Cuffdiff on data 43, data 38, and others\". We'll\nmostly be interested in the file ending with 'gene differential expression\ntesting' which contains the statistical results from testing the level of\ngene expression between the batch condition and chem condition.  Filter based on column 14 (\u2018significant\u2019) - a binary assessment of\nq_value > 0.05, where q_value is p_value adjusted for multiple testing.\nUnder Basic Tools, click on  Filter and Sort > Filter :   Filter:  \"Cuffdiff on data....: gene differential expression testing\"  With following condition:  c14=='yes'  Execute   This will keep only those entries that Cuffdiff has marked as\nsignificantly differentially expressed. There should be 53 differentially\nexpressed genes in this list.  We can rename this file by clicking on the  pencil icon  of\nthe outputted file and change the name from \"Filter on data x\" to\n\"Cuffdiff_Significant_DE_Genes\".",
            "title": "2.  Explore the Cuffdiff output files"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-4-count-reads-in-features-30-min",
            "text": "HTSeq-count creates a count matrix using the number of the reads from each bam\nfile that map to the genomic features in the genes.gtf. For each feature (a\ngene for example) a count matrix shows how many reads were mapped to this\nfeature.    Use HTSeq-count to count the number of reads for each feature. \n    In the left tool panel menu, under NGS Analysis, select\n     NGS: RNA Analysis > SAM/BAM to count matrix  and set the parameters as follows:     Gene model (GFF) file to count reads over from your current history:  genes.gtf  bam/sam file from your history: \n  (Select all six bam files using the shift key.)  batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam  chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Use defaults for the other fields  Execute     Examine the outputted matrix by using the  eye icon . \n    Each column corresponds to a sample and each row corresponds to a gene. By\n    sight, see if you can find a gene you think is differentially expressed\n    from looking at the counts.    We now have a count matrix, with a count against each corresponding sample. We\nwill use this matrix in later sections to calculate the differentially\nexpressed genes.",
            "title": "Section 4. Count reads in features [30 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-5-edger-30-min",
            "text": "edgeR \nis an R package, that is used for analysing differential expression of\nRNA-Seq data and can either use exact statistical methods or generalised\nlinear models.",
            "title": "Section 5: edgeR  [30 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-edger",
            "text": "In the Galaxy tool panel, under NGS Analysis, select NGS: RNA > Differential_Count  and set the parameters as follows:   Select an input matrix - rows are contigs, columns are counts for each\n  sample:  bams to DGE count matrix_htseqsams2mx.xls  Title for job outputs:  Differential_Counts_edgeR  Treatment Name:  Batch  Select columns containing treatment:     batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam    Control Name:  Chem  Select columns containing control:     chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Run this model using edgeR:  Run edgeR  Use defaults for the other fields  Execute",
            "title": "1.  Generate a list of differentially expressed genes using edgeR"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-from-the-previous-step",
            "text": "Examine the Differential_Counts_edgeR_topTable_edgeR.xls file by\n    clicking on the  eye icon .\n    This file is a list of genes sorted by p-value from using EdgeR to\n    perform differential expression analysis.  Examine the Differential_Counts_edgeR.html file. This file has some\n    output logs and plots from running edgeR. If you are familiar with R,\n    you can examine the R code used for analysis by scrolling to the bottom\n    of the file, and clicking Differential_Counts.Rscript to download the\n    Rscript file. \n    If you are curious about the statistical methods edgeR uses, you can\n    read the  edgeR user's guide at\n    Bioconductor .",
            "title": "2.  Examine the outputs from the previous step"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-extract-the-significant-differentially-expressed-genes",
            "text": "Under Basic Tools, click on  Filter and Sort > Filter :   Filter:  \"Differential_Counts_edgeR_topTable_edgeR.xls\"  With following condition:  c6 <= 0.05  Execute   This will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 55 genes in this file.\nRename this file by clicking on the  pencil icon  of and change the name\nfrom \"Filter on data x\" to \"edgeR_Significant_DE_Genes\".",
            "title": "3.  Extract the significant differentially expressed genes."
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-6-deseq2-30-min",
            "text": "DESeq2  is an\nR package that uses a negative binomial statistical model to find differentially\nexpressed genes. It can work without replicates (unlike edgeR) but the author\nstrongly advises against this for reasons of statistical validity.",
            "title": "Section 6. DESeq2 [30 min]"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-generate-a-list-of-differentially-expressed-genes-using-deseq2",
            "text": "In the Galaxy tool panel, under NGS Analysis, select NGS: RNA Analysis > Differential_Count  and set the parameters as follows:     Select an input matrix - rows are contigs, columns are counts for each\n  sample:  bams to DGE count matrix_htseqsams2mx.xls  Title for job outputs:  Differential_Counts_DESeq2  Treatment Name:  Batch  Select columns containing treatment:     batch1-accepted_hits.bam  batch2-accepted_hits.bam  batch3-accepted_hits.bam    Control Name:  Chem  Select columns containing control:     chem1-accepted_hits.bam  chem2-accepted_hits.bam  chem3-accepted_hits.bam    Run this model using edgeR:  Do not run edgeR  Run the same model with DESeq2 and compare findings:  Run DESeq2",
            "title": "1.  Generate a list of differentially expressed genes using DESeq2"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-examine-the-outputs-the-previous-step",
            "text": "Examine the Differential_Counts_DESeq2_topTable_DESeq2.xls file.\n    This file is a list of genes sorted by p-value from using DESeq2 to\n    perform differential expression analysis.  Examine the Differential_Counts_DESeq2.html file. This file has some\n    output logs and plots from running DESeq2. Take a look at the PCA plot.     More info on PCA plots   PCA plots are useful for exploratory data analysis. Samples which are more\nsimilar to each other are expected to cluster together. A count matrix often\nhas thousands of dimensions (one for each feature) and our PCA plot generated in\nthe previous step transforms the data so the most variability is represented in principal\ncomponents 1 and 2 (PC1 and PC2 represented by the x-axis and y-axis respectively).   Take note of the scales on the x-axis and the y-axis. The x-axis representing\nthe first principal component accounts for 96% of the variance and ranges from\napproximately -6 to +6, while the y-axis ranges from approximately -1 to +1.  For both conditions, the 3 replicates tend to be closer to each other than they are to replicates from the other condition.  Additionally, within conditions, the lower glucose (chem) condition shows more\nvariability between replicates than the higher glucose (batch) condition.     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "2.  Examine the outputs the previous step"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-filter-out-the-significant-differentially-expressed-genes",
            "text": "Under Basic Tools, click on  Filter and Sort > Filter :   Filter:  \"Differential_Counts_DESeq2_topTable_DESeq2.xls\"  With following condition:  c7 <= 0.05  Execute   This will keep the genes that have an adjusted p-value of less\nor equal to 0.05. There should be 53 genes in this file.\nRename this file by clicking on the  pencil icon  of and change the name\nfrom \"Filter on data x\" to \"DESeq2_Significant_DE_Genes\". You should see\nthe first few differentially expressed genes are similar to the ones\nidentified by EdgeR.",
            "title": "3.  Filter out the significant differentially expressed genes."
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-7-how-much-concordance-is-there-between-methods",
            "text": "We are interested in how similar the identified genes are between the different\nstatistial methods used by Cuffdiff, edgeR, and DESeq2. We can generate a\nVenn diagram to visualise the amount of overlap.    Generate a Venn diagram of the output of the 3 differential expression tools. \n    Note that column index 2 (or c3) contains the gene name in the CuffDiff output.\n    Similarly column index 0 (or c1) in EdgeR and DESeq2 contain the gene names. \n    In the Galaxy tool panel, under Statistics and Visualisation, select\n     Graph/Display Data > proportional venn  and set the parameters as follows:     title:  Common genes  input file 1:  Cuffdiff_Significant_DE_Genes  column index:  2  as name:  Cuffdiff  input file 2:  edgeR_Significant_DE_Genes  column index file 2:  0  as name file 2:  edgeR  two or three:  three  input file 3:  DESeq2_Significant_DE_Genes  column index file 3:  0  as name file 3:  DESeq2  Execute     View the generated Venn diagram.\n    Agreement between the tools is good: there are 49 differentially expressed\n    genes that all three tools agree upon, and only a handful that are\n    exclusive to each tool.     Generate the common list of significantly expressed genes identified by the\n    three mentioned tools by extracting the respective gene list columns and\n    intersecting:    Under Basic Tools in the Galaxy tool panel, select\n     Text Manipulation > cut   Cut columns:  c3  Delimited by:  Tab  From:  Cuffdiff_Significant_DE_Genes  Execute   Rename the output to something like 'Cuffdiff_gene_list'    Select  Text Manipulation > cut   Cut columns:  c1  Delimited by:  Tab  From:  edgeR_Significant_DE_Genes  Execute   Rename the output to something like 'edgeR_gene_list'    Select  Text Manipulation > cut   Cut columns:  c1  Delimited by:  Tab  From:  DESeq2_Significant_DE_Genes  Execute   Rename the output to something like 'DESeq2_gene_list'    Under Basic Tools in the Galaxy tool panel, select\n     Join, Subtract and Group > Compare two Datasets   Compare:  Cuffdiff_gene_list  against:  edgeR_gene_list  Use defaults for the other fields  Execute   Rename the output to something like 'Cuffdiff_edgeR_common_gene_list'    Select  Join, Subtract and Group > Compare two Datasets   Compare:  Cuffdiff_edgeR_common_gene_list  against:  DESeq2_gene_list  Use defaults for the other fields  Execute   Rename the output to something like 'Cuffdiff_edgeR_DESeq2_common_gene_list'      We now have a list of 49 genes that have been identified as significantly\ndifferentially expressed by all three tools.",
            "title": "Section 7: How much concordance is there between methods?"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#section-8-gene-set-enrichment-analysis",
            "text": "The biological question being asked in the original paper is essentially:  \"What is the global response of the yeast transcriptome in the shift from\ngrowth at glucose excess conditions (batch) to glucose-limited conditions\n(chemostat)?\"     We can address this question by attempting to interpret our differentially\nexpressed gene list at a higher level, perhaps by examining the categories of\ngene and protein networks that change in response to glucose.  For example, we can input our list of differentially expressed genes to a Gene\nOntology (GO) enrichment analysis tool such as GOrilla to find out the GO\nenriched terms.  NOTE: Because of time-constraints in this tutorial, the analysis was confined to\na single chromosome (chromosome I) and as a consequence we don\u2019t have\nsufficient information to look for groups of differentially expressed genes\n(simply because we don\u2019t have enough genes identified from the one chromosome to\nlook for statistically convincing over-representation of any particular gene\ngroup).    Download the list of genes  here in a plain-text file \n    to your local computer by right clicking on the link and selecting Save Link As...  Note that there are ~2500 significantly differentially expressed genes\nidentified in the full analysis. Also note that the genes are ranked in\norder of statistical significance. This is critical for the next step.    Explore the data using gene set enrichment analysis (GSEA) using the online\n    tool GOrilla   Go to  cbl-gorilla.cs.technion.ac.il  Choose Organism:  Saccharomyces cerevisiae  Choose running mode:  Single ranked list of genes  Open the gene list you downloaded in the previous step in a text editor.\n  Select the full list, then copy and paste the list into the text box.  Choose an Ontology:  Process  Search Enriched GO terms     Once the analysis has finished running, you will be redirected to a\n    page depicting the GO enriched biological processes and its significance\n    (indicated by colour), based on the genes you listed.  Scroll down to view a table of GO terms and their significance scores.\nIn the last column, you can toggle the  [+] Show genes  to see the\nlist of associated genes.    Experiment with different ontology categories (Function, Component) in GOrilla.      At this stage you are interpreting the experiment in different ways, potentially\ndiscovering information that will lead you to further lab experiments. This\nis driven by your biological knowledge of the problem space. There are an\nunlimited number of methods for further interpretation of which GSEA is just one.",
            "title": "Section 8: Gene set enrichment analysis"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#optional-extension-degust",
            "text": "Degust is an interactive visualiser for analysing RNA-seq data. It runs as a\nweb service and can be found at  vicbioinformatics.com/degust/ .",
            "title": "Optional extension: Degust"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#1-load-count-data-into-degust",
            "text": "In Galaxy, download the count data \"bams to DGE count matrix_htseqsams2mx.xls\"\n    generated in Section 4 using the  disk icon .  Go to  vicbioinformatics.com/degust/ \n    and click on \"Upload your counts file\".  Click \"Choose file\" and upload the recently downloaded Galaxy tabular file\n    containing your RNA-seq counts.",
            "title": "1. Load count data into Degust"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#2-configure-your-uploaded-data",
            "text": "Give your visualisation a name.  For the Info column, select Contig.  Add two conditions: batch and chem. For each condition, select the three\n    samples which correspond with the condition.  Click  Save changes  and view your data.     Show screenshot       \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink3\").click(function(e){\n            e.preventDefault();\n            $(\"#showable3\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "2. Configure your uploaded data"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#3-explore-your-data",
            "text": "Read through the Degust tour of features. Explore the parallel coordinates plot,\nMA plot, MDS plot, heatmap and gene list. Each is fully interactive and\ninfluences other portions on the display depending on what is selected.   On the right side of the page is an options module which can set thresholds to\nfilter genes using statistical significance or absolute-fold-change.  On the left side is a dropdown box you can specify the method (Voom/Limma or\nedgeR) used to perform differential expression analysis on the data. You can\nalso view the R code by clicking \"Show R code\" under the options module on\nthe right.",
            "title": "3. Explore your data"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#4-explore-the-demo-data",
            "text": "Degust also provides an example dataset with 4 conditions and more genes. You\ncan play with the demo dataset by clicking on the \"Try the demo\" button on the\nDegust homepage. The demo dataset includes a column with an EC number for each\ngene. This means genes can be displayed on Kegg pathways using the module on\nthe right.",
            "title": "4. Explore the demo data"
        },
        {
            "location": "/tutorials/rna_seq_dge_advanced/rna_seq_advanced_tutorial/#references",
            "text": "[1] Nookaew I, Papini M, Pornputtpong N, Scalcinati G, Fagerberg L, Uhl\u00e9n M, Nielsen J: A comprehensive comparison of RNA-Seq-based transcriptome analysis from reads to differential gene expression and cross-comparison with microarrays: a case study in Saccharomyces cerevisiae. Nucleic Acids Res 2012, 40 (20):10084 \u2013 10097. doi:10.1093/nar/gks804. Epub 2012 Sep 10",
            "title": "References"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/",
            "text": "RNA-Seq Experimental Design\n\n\nWhat is RNA-seq?\n\n\nRNA-seq is a method of measuring gene expression using shotgun sequencing. The\nprocess involves reverse transcribing RNA into cDNA, then sequencing fragments\non a high-throughput platform such as Illumina to obtain a large number of\nshort reads. For each sample, the reads are then aligned to a genome, and the\nnumber of reads aligned to each gene or feature is recorded.\n\n\nA typical RNA-seq experiment aims to find differentially expressed genes\nbetween two conditions (e.g. up and down-regulated genes in knock-out mice\ncompared to wild-type mice). RNA-seq can also be used to discover new\ntranscripts, splice variants, and fusion genes.\n\n\nWhy is a good experimental design vital?\n\n\nAn RNA-seq experiment produces high dimensional data. This means we get a huge\nnumber of observations for a small number of samples. For example, the\nexpression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3\nwild-type). A frequently used approach to analyse RNA-seq data is to fit each\ngene to a linear model where for each of the 20,000 genes, parameters need to\nbe estimated using a small number of observations. To complicate matters, each\nmeasurement of gene expression is comprised of a mix of biological signal and\nunwanted noise. Thus, in order to perform a robust statistical analysis, the\nmethodology must be carefully designed.\n\n\nBefore you begin any RNA-seq experiment, some questions you should ask\nyourself are:\n\n\n\n\nWhy do you expect to find differentially expressed genes in the particular\n   tissue?\n\n\nWhat types of genes do you expect to find differentially expressed?\n\n\nWhat are the sources of variability from your samples?\n\n\nWhere do you expect most of your variation to come from?\n\n\n\n\nA coherent experimental design is the groundwork of a successful experiment.\nYou should invest time and thought in designing a robust experiment as failing\nto think this step through can lead to unusable data and wasted time, money,\nand effort.\n\n\nIt is also useful to think about the statistical methods you will use to\nanalyse the data. If you're planning to bring a data analyst or\nbioinformatician onboard for data analysis, you should include him or her in\nthe experimental design stage.\n\n\nTerminology\n\n\nBefore progressing, it may be useful to define some terms which are commonly\nused in RNA-seq.\n\n\n\n  \n\n    \nVariability:\n    \nA measure of how much the data is spread around. Variance is\n    mathematically defined as the average of the squared difference between\n    observations and the expected value. Simply put, a larger variance means\n    it is harder to identify differentially expressed genes.\n  \n\n    \nFeature:\n    \nA defined genomic region. Usually a gene in RNA-seq, but can also\n    refer to any region such as an exon or an isoform. In RNA-seq, an estimate\n    of abundance is obtained for each feature.\n  \n\n    \nBiological replicates:\n    \nSamples that have been obtained from biologically separate samples.\n    This can mean different individual organisms (e.g. tissue samples from\n    different mice), different samplings of the same tumour, or different\n    population of cells grown separately from each other but originating from\n    the same cell-line. For example, the samples obtained from three different\n    knock-out mice could be considered biological replicates in a knock-out\n    versus wild-type experiment. A biological replicate combines both technical\n    and biological variability as it is also an independent case of all the\n    technical steps.\n  \n\n    \nTechnical replicates:\n    \nSamples in which the starting biological sample is the same, but the\n    replicates are processed separately. For example, if a biological sample\n    is divided and two different library preps are processed and sequenced,\n    those two samples would be considered technical replicates.\n  \n\n    \nCovariate:\n    \nThe term 'covariate' is often used interchangeably with 'factor' or\n    'variable' in RNA-seq. The term refers to a property of the sample which\n    may have some influence on gene expression and should be represented in\n    the RNA-seq model. Covariates in RNA-seq are often categorical (e.g.\n    treatment condition, sex, batch), but continuous factors are also possible\n    (e.g. time points, age). A linear model will contain terms to represent\n    the relationships between covariates and each sample. Each possible value\n    a factor can take is called a level (e.g. 'male' and 'female' are two\n    levels in the factor 'sex'). Factors can either be directly of interest\n    to the experiment (e.g. treatment condition) or not of interest (also\n    known as nuisance variables) (e.g. sex, batch). The purpose of covariates\n    is to explain the variance seen in samples.\n  \n\n    \nConfounding variable:\n    \nA confounding variable is a nuisance variable that is associated with\n    the factor of interest. Possible confounding factors should be controlled\n    for so they don't interfere with analysis. For example, if all knock-out\n    mice samples were harvested in the morning and all wild-type mice samples\n    were harvested in the afternoon, the time of sample collection would be a\n    confounding factor as the effects from sample collection time and from the\n    knock-out cannot be separated.\n  \n\n    \nStatistical power:\n    \nThe ability to identify differentially expressed genes when there\n    really is a difference. This is partly dependent on variance and therefore\n    is affected by the number of replicates available and sequencing depth.\n\n\n\n\nThe importance of replicates to estimate variance\n\n\nWhen performing a differential gene expression analysis, we look at the\nexpression values of each gene and try to determine if the expression is\nsignificantly different in the different conditions (e.g. knock-out and\nwild-type). The ability to distinguish whether a gene is differentially\nexpressed is partly determined by the estimates of variability obtained by\nusing multiple observations in each condition.\n\n\nVariability is present in two forms: technical variability and biological\nvariability.\n\n\nCombined biological and technical variability is measured using biological \nreplicates. Biological variability is the main source of variability and is \ndue to natural variation in the population and within cells. This includes \ndifferent individuals having different levels of a particular gene and the \nstochastic nature of expression levels in different cells.\n\n\nTechnical variability is measured using technical replicates. Technical\nvariability is often very small compared to biological variability.  Usually\nthe question is whether an observed difference is greater than the total \nvariability (i.e. significant).  As combined variability is measured by \nbiological replicates technical replicates are only important if you need to\nknow the degree of biological variability or technical variability.  An\nexample of wanting technical variability would be method development. The\nmain source of technical variation comes from RNA processing and from \nlibrary prep. Variability from sequencing in different flow cells or different\nlanes is usually minimal. Generally, creating technical replicates from multiple\nlibrary preps is unnecessary for RNA-seq experiments.\n\n\nThe amount of variance between your biological replicates will affect the\noutcome of your analysis. Ideally, you aim to have minimal variability between\nsamples so you only measure the effect of the condition of interest. Too much\nvariability between samples can drown out the signal of truly differentially\nexpressed genes. Controlling for possible confounding factors between\nconditions is also important to prevent falsely attributing differential\nexpression to the condition of interest.\n\n\nStrategies to minimise variation between samples and to control confounding\nvariables include:\n\n\n\n\nchoosing organisms from the same litter,\n\n\nchoosing organisms of the same sex if possible,\n\n\nusing a constant sample collection time,\n\n\nhaving the same laboratory technician perform each library prep,\n\n\nrandomising samples to prevent a confounding batch effect if all samples\n   can't be processed at one time.\n\n\n\n\nIf variation between samples can not be removed it should be balanced between\nconditions of interest as much as possible, and carefully recorded to allow\nits effect to be measured and potentially removed during analysis.\n\n\nHow many replicates and how many reads do I need?\n\n\nTwo very common question asked are:\n\n\n\n\nhow many biological replicates do I need, and\n\n\nwhat sequencing depth is needed for each sample\n\n\n\n\nin order to have enough statistical power for my RNA-seq experiment?\n\n\nThese questions cannot be precisely answered without a pilot study. A small\namount of data (minimum of two biological replicates for each condition with\nat least 10M reads) can estimate the amount of biological variation, which\ndetermines how many biological replicates are required. Performing a pilot\nstudy is highly recommended to estimate statistical power and identify\npossible problems before investing more time and money into the project.\n\n\nScotty\n is a web-based tool\nthat uses data generated from a pilot study to optimize a design for\nstatistical power. With a limited budget, one must balance sequence coverage\nand number of biological replicates. Scotty also has a cost estimate feature\nwhich returns the most powerful design within budget constraints.\n\n\nAs a general rule, the number of biological replicates should never be below 3.\nFor a basic RNA-seq differential expression experiment, 10M to 20M reads per\nsample is usually enough.  If similar data exists it can be helpful to check\nthe read counts for key genes of interest to estimate the required depth.\n\n\nBiological variability is usually the largest effect limiting the power of \nRNA-seq analysis.  The most improvement in an experiment will usually be \nachieved by increasing the biological replication to improve estimation of \nthe biological variation.\n\n\nIt is often possible to design experiments where the analysis is done \nincrementally such that a pilot study is added to with an additional block of\nsamples or a pool of libraries is sequenced to additional depth. In these cases\ncare must be taken to balance the design in a manner that each stage is a\nvalid experiment in its own right.  This can allow a focused question to be \nanswered in the first stage, with an ability to either address issues or \nprogress to a second stage with additional questions.\n\n\nSequencing options to consider\n\n\nHow much total RNA is needed:\n\nMany sequencing centres such as\n\nAGRF\n\nrecommend at least 250ng of total RNA for RNA sequencing. It is possible to go\nas low as 100ng of total RNA, but results are not guaranteed. The quality of\nRNA is also important when making libraries. A RNA Integrity Number (RIN) is a\nnumber from 1 (poor) to 10 (good) and can indicate how much degradation there\nis in the sample. A poor score can lead to over representation at the 3' end\nof the transcript and low yield. Samples with low RIN scores (below 8) are\nnot recommended for sequencing.  Care should also be taken to ensure RIN is\nconsistent between conditions to avoid confounding this technical effect with\nthe biological question.\n\n\nChoosing an enrichment method:\n\nRibosomal RNA makes up >95% of total cellular RNA, so a preparation for\nRNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA.\nPoly-A enrichment is recommended for most standard RNA-seq experiments, but\nwill not provide information about microRNAs and other non-coding RNA species.\nIn general, ribo-depleted RNA-seq data will contain more noise, however, the\nprotocol is recommended if you have poor or variable quality of RNA as the 3\u2019\nbias of poly-A enrichment will be more pronounced with increased RNA\ndegradation. The amount of RNA needed for each method differs. For Poly-A\nenrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of\n200ng is recommended.\n\n\nChoosing read type:\n\nFor basic differential expression analysis RNA-seq experiments, single-end\nsequencing is recommended to obtain gene transcript counts. In more advanced\nexperiments, paired-ends are useful for determining transcript structure and\ndiscovering splice variants.\n\n\nChoosing strandedness:\n  \n\n\nWith a non-directional (unstranded) protocol, there is no way to identify\nwhether a read originated from the coding strand or its reverse complement.\nNon-directional protocols allow mapping of a read to a genomic location, but\nnot the direction in which the RNA was transcribed. They are therefore used to\ncount transcripts for known genes, and are recommended for basic RNA-seq\nexperiments. Directional protocols (stranded) preserve strand information and\nare useful for novel transcript discovery.\n\n\nMultiplexing:\n\nMultiplexing is an approach to sequence multiple samples in the same\nsequencing lane. By sequencing all samples in the same lane, multiplexing can\nalso minimise bias from lane effects.\n\n\nSpike-in controls:\n\nRNA-seq spike-in controls are a set of synthetic RNAs of known concentration\nwhich act as negative or positive controls. These controls have been used for\nnormalisation and quality control, but recent work has shown that the amount\nof technical variability in their use dramatically reduces their utility.\n\n\nSummary\n\n\n\n\nA good experimental design is vital for a successful experiment. If you're\n   planning to work with a data analyst or bioinformatician, include them in\n   the design stage.\n\n\nAim to minimise variability by identifying possible sources of variance in\n   your samples.\n\n\nBiological replicates are important. The number of biological replicates\n   you should have should never be below 3. Technical replicates are often\n   unnecessary.\n\n\nPilot studies are highly recommended for identifying how many replicates\n   and how many reads you should have for enough statistical power in your\n   experiment.\n\n\nFor basic RNA-seq experiments, poly-A enriched, single-ended, unstranded\n   sequencing at depths of 10M to 20M is probably what you want.",
            "title": "RNA-seq DGE Experimental Design"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#rna-seq-experimental-design",
            "text": "",
            "title": "RNA-Seq Experimental Design"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#what-is-rna-seq",
            "text": "RNA-seq is a method of measuring gene expression using shotgun sequencing. The\nprocess involves reverse transcribing RNA into cDNA, then sequencing fragments\non a high-throughput platform such as Illumina to obtain a large number of\nshort reads. For each sample, the reads are then aligned to a genome, and the\nnumber of reads aligned to each gene or feature is recorded.  A typical RNA-seq experiment aims to find differentially expressed genes\nbetween two conditions (e.g. up and down-regulated genes in knock-out mice\ncompared to wild-type mice). RNA-seq can also be used to discover new\ntranscripts, splice variants, and fusion genes.",
            "title": "What is RNA-seq?"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#why-is-a-good-experimental-design-vital",
            "text": "An RNA-seq experiment produces high dimensional data. This means we get a huge\nnumber of observations for a small number of samples. For example, the\nexpression of ~20,000 genes could be measured for 6 samples (3 knock-out and 3\nwild-type). A frequently used approach to analyse RNA-seq data is to fit each\ngene to a linear model where for each of the 20,000 genes, parameters need to\nbe estimated using a small number of observations. To complicate matters, each\nmeasurement of gene expression is comprised of a mix of biological signal and\nunwanted noise. Thus, in order to perform a robust statistical analysis, the\nmethodology must be carefully designed.  Before you begin any RNA-seq experiment, some questions you should ask\nyourself are:   Why do you expect to find differentially expressed genes in the particular\n   tissue?  What types of genes do you expect to find differentially expressed?  What are the sources of variability from your samples?  Where do you expect most of your variation to come from?   A coherent experimental design is the groundwork of a successful experiment.\nYou should invest time and thought in designing a robust experiment as failing\nto think this step through can lead to unusable data and wasted time, money,\nand effort.  It is also useful to think about the statistical methods you will use to\nanalyse the data. If you're planning to bring a data analyst or\nbioinformatician onboard for data analysis, you should include him or her in\nthe experimental design stage.",
            "title": "Why is a good experimental design vital?"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#terminology",
            "text": "Before progressing, it may be useful to define some terms which are commonly\nused in RNA-seq.  \n   \n     Variability:\n     A measure of how much the data is spread around. Variance is\n    mathematically defined as the average of the squared difference between\n    observations and the expected value. Simply put, a larger variance means\n    it is harder to identify differentially expressed genes.\n   \n     Feature:\n     A defined genomic region. Usually a gene in RNA-seq, but can also\n    refer to any region such as an exon or an isoform. In RNA-seq, an estimate\n    of abundance is obtained for each feature.\n   \n     Biological replicates:\n     Samples that have been obtained from biologically separate samples.\n    This can mean different individual organisms (e.g. tissue samples from\n    different mice), different samplings of the same tumour, or different\n    population of cells grown separately from each other but originating from\n    the same cell-line. For example, the samples obtained from three different\n    knock-out mice could be considered biological replicates in a knock-out\n    versus wild-type experiment. A biological replicate combines both technical\n    and biological variability as it is also an independent case of all the\n    technical steps.\n   \n     Technical replicates:\n     Samples in which the starting biological sample is the same, but the\n    replicates are processed separately. For example, if a biological sample\n    is divided and two different library preps are processed and sequenced,\n    those two samples would be considered technical replicates.\n   \n     Covariate:\n     The term 'covariate' is often used interchangeably with 'factor' or\n    'variable' in RNA-seq. The term refers to a property of the sample which\n    may have some influence on gene expression and should be represented in\n    the RNA-seq model. Covariates in RNA-seq are often categorical (e.g.\n    treatment condition, sex, batch), but continuous factors are also possible\n    (e.g. time points, age). A linear model will contain terms to represent\n    the relationships between covariates and each sample. Each possible value\n    a factor can take is called a level (e.g. 'male' and 'female' are two\n    levels in the factor 'sex'). Factors can either be directly of interest\n    to the experiment (e.g. treatment condition) or not of interest (also\n    known as nuisance variables) (e.g. sex, batch). The purpose of covariates\n    is to explain the variance seen in samples.\n   \n     Confounding variable:\n     A confounding variable is a nuisance variable that is associated with\n    the factor of interest. Possible confounding factors should be controlled\n    for so they don't interfere with analysis. For example, if all knock-out\n    mice samples were harvested in the morning and all wild-type mice samples\n    were harvested in the afternoon, the time of sample collection would be a\n    confounding factor as the effects from sample collection time and from the\n    knock-out cannot be separated.\n   \n     Statistical power:\n     The ability to identify differentially expressed genes when there\n    really is a difference. This is partly dependent on variance and therefore\n    is affected by the number of replicates available and sequencing depth.",
            "title": "Terminology"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#the-importance-of-replicates-to-estimate-variance",
            "text": "When performing a differential gene expression analysis, we look at the\nexpression values of each gene and try to determine if the expression is\nsignificantly different in the different conditions (e.g. knock-out and\nwild-type). The ability to distinguish whether a gene is differentially\nexpressed is partly determined by the estimates of variability obtained by\nusing multiple observations in each condition.  Variability is present in two forms: technical variability and biological\nvariability.  Combined biological and technical variability is measured using biological \nreplicates. Biological variability is the main source of variability and is \ndue to natural variation in the population and within cells. This includes \ndifferent individuals having different levels of a particular gene and the \nstochastic nature of expression levels in different cells.  Technical variability is measured using technical replicates. Technical\nvariability is often very small compared to biological variability.  Usually\nthe question is whether an observed difference is greater than the total \nvariability (i.e. significant).  As combined variability is measured by \nbiological replicates technical replicates are only important if you need to\nknow the degree of biological variability or technical variability.  An\nexample of wanting technical variability would be method development. The\nmain source of technical variation comes from RNA processing and from \nlibrary prep. Variability from sequencing in different flow cells or different\nlanes is usually minimal. Generally, creating technical replicates from multiple\nlibrary preps is unnecessary for RNA-seq experiments.  The amount of variance between your biological replicates will affect the\noutcome of your analysis. Ideally, you aim to have minimal variability between\nsamples so you only measure the effect of the condition of interest. Too much\nvariability between samples can drown out the signal of truly differentially\nexpressed genes. Controlling for possible confounding factors between\nconditions is also important to prevent falsely attributing differential\nexpression to the condition of interest.  Strategies to minimise variation between samples and to control confounding\nvariables include:   choosing organisms from the same litter,  choosing organisms of the same sex if possible,  using a constant sample collection time,  having the same laboratory technician perform each library prep,  randomising samples to prevent a confounding batch effect if all samples\n   can't be processed at one time.   If variation between samples can not be removed it should be balanced between\nconditions of interest as much as possible, and carefully recorded to allow\nits effect to be measured and potentially removed during analysis.",
            "title": "The importance of replicates to estimate variance"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#how-many-replicates-and-how-many-reads-do-i-need",
            "text": "Two very common question asked are:   how many biological replicates do I need, and  what sequencing depth is needed for each sample   in order to have enough statistical power for my RNA-seq experiment?  These questions cannot be precisely answered without a pilot study. A small\namount of data (minimum of two biological replicates for each condition with\nat least 10M reads) can estimate the amount of biological variation, which\ndetermines how many biological replicates are required. Performing a pilot\nstudy is highly recommended to estimate statistical power and identify\npossible problems before investing more time and money into the project.  Scotty  is a web-based tool\nthat uses data generated from a pilot study to optimize a design for\nstatistical power. With a limited budget, one must balance sequence coverage\nand number of biological replicates. Scotty also has a cost estimate feature\nwhich returns the most powerful design within budget constraints.  As a general rule, the number of biological replicates should never be below 3.\nFor a basic RNA-seq differential expression experiment, 10M to 20M reads per\nsample is usually enough.  If similar data exists it can be helpful to check\nthe read counts for key genes of interest to estimate the required depth.  Biological variability is usually the largest effect limiting the power of \nRNA-seq analysis.  The most improvement in an experiment will usually be \nachieved by increasing the biological replication to improve estimation of \nthe biological variation.  It is often possible to design experiments where the analysis is done \nincrementally such that a pilot study is added to with an additional block of\nsamples or a pool of libraries is sequenced to additional depth. In these cases\ncare must be taken to balance the design in a manner that each stage is a\nvalid experiment in its own right.  This can allow a focused question to be \nanswered in the first stage, with an ability to either address issues or \nprogress to a second stage with additional questions.",
            "title": "How many replicates and how many reads do I need?"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#sequencing-options-to-consider",
            "text": "How much total RNA is needed: \nMany sequencing centres such as AGRF \nrecommend at least 250ng of total RNA for RNA sequencing. It is possible to go\nas low as 100ng of total RNA, but results are not guaranteed. The quality of\nRNA is also important when making libraries. A RNA Integrity Number (RIN) is a\nnumber from 1 (poor) to 10 (good) and can indicate how much degradation there\nis in the sample. A poor score can lead to over representation at the 3' end\nof the transcript and low yield. Samples with low RIN scores (below 8) are\nnot recommended for sequencing.  Care should also be taken to ensure RIN is\nconsistent between conditions to avoid confounding this technical effect with\nthe biological question.  Choosing an enrichment method: \nRibosomal RNA makes up >95% of total cellular RNA, so a preparation for\nRNA-seq must either enrich for mRNA using poly-A enrichment, or deplete rRNA.\nPoly-A enrichment is recommended for most standard RNA-seq experiments, but\nwill not provide information about microRNAs and other non-coding RNA species.\nIn general, ribo-depleted RNA-seq data will contain more noise, however, the\nprotocol is recommended if you have poor or variable quality of RNA as the 3\u2019\nbias of poly-A enrichment will be more pronounced with increased RNA\ndegradation. The amount of RNA needed for each method differs. For Poly-A\nenrichment a minimum of 100ng is needed while for ribo-depletion, a minimum of\n200ng is recommended.  Choosing read type: \nFor basic differential expression analysis RNA-seq experiments, single-end\nsequencing is recommended to obtain gene transcript counts. In more advanced\nexperiments, paired-ends are useful for determining transcript structure and\ndiscovering splice variants.  Choosing strandedness:     With a non-directional (unstranded) protocol, there is no way to identify\nwhether a read originated from the coding strand or its reverse complement.\nNon-directional protocols allow mapping of a read to a genomic location, but\nnot the direction in which the RNA was transcribed. They are therefore used to\ncount transcripts for known genes, and are recommended for basic RNA-seq\nexperiments. Directional protocols (stranded) preserve strand information and\nare useful for novel transcript discovery.  Multiplexing: \nMultiplexing is an approach to sequence multiple samples in the same\nsequencing lane. By sequencing all samples in the same lane, multiplexing can\nalso minimise bias from lane effects.  Spike-in controls: \nRNA-seq spike-in controls are a set of synthetic RNAs of known concentration\nwhich act as negative or positive controls. These controls have been used for\nnormalisation and quality control, but recent work has shown that the amount\nof technical variability in their use dramatically reduces their utility.",
            "title": "Sequencing options to consider"
        },
        {
            "location": "/tutorials/rna_seq_exp_design/rna_seq_experimental_design/#summary",
            "text": "A good experimental design is vital for a successful experiment. If you're\n   planning to work with a data analyst or bioinformatician, include them in\n   the design stage.  Aim to minimise variability by identifying possible sources of variance in\n   your samples.  Biological replicates are important. The number of biological replicates\n   you should have should never be below 3. Technical replicates are often\n   unnecessary.  Pilot studies are highly recommended for identifying how many replicates\n   and how many reads you should have for enough statistical power in your\n   experiment.  For basic RNA-seq experiments, poly-A enriched, single-ended, unstranded\n   sequencing at depths of 10M to 20M is probably what you want.",
            "title": "Summary"
        },
        {
            "location": "/tutorials/assembly/assembly/",
            "text": "Microbial de novo Assembly for Illumina Data\n\n\nIntroductory Tutorial\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\n\n\n\nTutorial Overview\n\n\nIn this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism.\n\n\nWhat\u2019s not covered\n\n\nThis tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads.\nIt does not cover more complicated aspects of assembly such as:\n\n\n\n\nIncorporation of other raw data types (454 reads, Sanger reads)\n\n\nGap filling techniques for \u201cfinishing\u201d an assembly\n\n\nMeasuring the accuracy of assemblies\n\n\n\n\nBackground\n\n\nRead the \nbackground to the workshop here\n\n\nWhere is the data in this tutorial from?\n\n\nThe data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the \nNCBI Short Read Archive (SRA)\n (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396.\n\n\nWe will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced.\n\n\nWe will use software called \nVelvet\n (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above.\n\n\nThe protocol:\n\n\nWe are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the \nVelvet Optimiser\n (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly.\n\n\nFollow this \nlink for an overview of the protocol\n\n\nThe protocol in a nutshell:\n\n\nInput:\n Raw reads from sequencer run on microbial DNA sample.\n\n\nOutput:\n File of assembled scaffolds/contigs and associated information.\n\n\nPreparation\n\n\nLogin to Galaxy\n\n\n\n\nOpen a browser and go to a Galaxy server. (what is \nGalaxy\n?)\n\n\nYou can use a galaxy server of your own \nor\n\n\nGalaxy Tute\n at genome.edu.au\n\n\n\n\n\n\nRegister as a new user if you don\u2019t already have an account on that particular server\n\n\n\n\n\n\n\n  NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.\n\n\n\n\nImport the DNA read data for the tutorial.\n\n\nYou can do this in a few ways. If you're using \ngalaxy-tut.genome.edu.au\n:\n\n\n\n\nGo to \nShared Data -> Published Histories\n and click on \"\nMicrobial_assembly_input_data\n\". Then click \n'Import History'\n at top right, wait for the history to be imported to your account, and then \n\u2018start using this history\u2019\n.\n\n\nThis will create a new Galaxy history in your account with all of the required data files.\n\n\nProceed to step 4.\n\n\n\n\nIf you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs.\n\n\n\n\nOn the Galaxy tools panel, click on \nGet data -> Upload File\n.\n\n\nClick on the \nPaste/Fetch Data\n button.\n\n\nPaste the URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz\n into the text box. Change the type to \nfastqsanger\n (Not \nfastqcsanger\n).\n\n\nClick on the \nPaste/Fetch Data\n button again.\n\n\nPaste the URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz\n into the text box and change it's type to \nfastqsanger\n as well.\n\n\nRepeat the process for the last URL: \nhttps://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna\n , but make it's type \nfasta\n\n\nClick on the \nStart\n button. Once all of the uploads are at 100%, click on the \nClose\n button.\n\n\nWhen the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the \n icon to the top right of the file name in the right hand Galaxy panel (the history panel)\n\n\n\n\nYou should now have the following files in your Galaxy history:\n\n\n\n\nERR048396_1.fastq\n - forward reads in fastq format\n\n\nERR048396_2.fastq\n - reverse reads in fastq format\n\n\nillumina_adapters.fa\n - Illumina adapter sequences in fasta format\n\n\n\n\nView the fastq files\n\n\nClick on the \n  icon to the top right of each fastq file to view the first part of the file\nIf you\u2019re not familiar with the FASTQ format, click here for an overview\n\n\nNOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019\n\n\n\n\nSection 1: Quality control\n\n\nThe basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.)\n\n\nMore detailed description of FastQC quality analysis can be found here.\n\n\nMore detailed description of Trimmomatic read quality filtering can be found here.\n\n\nRun FastQC on both input read files\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS QC and manipulation > FastQC: Comprehensive QC\n (down the bottom of this category) and run with these parameters:\n\n\n\"FASTQ reads\": \nERR048396_1.fastq\n\n\nUse default for other fields\n\n\n\n\n\n\nClick \nExecute\n\n\nNow repeat the above process on the second read file: \nERR048396_2.fastq\n\n\n\n\n\n\n Note: This may take a few minutes, depending on how busy Galaxy is.\n\n\n\n\nIt is important to do both read files as the quality can be very different between them.\n\n\nFigure 1: Screenshot of FastQC interface in Galaxy\n\n\n\n\nExamine the FastQC output\n\n\nYou should have two output objects from the first step:\n\n\n\n\nFastQC_ERR048396_1.fastqc.html\n\n\nFastQC_ERR048396_2.fastqc.html\n\n\n\n\nThese are a html outputs which show the results of all of the tests FastQC performed on the read files.\n\n\n\n\nClick on the \n icon of each of these objects in turn to see the FastQC output.\n\n\n\n\nThe main parts of the output to evaluate are:\n\n\n\n\nBasic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%.\n\n\nPer base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.\n\n\n\n\nFigure 2: Screenshot of FastQC output in Galaxy\n\n\n\n\nQuality trim the reads using Trimmomatic.\n\n\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS QC and manipulation > Trimmomatic\n and run with these parameters (only the non-default selections are listed here):\n\n\n\n\n\"Input FASTQ file (R1/first of pair)\": \nERR048396_1.fastq\n\n\n\"Input FASTQ file (R2/second of pair)\": \nERR048396_2.fastq\n\n\n\"Perform initial ILLUMINACLIP step?\": \nYes\n\n\n\"Adapter sequences to use\": \nTruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq)\n\n\n\"How accurate ... read alignment\": \n40\n\n\n\"How accurate ... against a read\": \n15\n\n\nWe will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations.\n\n\nClick \nInsert Trimmomatic Operation\n\n\nAdd \nCut bases ... (LEADING)\n\n\n\"Minimum quality required to keep a base\": \n15\n\n\n\n\n\n\nRepeat the \nInsert Trimmomatic Operation\n for:\n\n\nTrim trailing bases, minimum quality: \n15\n\n\nMinimum length read: \n35\n\n\n\n\n\n\n\n\n\n\n\n\nClick \nExecute\n\n      \n\n\n\n\n\n\nFigure 3: Screenshot of Trimmomatic inputs in Galaxy\n\n\n\n\nExamine the Trimmomatic output FastQ files.\n\n\nYou should have 4 new objects in your history from the output of Trimmomatic:\n\n\n\n\nTrimmomatic on data 2 and data 1 (R1 Paired)\n\n\nTrimmomatic on data 2 and data 1 (R1 Unpaired)\n\n\nTrimmomatic on data 2 and data 1 (R2 Paired)\n\n\nTrimmomatic on data 2 and data 1 (R2 Unpaired)\n\n\n\n\nClick on the \n on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.\n\n\n\n\nSection 2: Assemble reads into contigs with Velvet and the Velvet Optimiser\n\n\nThe aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser.\n\n\nWe will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process.\n\n\nClick here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser\n\n\nDe novo assembly of the reads into contigs\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nNGS: Assembly -> Velvet Optimiser\n and run with these parameters (only the non-default selections are listed here):\n\n\n\"Start k-mer value\": \n55\n\n\n\"End k-mer value\": \n69\n\n\nIn the input files section:\n\n\n\"Select first set of reads\": \nTrimmomatic on data 2 and data 1 (R1 paired)\n\n\n\"Select second set of reads\": \nTrimmomatic on data 2 and data 1 (R2 paired)\n\n\n\n\n\n\nClick the \nInsert Input Files\n button and add the following:\n\n\n\"Single or paired end reads\": \nSingle\n\n\n\"Select the reads\": \nTrimmomatic on data 2 and data 1 (R1 unpaired)\n\n\n\n\n\n\nRepeat the above process to add the other unpaired read set \nTrimmomatic on data 2 and data 1 (R2 unpaired)\n as well.\n\n\n\n\n\n\nClick \nExecute\n.\n\n\n\n\nFigure 4: Screenshot of Velvet Optimiser inputs in Galaxy\n\n\n\n\nExamine assembly output\n\n\nOnce step 1 is complete, you should now have 2 new objects in your history:\n\n \nVelvetOptimiser on data 9, data 7, and others: Contigs\n\n\n \nVelvetOptimiser on data 9, data 7, and others: Contig Stats\n\n\nClick on the \n icon of the various objects.\n\n\n\n\n\n\nContigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.)\n\n\n\n\n\n\nContig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)\n\n\n\n\n\n\nFigure 5: Screenshot of assembled contigs (a) and contig stats (b)\n\n\na\n\n\n\n\nb\n\n\n\n\nCalculate some statistics on the assembled contigs\n\n\n\n\nFrom the tools menu in the left hand panel of Galaxy, select \nFASTA Manipulation -> Fasta Statistics\n and run with these parameters:\n\n\n\"Fasta or multifasta file\": \nVelvet Optimiser ... Contigs\n\n\n\n\n\n\nClick \nExecute\n\n\nExamine the Fasta Stats output\n\n\n\n\nYou should now have one more object in your history: \nFasta Statistics on data 10: Fasta summary stats\n\n\nClick on the \n icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.\n\n\n\n\nSection 3: Extension.\n\n\nExamine the contig coverage depth and blast a high coverage contig against a protein database.\n\n\nExamine the contig coverage depth.\n\n\nLook at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the \n icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1).\n\n\n\n\nWe can easily filter out these short contigs from this information list by using the \nFilter and Sort -> Filter tool.\n\n\nSet the following:\n\n\n\"Filter\": \nVelvet Optimiser on data 8, data 7 and others: Contig stats\n\n\n\"With the following condition\": \nc2 > 100\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThe new data object in the history is called: \nFilter on data 11\n.\n\n\nClick on its \n icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32.  There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times!\n\n\nLets have a look at one of these contigs and see if we can find out what it is.\n\n\nExtract a single sequence from the contigs file.\n\n\nNote the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily.\n\n\nTo do this we will use the tool:\n\n\n\n\nFasta manipulation -> Fasta Extract Sequence\n\n\nSet the following:\n\n\n\"Fasta or multifasta file\": \nVelvet Optimiser ... : Contigs\n\n\n\"Sequence ID (or partial): \nNODE_1_...\n (for example)\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nThe new data object in the history is called: \nFasta Extract Sequence on data 10: Fasta\n.\n\n\nClick on its \n icon to view it. It is a single sequence in fasta format.\n\n\nBlast sequence to determine what it contains.\n\n\nWe want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this:\n\n\n\n\nBring up the sequence of the contig into the main window of the browser by clicking on the \n icon if it isn\u2019t already.\n\n\nSelect the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser.\n\n\nCopy the selected sequence to the clipboard.\n\n\nOpen a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi\n\n\nUnder the BASIC BLAST section, click \u201cblastx\u201d.\n\n\nPaste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s).\n\n\nChange the Genetic code to: Bacteria and Archaea (11)\n\n\nClick the button labelled: BLAST\n\n\n\n\nAfter a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.\n\n\nFigure 6: Screenshot of the output from the NCBI Blast website",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials/assembly/assembly/#microbial-de-novo-assembly-for-illumina-data",
            "text": "",
            "title": "Microbial de novo Assembly for Illumina Data"
        },
        {
            "location": "/tutorials/assembly/assembly/#introductory-tutorial",
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)",
            "title": "Introductory Tutorial"
        },
        {
            "location": "/tutorials/assembly/assembly/#tutorial-overview",
            "text": "In this tutorial we cover the concepts of Microbial de novo assembly using a very small synthetic dataset from a well studied organism.  What\u2019s not covered  This tutorial covers the basic aspects of microbial de novo assembly from Illumina paired end or single end reads.\nIt does not cover more complicated aspects of assembly such as:   Incorporation of other raw data types (454 reads, Sanger reads)  Gap filling techniques for \u201cfinishing\u201d an assembly  Measuring the accuracy of assemblies",
            "title": "Tutorial Overview"
        },
        {
            "location": "/tutorials/assembly/assembly/#background",
            "text": "Read the  background to the workshop here  Where is the data in this tutorial from?  The data for this tutorial is from a whole genome sequencing experiment of a multi-drug resistant strain of the bacterium Staphylococcus aureus. The DNA was sequenced using an Illumina GAII sequencing machine. The data we are going to use consists of about 4 million x 75 base-pair, paired end reads (two FASTQ read files, one for each end of a DNA fragment.) The data was downloaded from the  NCBI Short Read Archive (SRA)  (http://www.ncbi.nlm.nih.gov/sra/). The specific sample is a public dataset published in April 2012 with SRA accession number ERR048396.  We will also use a FASTA file containing the sequences of the Illumina adapters used in the sequencing process. It is desirable to remove these as they are artificial sequences and not part of the bacterium that was sequenced.  We will use software called  Velvet  (Zerbino et al 2008) for the main de novo assembly, as well as some other peripheral software for pre- and post-processing of the data. Details of these can be found in the background document linked above.  The protocol:  We are performing a de novo assembly of the read data into contigs and then into scaffolds (appropriately positioned contigs loosely linked together). We firstly need to check the quality of the input data as this will help us choose the most appropriate range of input parameters for the assembly and will guide us on an appropriate quality trimming/cleanup strategy. We will then use an iterative method to assemble the reads using the  Velvet Optimiser  (a program that performs lots of Velvet assemblies searching for an optimum outcome.) Once this is complete we will obtain summary statistics on the final results (contigs) of the assembly.  Follow this  link for an overview of the protocol  The protocol in a nutshell:  Input:  Raw reads from sequencer run on microbial DNA sample.  Output:  File of assembled scaffolds/contigs and associated information.",
            "title": "Background"
        },
        {
            "location": "/tutorials/assembly/assembly/#preparation",
            "text": "",
            "title": "Preparation"
        },
        {
            "location": "/tutorials/assembly/assembly/#login-to-galaxy",
            "text": "Open a browser and go to a Galaxy server. (what is  Galaxy ?)  You can use a galaxy server of your own  or  Galaxy Tute  at genome.edu.au    Register as a new user if you don\u2019t already have an account on that particular server    \n  NOTE: Firefox/Safari/Chrome all work well, Internet Explorer not so well.",
            "title": "Login to Galaxy"
        },
        {
            "location": "/tutorials/assembly/assembly/#import-the-dna-read-data-for-the-tutorial",
            "text": "You can do this in a few ways. If you're using  galaxy-tut.genome.edu.au :   Go to  Shared Data -> Published Histories  and click on \" Microbial_assembly_input_data \". Then click  'Import History'  at top right, wait for the history to be imported to your account, and then  \u2018start using this history\u2019 .  This will create a new Galaxy history in your account with all of the required data files.  Proceed to step 4.   If you are using a different Galaxy server, you can upload the data directly to Galaxy using the file URLs.   On the Galaxy tools panel, click on  Get data -> Upload File .  Click on the  Paste/Fetch Data  button.  Paste the URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_1.fastq.gz  into the text box. Change the type to  fastqsanger  (Not  fastqcsanger ).  Click on the  Paste/Fetch Data  button again.  Paste the URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/ERR048396_2.fastq.gz  into the text box and change it's type to  fastqsanger  as well.  Repeat the process for the last URL:  https://swift.rc.nectar.org.au:8888/v1/AUTH_377/public/Assembly/illumina_adapters.fna  , but make it's type  fasta  Click on the  Start  button. Once all of the uploads are at 100%, click on the  Close  button.  When the files have finished uploading, rename them to \u2018ERR048396_1.fastq\u2019, \u2018ERR048396_2.fastq\u2019 and \u2018illumina_adapters.fna\u2019 respectively by clicking on the   icon to the top right of the file name in the right hand Galaxy panel (the history panel)   You should now have the following files in your Galaxy history:   ERR048396_1.fastq  - forward reads in fastq format  ERR048396_2.fastq  - reverse reads in fastq format  illumina_adapters.fa  - Illumina adapter sequences in fasta format",
            "title": "Import the DNA read data for the tutorial."
        },
        {
            "location": "/tutorials/assembly/assembly/#view-the-fastq-files",
            "text": "Click on the    icon to the top right of each fastq file to view the first part of the file\nIf you\u2019re not familiar with the FASTQ format, click here for an overview  NOTE: If you log out of Galaxy and log back in at a later time your data and results from previous experiments will be available in the right panel of your screen called the \u2018History.\u2019",
            "title": "View the fastq files"
        },
        {
            "location": "/tutorials/assembly/assembly/#section-1-quality-control",
            "text": "The basic process here is to collect statistics about the quality of the reads in the sample FASTQ readsets. We will then evaluate their quality and choose an appropriate regime for quality filtering using Trimmomatic (a fastq read quality trimmer.)  More detailed description of FastQC quality analysis can be found here.  More detailed description of Trimmomatic read quality filtering can be found here.",
            "title": "Section 1: Quality control"
        },
        {
            "location": "/tutorials/assembly/assembly/#run-fastqc-on-both-input-read-files",
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS QC and manipulation > FastQC: Comprehensive QC  (down the bottom of this category) and run with these parameters:  \"FASTQ reads\":  ERR048396_1.fastq  Use default for other fields    Click  Execute  Now repeat the above process on the second read file:  ERR048396_2.fastq     Note: This may take a few minutes, depending on how busy Galaxy is.   It is important to do both read files as the quality can be very different between them.",
            "title": "Run FastQC on both input read files"
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-1-screenshot-of-fastqc-interface-in-galaxy",
            "text": "",
            "title": "Figure 1: Screenshot of FastQC interface in Galaxy"
        },
        {
            "location": "/tutorials/assembly/assembly/#examine-the-fastqc-output",
            "text": "You should have two output objects from the first step:   FastQC_ERR048396_1.fastqc.html  FastQC_ERR048396_2.fastqc.html   These are a html outputs which show the results of all of the tests FastQC performed on the read files.   Click on the   icon of each of these objects in turn to see the FastQC output.   The main parts of the output to evaluate are:   Basic statistics. This section tells us that the ASCII quality encoding format used was Sanger/Illumina 1.9 and the reads are length 75 and the percent GC content of the entire file is 35%.  Per base sequence quality. In the plot you should see that most of the early bases are up around the '32' mark and then increase to 38-40, which is very high quality; The spread of quality values for the last few bases increases and some of the outliers have quality scores of less than 30. This is a very good quality dataset. 20 is often used as a cutoff for reliable quality.",
            "title": "Examine the FastQC output"
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-2-screenshot-of-fastqc-output-in-galaxy",
            "text": "",
            "title": "Figure 2: Screenshot of FastQC output in Galaxy"
        },
        {
            "location": "/tutorials/assembly/assembly/#quality-trim-the-reads-using-trimmomatic",
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS QC and manipulation > Trimmomatic  and run with these parameters (only the non-default selections are listed here):   \"Input FASTQ file (R1/first of pair)\":  ERR048396_1.fastq  \"Input FASTQ file (R2/second of pair)\":  ERR048396_2.fastq  \"Perform initial ILLUMINACLIP step?\":  Yes  \"Adapter sequences to use\":  TruSeq3 (additional seqs) (paired end, for MiSeq and HiSeq)  \"How accurate ... read alignment\":  40  \"How accurate ... against a read\":  15  We will use the default settings for the SLIDING_WINDOW operation but we need to add a few more Trimmomatic operations.  Click  Insert Trimmomatic Operation  Add  Cut bases ... (LEADING)  \"Minimum quality required to keep a base\":  15    Repeat the  Insert Trimmomatic Operation  for:  Trim trailing bases, minimum quality:  15  Minimum length read:  35       Click  Execute",
            "title": "Quality trim the reads using Trimmomatic."
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-3-screenshot-of-trimmomatic-inputs-in-galaxy",
            "text": "",
            "title": "Figure 3: Screenshot of Trimmomatic inputs in Galaxy"
        },
        {
            "location": "/tutorials/assembly/assembly/#examine-the-trimmomatic-output-fastq-files",
            "text": "You should have 4 new objects in your history from the output of Trimmomatic:   Trimmomatic on data 2 and data 1 (R1 Paired)  Trimmomatic on data 2 and data 1 (R1 Unpaired)  Trimmomatic on data 2 and data 1 (R2 Paired)  Trimmomatic on data 2 and data 1 (R2 Unpaired)   Click on the   on one of the objects to look at its contents. You\u2019ll notice that not all of the reads are the same length now, as they have had the illumina adapters cut out of them and they\u2019ve been quality trimmed.",
            "title": "Examine the Trimmomatic output FastQ files."
        },
        {
            "location": "/tutorials/assembly/assembly/#section-2-assemble-reads-into-contigs-with-velvet-and-the-velvet-optimiser",
            "text": "The aim here is to assemble the trimmed reads into contigs/scaffolds using Velvet and the Velvet Optimiser.  We will use a single tool, Velvet Optimiser, which takes the trimmed reads from Trimmomatic and performs numerous Velvet assemblies to find the best one. We need to add the reads in two separate libraries. One for the still paired reads and the other for the singleton reads orphaned from their pairs by the trimming process.  Click here for a more detailed explanation of Velvet assemblies and the Velvet Optimiser",
            "title": "Section 2: Assemble reads into contigs with Velvet and the Velvet Optimiser"
        },
        {
            "location": "/tutorials/assembly/assembly/#de-novo-assembly-of-the-reads-into-contigs",
            "text": "From the tools menu in the left hand panel of Galaxy, select  NGS: Assembly -> Velvet Optimiser  and run with these parameters (only the non-default selections are listed here):  \"Start k-mer value\":  55  \"End k-mer value\":  69  In the input files section:  \"Select first set of reads\":  Trimmomatic on data 2 and data 1 (R1 paired)  \"Select second set of reads\":  Trimmomatic on data 2 and data 1 (R2 paired)    Click the  Insert Input Files  button and add the following:  \"Single or paired end reads\":  Single  \"Select the reads\":  Trimmomatic on data 2 and data 1 (R1 unpaired)    Repeat the above process to add the other unpaired read set  Trimmomatic on data 2 and data 1 (R2 unpaired)  as well.    Click  Execute .",
            "title": "De novo assembly of the reads into contigs"
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-4-screenshot-of-velvet-optimiser-inputs-in-galaxy",
            "text": "",
            "title": "Figure 4: Screenshot of Velvet Optimiser inputs in Galaxy"
        },
        {
            "location": "/tutorials/assembly/assembly/#examine-assembly-output",
            "text": "Once step 1 is complete, you should now have 2 new objects in your history:   VelvetOptimiser on data 9, data 7, and others: Contigs    VelvetOptimiser on data 9, data 7, and others: Contig Stats  Click on the   icon of the various objects.    Contigs: You\u2019ll see the first MB of the file. Note that the contigs are named NODE_XX_length_XXXX_cov_XXX.XXX. This information tells you how long (in k-mer length) each contig is and what it\u2019s average k-mer coverage is. (See detailed explanation of Velvet and Velvet Optimiser for explanation of k-mer coverage and k-mer length.)    Contig stats: This shows a table of the contigs and their k-mer coverages and which read library contributed to the coverage. It is interesting to note that some of them have much higher coverage than the average. These are most likely to be repeated contigs. (Things like ribosomal RNA and IS elements.)",
            "title": "Examine assembly output"
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-5-screenshot-of-assembled-contigs-a-and-contig-stats-b",
            "text": "",
            "title": "Figure 5: Screenshot of assembled contigs (a) and contig stats (b)"
        },
        {
            "location": "/tutorials/assembly/assembly/#a",
            "text": "",
            "title": "a"
        },
        {
            "location": "/tutorials/assembly/assembly/#b",
            "text": "",
            "title": "b"
        },
        {
            "location": "/tutorials/assembly/assembly/#calculate-some-statistics-on-the-assembled-contigs",
            "text": "From the tools menu in the left hand panel of Galaxy, select  FASTA Manipulation -> Fasta Statistics  and run with these parameters:  \"Fasta or multifasta file\":  Velvet Optimiser ... Contigs    Click  Execute  Examine the Fasta Stats output   You should now have one more object in your history:  Fasta Statistics on data 10: Fasta summary stats  Click on the   icon next to this object and have a look at the output. You\u2019ll see a statistical summary of the contigs including various length stats, the % GC content, the n50 as well as the number of contigs and the number of N bases contained in them.",
            "title": "Calculate some statistics on the assembled contigs"
        },
        {
            "location": "/tutorials/assembly/assembly/#section-3-extension",
            "text": "Examine the contig coverage depth and blast a high coverage contig against a protein database.",
            "title": "Section 3: Extension."
        },
        {
            "location": "/tutorials/assembly/assembly/#examine-the-contig-coverage-depth",
            "text": "Look at the Contig Stats data (Velvet Optimiser vlsci on data 8, data 9, and data 7: Contig stats) by clicking on the   icon. Note that column 2 contig length (lgth), shows a number of very short contigs (some are length 1).   We can easily filter out these short contigs from this information list by using the  Filter and Sort -> Filter tool.  Set the following:  \"Filter\":  Velvet Optimiser on data 8, data 7 and others: Contig stats  \"With the following condition\":  c2 > 100    Click  Execute   The new data object in the history is called:  Filter on data 11 .  Click on its   icon to view it. Look through the list taking note of the coverages. Note that the average of the coverages (column 6) seems to be somewhere between 16 and 32.  There are a lot of contigs with coverage 16. We could say that these contigs only appear once in the genome of the bacteria. Therefore, contigs with double this coverage would appear twice. Note that some of the coverages are >400! These contigs will appear in the genome more than 20 times!  Lets have a look at one of these contigs and see if we can find out what it is.",
            "title": "Examine the contig coverage depth."
        },
        {
            "location": "/tutorials/assembly/assembly/#extract-a-single-sequence-from-the-contigs-file",
            "text": "Note the contig number (column 1 in the Contig stats file) of a contig with a coverage of over 300. There should be a few of them. We need to extract the fasta sequence of this contig from the contigs multifasta so we can see it more easily.  To do this we will use the tool:   Fasta manipulation -> Fasta Extract Sequence  Set the following:  \"Fasta or multifasta file\":  Velvet Optimiser ... : Contigs  \"Sequence ID (or partial):  NODE_1_...  (for example)    Click  Execute   The new data object in the history is called:  Fasta Extract Sequence on data 10: Fasta .  Click on its   icon to view it. It is a single sequence in fasta format.",
            "title": "Extract a single sequence from the contigs file."
        },
        {
            "location": "/tutorials/assembly/assembly/#blast-sequence-to-determine-what-it-contains",
            "text": "We want to find out what this contig is or what kind of coding sequence (if any) it contains. So we will blast the sequence using the NCBI blast website. (External to Galaxy). To do this:   Bring up the sequence of the contig into the main window of the browser by clicking on the   icon if it isn\u2019t already.  Select the entire sequence by clicking and dragging with the mouse or by pressing ctrl-a in the browser.  Copy the selected sequence to the clipboard.  Open a new tab of your browser and point it to: http://blast.ncbi.nlm.nih.gov/Blast.cgi  Under the BASIC BLAST section, click \u201cblastx\u201d.  Paste the sequence into the large text box labelled: Enter Accession number(s), gi(s) or FASTA sequence(s).  Change the Genetic code to: Bacteria and Archaea (11)  Click the button labelled: BLAST   After a while the website will present a report of the blast run. Note that the sequence we blasted (if you chose NODE_1) is identical to part of a transposase gene (IS256) from a similar Staphylococcus aureus bacteria. These transposases occur frequently as repeats in bacterial genomes and so we shouldn\u2019t be surprised at its very high coverage.",
            "title": "Blast sequence to determine what it contains."
        },
        {
            "location": "/tutorials/assembly/assembly/#figure-6-screenshot-of-the-output-from-the-ncbi-blast-website",
            "text": "",
            "title": "Figure 6: Screenshot of the output from the NCBI Blast website"
        },
        {
            "location": "/tutorials/assembly/assembly-background/",
            "text": "De novo genome assembly using Velvet\n\n\nBackground\n\n\nIntroduction to de novo assembly\n\n\nDNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated.\n\n\nThe sequence assembly issue was neatly summed up by the following quote:\n\n\n\"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces.  The book may have many repeated paragraphs,  and some shreds may be modified to have typos.  Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\"\n \u2013 Wikipedia: Sequence assembly.\n\n\nAn addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known.\n\n\nThis tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino  et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system.\n\n\n\n\nThe Galaxy workflow platform\n\n\nWhat is Galaxy?\n\n\nGalaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.\n\n\nGalaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.\n\n\nGalaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account\n\n\nLearn more about Galaxy here\n\n\nFigure 1: The Galaxy interface\n\n\nTools on the left, data in the middle, analysis workflow on the right.\n\n\n\n\n\n\nDe novo assembly with Velvet and the Velvet Optimiser.\n\n\nVelvet\n\n\nVelvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.\n\n\nde Bruijn graphs\n\n\nA de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence.\n\n\nFigure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0.\n\n\n\n\nFrom: https://cameroncounts.wordpress.com/2015/02/28/1247/\n\n\nVelvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters.\n\n\nVelvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.\n\n\nThe Velvet algorithm\n\n\nStep 1: Hashing the reads.\n\n\n\n\nVelvet breaks up each read into k-mers of length k.\n\n\nA k-mer is a k length subsequence of the read.\n\n\nA 36 base pair long read would have 6 different 31-mers.\n\n\nThe k-mers and their reverse complements are added to a hash table to categorize them.\n\n\nEach k-mer is stored once but the number of times it appears is also recorded.\n\n\nThis step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.\n\n\n\n\nStep 2: Constructing the de Bruijn graph.\n\n\n\n\nVelvet adds the k-mers one-by-one to the graph.\n\n\nAdjacent k-mers overlap by k-1 nucleotides.\n\n\nA k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node.\n\n\nEach node stores the average number of times its k-mers appear in the hash table.\n\n\nFigure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5.\n\n\nDifferent sequences can be read off the graph by following a different path through it. (Figure 3)\n\n\n\n\nFigure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph.\n \n(Base figure Zerbino et al 2008.)\n\n\n\n\nStep 3: Simplification of the graph.\n\n\n\n\nChain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes.\n\n\nTip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path.\n\n\nBubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences.\n\n\nVelvet compares the paths using dynamic programming.\n\n\nIf they are highly similar, the paths are merged.\n\n\nError removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes.\n\n\nPaired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs.\n\n\n\n\nFigure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph.\n  \n(Base figure Zerbino et al 2008.)\n\n\n\n\nStep 4: Read off the contigs.\n\n\n\n\nFollow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs.\n\n\nWhere there is an ambiguous divergence/convergence, stop the current contig and start a new one.\n\n\n\n\nK-mer size and coverage cutoff values\n\n\nThe size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down.\n\n\nThe coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data.\n\n\nEach dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.\n\n\nVelvet Optimiser\n\n\nThe Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies.  It uses different optimisation functions for k and c and these can be user controlled.\n\n\nIt requires the user to input a range of k values to search (to cut down on running time).\n\n\nReferences\n\n\nhttp://en.wikipedia.org/wiki/Sequence_assembly\n\n\nZerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829\n\n\nZerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407.\n\n\nGladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.",
            "title": "Method Background"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#de-novo-genome-assembly-using-velvet",
            "text": "",
            "title": "De novo genome assembly using Velvet"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#background",
            "text": "",
            "title": "Background"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#introduction-to-de-novo-assembly",
            "text": "DNA sequence assembly from short fragments (< 200 bp) is often the first step of any bioinformatic analysis. The goal of assembly is to take the millions of short reads produced by sequencing instruments and re-construct the DNA from which the reads originated.  The sequence assembly issue was neatly summed up by the following quote:  \"The problem of sequence assembly can be compared to taking many copies of a book, passing them all through a shredder, and piecing a copy of the book back together from only shredded pieces.  The book may have many repeated paragraphs,  and some shreds may be modified to have typos.  Excerpts from another book may be added in, and some shreds may be completely unrecognizable.\"  \u2013 Wikipedia: Sequence assembly.  An addition to the above for paired end sequencing is that now some of the shreds are quite long but only about 10% of the words from both ends of the shred are known.  This tutorial describes de novo assembly of Illumina short reads using the Velvet assembler (Zerbino  et al. 2008, 2009) and the Velvet Optimiser (Gladman & Seemann, 2009) from within the Galaxy workflow management system.",
            "title": "Introduction to de novo assembly"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#the-galaxy-workflow-platform",
            "text": "",
            "title": "The Galaxy workflow platform"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#what-is-galaxy",
            "text": "Galaxy is an online bioinformatics workflow management system. Essentially, you upload your files, create various analysis pipelines and run them, then visualise your results.  Galaxy is really an interface to the various tools that do the data processing; each of these tools could be run from the command line, outside of Galaxy. Galaxy makes it easier to link up the tools together and visualise the entire analysis pipeline.  Galaxy uses the concept of 'histories'. Histories are sets of data and workflows that act on that data. The data for this workshop is available in a shared history, which you can import into your own Galaxy account  Learn more about Galaxy here  Figure 1: The Galaxy interface  Tools on the left, data in the middle, analysis workflow on the right.",
            "title": "What is Galaxy?"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#de-novo-assembly-with-velvet-and-the-velvet-optimiser",
            "text": "",
            "title": "De novo assembly with Velvet and the Velvet Optimiser."
        },
        {
            "location": "/tutorials/assembly/assembly-background/#velvet",
            "text": "Velvet is software to perform dna assembly from short reads by manipulating de Bruijn graphs. It is capable of forming long contigs (n50 of in excess of 150kb) from paired end short reads. It has several input parameters for controlling the structure of the de Bruijn graph and these must be set optimally to get the best assembly possible. Velvet can read Fasta, FastQ, sam or bam files. However, it ignores any quality scores and simply relies on sequencing depth to resolve errors. The Velvet Optimiser software performs many Velvet assemblies with various parameter sets and searches for the optimal assembly automatically.",
            "title": "Velvet"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#de-bruijn-graphs",
            "text": "A de Bruijn graph is a directed graph which represents overlaps between sequences of symbols. The size of the sequence contained in the nodes of the graph is called the word-length or k-mer size. In Figure 2, the word length is 3. The two symbols are 1 and 0. Each node in the graph has the last two symbols of the previous node and 1 new symbol. Sequences of symbols can be produced by traversing the graph and adding the \u201cnew\u201d symbol to the growing sequence.  Figure 2: A de Bruijn graph of word length 3 for the symbols 1 and 0.   From: https://cameroncounts.wordpress.com/2015/02/28/1247/  Velvet constructs a de Bruijn graph of the reads. It has 4 symbols (A, C, G and T - N\u2019s are converted to A\u2019s) The word length (or k-mer size) is one of Velvet\u2019s prime parameters.  Velvet is not the only assembly software that works in this manner. Euler, Edena and SOAP de novo are examples of others.",
            "title": "de Bruijn graphs"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#the-velvet-algorithm",
            "text": "",
            "title": "The Velvet algorithm"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#step-1-hashing-the-reads",
            "text": "Velvet breaks up each read into k-mers of length k.  A k-mer is a k length subsequence of the read.  A 36 base pair long read would have 6 different 31-mers.  The k-mers and their reverse complements are added to a hash table to categorize them.  Each k-mer is stored once but the number of times it appears is also recorded.  This step is performed by \u201cvelveth\u201d - one of the programs in the Velvet suite.",
            "title": "Step 1: Hashing the reads."
        },
        {
            "location": "/tutorials/assembly/assembly-background/#step-2-constructing-the-de-bruijn-graph",
            "text": "Velvet adds the k-mers one-by-one to the graph.  Adjacent k-mers overlap by k-1 nucleotides.  A k-mer which has no k-1 overlaps with any k-mer already on the graph starts a new node.  Each node stores the average number of times its k-mers appear in the hash table.  Figure 3 shows a section of a de Bruijn graph constructed by Velvet for k=5.  Different sequences can be read off the graph by following a different path through it. (Figure 3)   Figure 3: Section of a simple de Bruijn graph of reads with k-mer size 5. Coloured sequences are constructed by following the appropriately coloured line through the graph.   (Base figure Zerbino et al 2008.)",
            "title": "Step 2: Constructing the de Bruijn graph."
        },
        {
            "location": "/tutorials/assembly/assembly-background/#step-3-simplification-of-the-graph",
            "text": "Chain merging: When there are two connected nodes in the graph without a divergence, merge the two nodes.  Tip clipping: Tips are short (typically) chains of nodes that are disconnected on one end. They will be clipped if their length is < 2 x k or their average k-mer depth is much less than the continuing path.  Bubble removal: Bubbles are redundant paths that start and end at the same nodes (Figure 4.) They are created by sequencing errors, biological variants or slightly varying repeat sequences.  Velvet compares the paths using dynamic programming.  If they are highly similar, the paths are merged.  Error removal: Erroneous connections are removed by using a \u201ccoverage cutoff\u201d. Genuine short nodes which cannot be simplified should have a high coverage. An attempt is made to resolve repeats using the \u201cexpected coverage\u201d of the graph nodes.  Paired end read information: Velvet uses algorithms called \u201cPebble\u201d and \u201cRock Band\u201d (Zerbino et al 2009) to order the nodes with respect to one another in order to scaffold them into longer contigs.   Figure 4: Representation of \u201cbubbles\u201d in a Velvet de Bruijn graph.    (Base figure Zerbino et al 2008.)",
            "title": "Step 3: Simplification of the graph."
        },
        {
            "location": "/tutorials/assembly/assembly-background/#step-4-read-off-the-contigs",
            "text": "Follow the chains of nodes through the graph and \u201cread off\u201d the bases to create the contigs.  Where there is an ambiguous divergence/convergence, stop the current contig and start a new one.",
            "title": "Step 4: Read off the contigs."
        },
        {
            "location": "/tutorials/assembly/assembly-background/#k-mer-size-and-coverage-cutoff-values",
            "text": "The size of the k-mers that construct the graph is very important and has a large effect on the outcome of the assembly. Generally, small k-mers create a graph with increased connectivity, more ambiguity (more divergences) and less clear \u201cpaths\u201d through the graph. Large k-mers produce graphs with less connectivity but higher specificity. The paths through the graph are clearer but they are less connected and prone to breaking down.  The coverage cutoff c used during the error correction step of Velvet also has a significant effect on the output of the assembly process. If c is too low, the assembly will contain nodes of the graph that are the product of sequencing errors and misconnections. If c is too high, it can create mis-assemblies in the contigs and destroys lots of useful data.  Each dataset has its own optimum values for the k-mer size and the coverage cutoff used in the error removal step. Choosing them appropriately is one of the challenges faced by new users of the Velvet software.",
            "title": "K-mer size and coverage cutoff values"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#velvet-optimiser",
            "text": "The Velvet Optimiser chooses the optimal values for k and c automatically by performing many runs of Velvet (partially in parallel) and interrogating the subsequent assemblies.  It uses different optimisation functions for k and c and these can be user controlled.  It requires the user to input a range of k values to search (to cut down on running time).",
            "title": "Velvet Optimiser"
        },
        {
            "location": "/tutorials/assembly/assembly-background/#references",
            "text": "http://en.wikipedia.org/wiki/Sequence_assembly  Zerbino DR, Birney E, Velvet: algorithms for de novo short read assembly using de Bruijn graphs, Genome Research, 2008, 18:821-829  Zerbino DR, McEwen GK, Margulies EH, Birney E, Pebble and rock band: heuristic resolution of repeats and scaffolding in the velvet short-read de novo assembler. PLoS One. 2009; 4(12):e8407.  Gladman SL, Seemann T, Velvet Optimiser, http://www.vicbioinformatics.com/software.shtml 2009.",
            "title": "References"
        },
        {
            "location": "/tutorials/assembly/spades/",
            "text": "Assembly using Spades\n\n\nBackground\n\n\nSpades is one of a number of \nde novo\n assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this \nlink\n.\n\n\nLearning objectives\n\n\nAt the end of this tutorial you should be able to:\n\n\n\n\nassemble the reads using Spades, and\n\n\nexamine the output assembly.\n\n\n\n\nImport and view data\n\n\nGalaxy\n\n\nIf you are using Galaxy-Mel or Galaxy-Qld, import the files:\n\n\n\n\nIn your browser, go to \nGalaxy-Mel\n or \nGalaxy-Qld\n \n\n\nIn the top Galaxy panel, go to \nUser\n and log in (or register, and then log in)\n\n\nIn the top Galaxy panel, go to \nShared Data\n and click on the drop down arrow\n\n\nClick on \nHistories\n\n\nClick on \nGenomics-workshop\n and then (over in the top right) \nImport history\n\n\nThe files will now be listed in the right hand panel (your current history).\n\n\n\n\n(Alternatively, see \nhere\n for information about how to start with Galaxy, and \nhere\n for the link to import the Galaxy history for this tutorial, if you don't already have them in your history.)\n\n\nThe data\n\n\nThe read set for today is from an imaginary \nStaphylococcus aureus\n bacterium with a miniature genome.\n\n\n\n\n\n\nThe whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.\n\n\n\n\n\n\nThe files we need for assembly are the \nmutant_R1.fastq\n and \nmutant_R2.fastq\n.\n\n\n\n\n\n\n(We don't need the reference genome sequences for this tutorial).\n\n\n\n\n\n\nThe reads are paired-end.\n\n\n\n\n\n\nEach read is 150 bases long. \n\n\n\n\n\n\nThe number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).\n\n\n\n\n\n\n\n\n\n\n\nClick on the View Data button (the \n) next to one of the FASTQ sequence files.\n\n\n\n\n\n\n\nAssemble reads with Spades\n\n\n\n\n\n\nWe will perform a \nde novo\n assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)\n\n\n\n\n\n\nGo to \nTools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades\n\n\n\n\n\n\nSet the following parameters (leave other settings as they are):\n\n\n\n\nRun only Assembly\n: \nYes\n [the \nYes\n button should be darker grey]\n\n\nKmers to use separated by commas:\n \n33,55,91\n  [note: no spaces]  \n\n\nCoverage cutoff:\n \nauto\n  \n\n\nFiles \u2192 Forward reads:\n \nmutant_R1.fastq\n  \n\n\nFiles \u2192 Reverse reads:\n \nmutant_R2.fastq\n  \n\n\n\n\n\n\n\n\nYour tool interface should look like this:\n\n\n\n\n\n\n\n\n\n\nClick \nExecute\n\n\n\n\nExamine the output\n\n\n\n\nGalaxy is now running Spades on the reads for you.\n\n\n\n\nWhen it is finished, you will have five (or more) new files in your history, including:\n\n\n\n\ntwo FASTA files of the resulting contigs and scaffolds\n\n\ntwo files for statistics about these\n\n\nthe Spades logfile\n\n\n\n\n\n\n\n\n\n\n\n\nClick on the View Data button \n on each of the files.\n\n\nNote that the short reads have been assembled into much longer contigs.\n\n\n(However, in this case, the contigs have not been assembled into larger scaffolds.)\n\n\nThe stats files will give you the length of each of the contigs, and the file should look something like this:",
            "title": "Microbial assembly: Spades"
        },
        {
            "location": "/tutorials/assembly/spades/#assembly-using-spades",
            "text": "",
            "title": "Assembly using Spades"
        },
        {
            "location": "/tutorials/assembly/spades/#background",
            "text": "Spades is one of a number of  de novo  assemblers that use short read sets as input (e.g. Illumina Reads), and the assembly method is based on de Bruijn graphs. For information about Spades see this  link .",
            "title": "Background"
        },
        {
            "location": "/tutorials/assembly/spades/#learning-objectives",
            "text": "At the end of this tutorial you should be able to:   assemble the reads using Spades, and  examine the output assembly.",
            "title": "Learning objectives"
        },
        {
            "location": "/tutorials/assembly/spades/#import-and-view-data",
            "text": "",
            "title": "Import and view data"
        },
        {
            "location": "/tutorials/assembly/spades/#galaxy",
            "text": "If you are using Galaxy-Mel or Galaxy-Qld, import the files:   In your browser, go to  Galaxy-Mel  or  Galaxy-Qld    In the top Galaxy panel, go to  User  and log in (or register, and then log in)  In the top Galaxy panel, go to  Shared Data  and click on the drop down arrow  Click on  Histories  Click on  Genomics-workshop  and then (over in the top right)  Import history  The files will now be listed in the right hand panel (your current history).   (Alternatively, see  here  for information about how to start with Galaxy, and  here  for the link to import the Galaxy history for this tutorial, if you don't already have them in your history.)",
            "title": "Galaxy"
        },
        {
            "location": "/tutorials/assembly/spades/#the-data",
            "text": "The read set for today is from an imaginary  Staphylococcus aureus  bacterium with a miniature genome.    The whole genome shotgun method used to sequence our mutant strain read set was produced on an Illumina DNA sequencing instrument.    The files we need for assembly are the  mutant_R1.fastq  and  mutant_R2.fastq .    (We don't need the reference genome sequences for this tutorial).    The reads are paired-end.    Each read is 150 bases long.     The number of bases sequenced is equivalent to 19x the genome sequence of the wildtype strain. (Read coverage 19x - rather low!).      Click on the View Data button (the  ) next to one of the FASTQ sequence files.",
            "title": "The data"
        },
        {
            "location": "/tutorials/assembly/spades/#assemble-reads-with-spades",
            "text": "We will perform a  de novo  assembly of the mutant FASTQ reads into long contiguous sequences (in FASTA format.)    Go to  Tools \u2192 NGS Analysis \u2192 NGS: Assembly \u2192 spades    Set the following parameters (leave other settings as they are):   Run only Assembly :  Yes  [the  Yes  button should be darker grey]  Kmers to use separated by commas:   33,55,91   [note: no spaces]    Coverage cutoff:   auto     Files \u2192 Forward reads:   mutant_R1.fastq     Files \u2192 Reverse reads:   mutant_R2.fastq        Your tool interface should look like this:      Click  Execute",
            "title": "Assemble reads with Spades"
        },
        {
            "location": "/tutorials/assembly/spades/#examine-the-output",
            "text": "Galaxy is now running Spades on the reads for you.   When it is finished, you will have five (or more) new files in your history, including:   two FASTA files of the resulting contigs and scaffolds  two files for statistics about these  the Spades logfile       Click on the View Data button   on each of the files.  Note that the short reads have been assembled into much longer contigs.  (However, in this case, the contigs have not been assembled into larger scaffolds.)  The stats files will give you the length of each of the contigs, and the file should look something like this:",
            "title": "Examine the output"
        },
        {
            "location": "/tutorials/pacbio/",
            "text": "Long read assembly workshop\n\n\nThis is a tutorial for a workshop on long-read (PacBio) genome assembly. \n\n\nIt demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data. \n\n\nOverview\n\n\nSimplified version of workflow:\n\n\n\n\n1. Get started\n\n\nYour workshop trainers will provide you with the address of a virtual machine. \n\n\nMac users\n\n\nOpen the Terminal. \n\n\n\n\nType in \n\n\n\n\nssh researcher@[your virtual machine address]\n\n\n\n\n\n\nType in the password provided. \n\n\n\n\nWindows users\n\n\nIf you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty. \n\n\n\n\nDownload putty \nhere\n.\n\n\nOpen. A configuration window will appear. \n\n\nUnder \"Host Name (or IP address)\" enter in the address of your virtual machine. \n\n\nUnder \"Port\" type in 22\n\n\nUnder \"Connection Type\" select \"SSH\"\n\n\nClick \"Open\"\n\n\nUnder \"Login as:\" enter \"researcher\"\n\n\nType in the password provided. \n\n\n\n\nCreate a new working directory on your remote computer.\n\n\nBecause we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop.\n\n\nIn your terminal:\n\n\n\n\nCreate a new directory called \"Workshop\"\n\n\n\n\nmkdir Workshop\n\n\n\n\n\n\nChange to that directory\n\n\n\n\ncd Workshop\n\n\n\n\nNOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again.\n\n\nFind your current directory by typing:\n\n\npwd\n\n\n\n\n2. Get data\n\n\nThe sample used in this tutorial is from a bacteria called \nStaphylococcus aureus\n. We have used a small section of its real genome so that the programs can run in the workshop time. \n\n\nThe files we need are:\n\n\n\n\npacbio.fq\n: the PacBio reads\n\n\nR1.fq\n: the Illumina forward reads\n\n\nR2.fq\n: the Illumina reverse reads\n\n\n\n\nIn a new tab, go to \nhttps://doi.org/10.5281/zenodo.1009308\n. \n\n\n\n\nNext to the first file, right-click (or control-click) the \"Download\" button, and select \"Copy link address\".\n\n\nBack in your terminal, enter \n\n\n\n\nwget [paste file link here]\n\n\n\n\n\n\nThe file should download. \n\n\nNote:\n paste the link to the file, not to the webpage.\n\n\nRepeat this for the other two files. \n\n\n\n\n3. Assemble\n\n\nWe will use the assembly software called \nCanu\n, version 1.6.\n\n\nRun Canu with these commands:\n\n\ncanu -p canu -d canu_outdir genomeSize=0.03m -pacbio-raw pacbio.fq\n\n\n\n\n\n\nthe first \ncanu\n tells the program to run\n\n\n-p canu\n names prefix for output files (\"canu\")\n\n\n-d canu_outdir\n names output directory (\"canu_outdir\")\n\n\ngenomeSize\n only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs). \n\n\nCanu will correct, trim and assemble the reads.\n\n\nVarious output will be displayed on the screen.\n\n\nNote\n: Canu could say \"Finished\" but may still be running. In this case, type \nsqueue\n to see if jobs are still running. \n\n\n\n\nIf you run \nsqueue\n you will see something like this:\n\n\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                 6      main canu_can research PD       0:00      1 (Dependency)\n               5_1      main cormhap_ research  R       0:29      1 master\n\n\n\n\nYou will know if \nCanu\n has completely finished when \nsqueue\n shows no jobs listed under the header row. \n\n\n4. Check assembly output\n\n\nMove into the canu output folder: \n\n\ncd canu_outdir\n\n\n\n\nView the list of files: \n\n\nls\n\n\n\n\n\n\nThe \ncanu.contigs.fasta\n are the assembled sequences.\n\n\nThe \ncanu.unassembled.fasta\n are the reads that could not be assembled.\n\n\nThe \ncanu.correctedReads.fasta.gz\n are the corrected Pacbio reads that were used in the assembly.\n\n\nThe \ncanu.contigs.gfa\n is the graph of the assembly.\n\n\nThe \ncanu.report\n file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly.\n\n\n\n\nDisplay summary information about the contigs: (\ninfoseq\n is a tool from \nEMBOSS\n)\n\n\ninfoseq canu.contigs.fasta\n\n\n\n\n\n\nThis will show the contigs found by Canu. e.g.,  tig00000001  39136\n\n\n\"tig00000001\" is the name given to the contig\n\n\n\"39136\" is the number of base pairs in that contig.\n\n\n\n\nThis matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output.\n\n\nWe should also look at the \ncanu.report\n. To do this:\n\n\nless canu.report\n\n\n\n\n\n\n\"less\" is a command to display the file on the screen.\n\n\nUse the up and down arrows to scroll up and down. \n\n\nYou will see lots of histograms of read lengths before and after processing, final contig construction, etc. \n\n\nFor a description of the outputs that Canu produces, see: \nhttp://canu.readthedocs.io/en/latest/tutorial.html#outputs\n\n\nType \nq\n to exit viewing the report. \n\n\n\n\nQuestions\n\n\nHow do long- and short-read assembly methods differ?\n\n\n\n\nAnswer (click to reveal)\n\nShort reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods.\n\n\n\nWhere can we find out the what the approximate genome size should be for the species being assembled?\n\n\n\n\nAnswer (click to reveal)\n\nGo to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column.\n\n\n\nIn the assembly output, what are the unassembled reads? \n\n\n\n\nAnswer (click to reveal)\n\nReads and low-coverage contigs that were not used in the assembly.\n\n\n\nWhat are the corrected reads? How did canu correct the reads?\n\n\n\n\nAnswer (click to reveal)\n\nCanu builds overlaps between reads. The consensus is used to correct the reads. \n\n\n\nWhere could you view the output .gfa and what would it show?\n\n\n\n\nAnswer (click to reveal)\n\nA useful program is \nBandage\n. If the assembly has multiple contigs, the assembly graph shows how these are connected. \n\n\n\n5. Trim and circularise\n\n\nBacteria have circular chromosomes. \n\n\n\n\nBecause of sequencing errors, there may be some \"overhang\" in the assembled linear sequence. \n\n\nOur assembly may have some overhang because it is 9000 bases longer than expected. \n\n\n\n\n\n\nAdapted from Figure 1. Hunt et al. Genome Biology 2015\n\n\nA tool called \nCirclator\n identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.\n\n\nMove back into your main analysis folder: \n\n\ncd ..\n\n\n\n\nRun Circlator\n\n\ncirclator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir\n\n\n\n\n\n\n\n\n(Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.)\n\n\n\n\n\n\n--threads\n is the number of cores\n\n\n\n\n--verbose\n prints progress information to the screen\n\n\ncanu_outdir/canu.contigs.fasta\n is the file path to the input Canu assembly\n\n\ncanu_outdir/canu.correctedReads.fasta.gz\n is the file path to the corrected Pacbio reads - note, fastA not fastQ\n\n\ncirclator_outdir\n is the name of the output directory.\n\n\n\n\nSome output will print to screen. When finished, it should say \"Circularized x of x contig(s)\".\n\n\nCheck the output\n\n\nMove into the Circlator output directory: \n\n\ncd circlator_outdir\n\n\n\n\nList the files: \n\n\nls\n\n\n\n\nCirclator has named the output files with numbers as prefixes. \n\n\nWere the contigs circularised?\n\n\nless 04.merge.circularise.log\n\n\n\n\n\n\n\"less\" is a command to display the file on the screen.\n\n\n04.merge.circularise.log\n is the name of the file. \n\n\nYes, the contig was circularised (last column).\n\n\nType \nq\n to exit.\n\n\n\n\nWhat are the trimmed contig sizes? \n\n\ninfoseq 06.fixstart.fasta\n\n\n\n\n\n\nThe contig \"tig00000001\" has a length of 30019.\n\n\nThis is about 9000 bases shorter than before circularisation. This was the \"overhang\" and has now been trimmed. \n\n\n\n\nCopy the circularised contigs file to the main analysis directory with a new name:\n\n\ncp 06.fixstart.fasta ../contig1.fasta\n\n\n\n\nMove back into the main folder: \n\n\ncd ..\n\n\n\n\nQuestions\n\n\nWere all the contigs circularised? \n\n\n\n\nAnswer (click to reveal)\n\nIn this example, yes, the contig was circularised.\n\n\n\nCirclator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs?\n\n\n\n\nAnswer (click to reveal)\n\nCirclator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.\n\n\n\n6. Find smaller plasmids\n\n\nPacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.\n\n\nThis section involves several steps:\n\n\n\n\nUse the Canu+Circlator output of a trimmed assembly contig.\n\n\nMap all the Illumina reads against this Pacbio-assembled contig.\n\n\nExtract any reads that \ndidn't\n map and assemble them together: this could be a plasmid, or part of a plasmid.\n\n\nLook for overhang: if found, trim.\n\n\n\n\nAlign Illumina reads to the PacBio contig\n\n\nIndex the contigs file:\n\n\nbwa index contig1.fasta\n\n\n\n\nAlign Illumina reads using using bwa mem:\n\n\nbwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam\n\n\n\n\n\n\nbwa mem\n is the alignment tool\n\n\n-t 4\n is the number of cores\n\n\ncontig1.fasta\n is the input assembly file\n\n\nR1.fq R2.fq\n are the Illumina reads\n\n\n| samtools sort\n pipes the output to samtools to sort\n\n\n> aln.bam\n sends the alignment to the file \naln.bam\n\n\n\n\nExtract unmapped Illumina reads\n\n\nIndex the alignment file:\n\n\nsamtools index aln.bam\n\n\n\n\nExtract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \"unmapped\" files:\n\n\nsamtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam\n\n\n\n\n\n\nfastq\n is a command that coverts a \n.bam\n file into fastq format\n\n\n-f 4\n : only output unmapped reads\n\n\n-1\n : put R1 reads into a file called \nunmapped.R1.fastq\n\n\n-2\n : put R2 reads into a file called \nunmapped.R2.fastq\n\n\n-s\n : put singleton reads into a file called \nunmapped.RS.fastq\n\n\naln.bam\n : input alignment file\n\n\n\n\nWe now have three files of the unampped reads: \n unmapped.R1.fastq\n, \n unmapped.R2.fastq\n, \n unmapped.RS.fastq\n.\n\n\nAssemble the unmapped reads\n\n\nAssemble with Spades:\n\n\nspades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly\n\n\n\n\n\n\n\n\n(Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.)\n\n\n\n\n\n\n-1\n is input file forward\n\n\n\n\n-2\n is input file reverse\n\n\n-s\n is unpaired\n\n\n--careful\n minimizes mismatches and short indels\n\n\n--cov-cutoff auto\n computes the coverage threshold (rather than the default setting, \"off\")\n\n\n-o\n is the output directory\n\n\n\n\nMove into the output directory: \n\n\ncd spades_assembly\n\n\n\n\nLook at the contigs: \n\n\ninfoseq contigs.fasta\n\n\n\n\n\n\n1 contig has been assembled with a length of 2359 bases. \n\n\n\n\nCopy it to a new file: \n\n\ncp contigs.fasta contig2.fasta\n\n\n\n\nTrim the plasmid\n\n\nTo trim any overhang on this plasmid, we will blast the start of contig2 against itself.\n\n\nTake the start of the contig: \n\n\nhead -n 10 contig2.fasta > contig2.fa.head\n\n\n\n\n\n\nhead -n 10\n takes the first ten lines of \ncontig2.fasta\n\n\n\n\n>\n sends that output to a new file called \ncontig2.fa.head\n\n\n\n\n\n\nWe want to see if the start of the contig matches the end (overhang).\n\n\n\n\n\n\nFormat the assembly file for blast: \n\n\nmakeblastdb -in contig2.fasta -dbtype nucl\n\n\n\n\n\n\nmakeblastdb\n makes a database for the tool Blast\n\n\nThis will generate three new files in the directory with suffixes .nhr, .nin and .nsq\n\n\n-in\n sets the input file as \n contig2.fasta\n\n\n-dbtype nucl\n sets the type to nucleotide (rather than protein)\n\n\n\n\nBlast the start of the assembly (.head file) against all of the assembly: \n\n\nblastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls\n\n\n\n\n\n\nblastn\n is the tool Blast, set as blast\nn\n to compare sequences of nucleotides to each other\n\n\n-query\n sets the input sequence as \ncontig2.fa.head\n\n\n-db\n sets the database as that of the original sequence \ncontig2.fasta\n. We don't have to specify the other files that were created when we formatted this file, but they need to present in our current directory. \n\n\n-evalue\n is the number of hits expected by chance, here set as 1e-3\n\n\n-dust no\n turns off the masking of low-complexity regions\n\n\n-out\n sets the output file as \ncontig2.bls\n\n\n\n\nLook at the hits (the matches): \n\n\nless contig2.bls\n\n\n\n\n\n\nThe first hit is at the start, as expected. We can see that \"Query 1\" (the start of the contig) is aligned to \"Sbject 1\" (the whole contig), for the first 540 bases.\n\n\nScroll down with the down arrow. \n\n\nThe second hit shows \"Query 1\" (the start of the contig) also matches to \"Sbject 1\" (the whole contig) at position 2253, all the way to the end, position 2359. \n\n\n\n\n\n\n\n\nThis is the overhang.\n\n\nTherefore, in the next step, we need to trim the contig to position 2252.\n\n\nType \nq\n to exit. \n\n\n\n\nFirst, change the name of the contig within the file:\n\n\nnano contig2.fasta\n\n\n\n\n\n\nnano\n opens up a text editor. \n\n\nUse the arrow keys to navigate. (The mouse won't work.)\n\n\nAt the first line, delete the text, which will be something like \">NODE_1_length_2359_cov_3.320333\"\n\n\nType in \">contig2\" \n\n\nDon't forget the \n>\n symbol\n\n\nPress Control-X\n\n\n\"Save modified buffer ?\" - type \nY\n\n\nPress the Enter key\n\n\n\n\nIndex the file (this will allow samtools to edit the file as it will have an index): \n\n\nsamtools faidx contig2.fasta\n\n\n\n\n\n\nfaidx\n means index the fasta file\n\n\n\n\nTrim the contig:\n\n\nsamtools faidx contig2.fasta contig2:1-2252 > plasmid.fasta\n\n\n\n\n\n\nthis extracts contig2 from position 1-2252\n\n\n\n\n> plasmid.fasta\n sends the extracted section to a new file\n\n\n\n\n\n\nWe now have a trimmed plasmid.\n\n\n\n\n\n\nCopy the plasmid file into the main folder: \n\n\ncp plasmid.fasta ../\n\n\n\n\nMove file back into main folder: \n\n\ncd ..\n\n\n\n\nCollect contigs\n\n\nCollect the chromosome and the plasmid in one fasta file (they will be 2 records in the file): \n\n\ncat contig1.fasta plasmid.fasta > genome.fasta\n\n\n\n\nSee the contigs and sizes:\n\n\ninfoseq genome.fasta\n\n\n\n\n\n\nchromosome: 30019\n\n\nplasmid: 2252\n\n\n\n\nQuestions\n\n\nWhy is this section so complicated?\n\n\n\n\nAnswer (click to reveal)\n\nFinding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744\n\n\n\nWhy can PacBio sequencing miss small plasmids?\n\n\n\n\nAnswer (click to reveal)\n\nLibrary prep size selection.\n\n\n\nWe extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing?\n\n\n\n\nAnswer (click to reveal)\n\nRepeats that have mapped to the PacBio assembly.\n\n\n\nHow do you find a plasmid in a Bandage graph?\n\n\n\n\nAnswer (click to reveal)\n\nIt is probably circular, matches the size of a known plasmid, and has a rep gene.\n\n\n\nAre there easier ways to find plasmids?\n\n\n\n\nAnswer (click to reveal)\n\nPossibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler\n\n\n\n7. Correct the assembly\n\n\nSequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly. \n\n\nMake an alignment file\n\n\nIndex the fasta file:\n\n\nbwa index genome.fasta\n\n\n\n\nAlign the Illumina reads:\n\n\nbwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam\n\n\n\n\n\n\nAligns Illumina \nR1.fq\n and \nR2.fq\n to the PacBio assembly \ngenome.fasta\n. \n\n\nThis produces a .bam file\n\n\n|\n pipes the output to samtools to sort (required for downstream processing)\n\n\n> pilon_aln.bam\n redirects the sorted bam to this file\n\n\n\n\nIndex the files:\n\n\nsamtools index pilon_aln.bam\n\n\n\n\nsamtools faidx genome.fasta\n\n\n\n\n\n\nNow we have an alignment file to use  with the tool \nPilon\n: \npilon_aln.bam\n\n\n\n\nRun Pilon\n\n\nRun:\n\n\npilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4\n\n\n\n\n\n\n--genome\n is the name of the input assembly to be corrected\n\n\n--frags\n is the alignment of the reads against the assembly\n\n\n--output\n is the name of the output prefix\n\n\n--fix\n is an option for types of corrections\n\n\n--mindepth\n gives a minimum read depth to use\n\n\n--changes\n produces an output file of the changes made\n\n\n--verbose\n prints information to the screen during the run\n\n\n--threads\n: number of cores\n\n\n\n\nLook at the changes file: \n\n\nless pilon1.changes\n\n\n\n\nExample:\n\n\n\n\n\n\nWe can see lots of cases where a deletion (represented by a dot) has been corrected to a base.  \n\n\nType \nq\n to exit. \n\n\n\n\nLook at the details of the fasta file: \n\n\ninfoseq pilon1.fasta\n\n\n\n\n\n\nchromosome - 30060 (net +41 bases)\n\n\nplasmid - 2252 (no change)\n\n\n\n\nChange the file name: \n\n\ncp pilon1.fasta assembly.fasta\n\n\n\n\nWe now have the corrected genome assembly of \nStaphylococcus aureus\n in .fasta format, containing a chromosome and a small plasmid.  \n\n\nQuestions\n\n\nWhy don't we correct earlier in the assembly process?\n\n\n\n\nAnswer (click to reveal)\n\nWe need to circularise the contigs and trim overhangs first.\n\n\n\nWhy can we use some reads (Illumina) to correct other reads (PacBio) ?\n\n\n\n\nAnswer (click to reveal)\n\nIllumina reads have higher accuracy.\n\n\n\nCould we just use PacBio reads to assemble the genome?\n\n\n\n\nAnswer (click to reveal)\n\nYes, if accuracy adequate.\n\n\n\n8. Comparative Genomics\n\n\nIn the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine complete bacterial genome. \n\n\nAssemblies\n\n\nThis bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades). \n\n\nAssembly graphs:\n\n\nLook at the assembly graph (usually has a suffix .gfa), in the program \nBandage\n. This shows how contigs are related, albeit with ambiguity in some places.\n\n\nThe assembly graph from Illumina reads (Spades assembly):\n\n\n\n\nThe assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid:\n\n\n\n\nHere we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement. \n\n\nDoes it matter that an assembly is in many contigs?\n\n\n\n\nAnswer (click to reveal)\n\nYes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure (\ne.g.\n the number of plasmids) and the location of genes of interest (\ne.g.\n gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information).\n\n\n\nAnnotations\n\n\nGenomic features such as genes can be identified with annotation tools. We have used a tool called \nProkka\n to annotate the two genomes described above. \n\n\nSome of the output data is displayed here:\n\n\n\n\n\n\n\n\n\n\nassembly:\n\n\nPacBio\n\n\nIllumina\n\n\n\n\n\n\n\n\n\n\nsize\n\n\n2,825,804\n\n\n2,792,905\n\n\n\n\n\n\ncontigs\n\n\n2\n\n\n123\n\n\n\n\n\n\nCDS\n\n\n2614\n\n\n2575\n\n\n\n\n\n\ntRNA\n\n\n61\n\n\n65\n\n\n\n\n\n\nrRNA\n\n\n19\n\n\n4\n\n\n\n\n\n\n\n\n\n\nWhy are there more CDS identified in the PacBio assembly? \n\n\n\n\nAnswer (click to reveal)\n\nThe PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results.\n\n\n\n\nWhy are there more rRNA identified in the PacBio assembly? \n\n\n\n\nAnswer (click to reveal)\n\nThere may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly. \n\n\n\n9. Summary\n\n\nIn this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome. \n\n\nProcedure and tools: \n\n\n\n\nCanu to assemble long-read PacBio data\n\n\nCirclator to trim and circularise contigs\n\n\nBWA-MEM to map shorter Illumina reads to the PacBio assembly\n\n\nSpades to assemble any unmapped, leftover Illumina reads (the plasmid)\n\n\nPilon to correct the PacBio assembly with the more accurate Illumina reads\n\n\n\n\nWe also looked at comparative genomics:\n\n\n\n\nBandage to examine assembly graphs\n\n\nProkka to annotate genomes with features such as genes\n\n\n\n\nFurther research:\n\n\n\n\nAlign genomes with Mauve: \ntutorial link\n\n\nFind core and pan genomes with Roary and Phandango: \ntutorial link\n\n\n\n\nMelbourne Bioinformatics tutorials:\n\n\n\n\nhttps://www.melbournebioinformatics.org.au/tutorials/\n\n\n\n\nAdditional microbial genomics tutorials:\n\n\n\n\nhttp://sepsis-omics.github.io/tutorials/",
            "title": "Long read assembly"
        },
        {
            "location": "/tutorials/pacbio/#long-read-assembly-workshop",
            "text": "This is a tutorial for a workshop on long-read (PacBio) genome assembly.   It demonstrates how to use long PacBio sequencing reads to assemble a bacterial genome, and includes additional steps for circularising, trimming, finding plasmids, and correcting the assembly with short-read Illumina data.",
            "title": "Long read assembly workshop"
        },
        {
            "location": "/tutorials/pacbio/#overview",
            "text": "Simplified version of workflow:",
            "title": "Overview"
        },
        {
            "location": "/tutorials/pacbio/#1-get-started",
            "text": "Your workshop trainers will provide you with the address of a virtual machine.",
            "title": "1. Get started"
        },
        {
            "location": "/tutorials/pacbio/#mac-users",
            "text": "Open the Terminal.    Type in    ssh researcher@[your virtual machine address]   Type in the password provided.",
            "title": "Mac users"
        },
        {
            "location": "/tutorials/pacbio/#windows-users",
            "text": "If you are using Windows 10, you might be able to use the Ubuntu Subsystem. Otherwise, install and open Putty.    Download putty  here .  Open. A configuration window will appear.   Under \"Host Name (or IP address)\" enter in the address of your virtual machine.   Under \"Port\" type in 22  Under \"Connection Type\" select \"SSH\"  Click \"Open\"  Under \"Login as:\" enter \"researcher\"  Type in the password provided.",
            "title": "Windows users"
        },
        {
            "location": "/tutorials/pacbio/#create-a-new-working-directory-on-your-remote-computer",
            "text": "Because we are starting a new analysis it is always good practice to start in a new empty directory. Therefore, we will create a new directory and change to it for the rest of the workshop.  In your terminal:   Create a new directory called \"Workshop\"   mkdir Workshop   Change to that directory   cd Workshop  NOTE: Every time you open a new terminal or Putty session, you will need to make sure you are in this directory again.  Find your current directory by typing:  pwd",
            "title": "Create a new working directory on your remote computer."
        },
        {
            "location": "/tutorials/pacbio/#2-get-data",
            "text": "The sample used in this tutorial is from a bacteria called  Staphylococcus aureus . We have used a small section of its real genome so that the programs can run in the workshop time.   The files we need are:   pacbio.fq : the PacBio reads  R1.fq : the Illumina forward reads  R2.fq : the Illumina reverse reads   In a new tab, go to  https://doi.org/10.5281/zenodo.1009308 .    Next to the first file, right-click (or control-click) the \"Download\" button, and select \"Copy link address\".  Back in your terminal, enter    wget [paste file link here]   The file should download.   Note:  paste the link to the file, not to the webpage.  Repeat this for the other two files.",
            "title": "2. Get data"
        },
        {
            "location": "/tutorials/pacbio/#3-assemble",
            "text": "We will use the assembly software called  Canu , version 1.6.  Run Canu with these commands:  canu -p canu -d canu_outdir genomeSize=0.03m -pacbio-raw pacbio.fq   the first  canu  tells the program to run  -p canu  names prefix for output files (\"canu\")  -d canu_outdir  names output directory (\"canu_outdir\")  genomeSize  only has to be approximate. (In this case we are using a partial genome of expected size 30,000 base pairs).   Canu will correct, trim and assemble the reads.  Various output will be displayed on the screen.  Note : Canu could say \"Finished\" but may still be running. In this case, type  squeue  to see if jobs are still running.    If you run  squeue  you will see something like this:               JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n                 6      main canu_can research PD       0:00      1 (Dependency)\n               5_1      main cormhap_ research  R       0:29      1 master  You will know if  Canu  has completely finished when  squeue  shows no jobs listed under the header row.",
            "title": "3. Assemble"
        },
        {
            "location": "/tutorials/pacbio/#4-check-assembly-output",
            "text": "Move into the canu output folder:   cd canu_outdir  View the list of files:   ls   The  canu.contigs.fasta  are the assembled sequences.  The  canu.unassembled.fasta  are the reads that could not be assembled.  The  canu.correctedReads.fasta.gz  are the corrected Pacbio reads that were used in the assembly.  The  canu.contigs.gfa  is the graph of the assembly.  The  canu.report  file is a summary of all of the steps Canu performed with information about the reads used, how they were handled and a whole lot of summary information about the assembly.   Display summary information about the contigs: ( infoseq  is a tool from  EMBOSS )  infoseq canu.contigs.fasta   This will show the contigs found by Canu. e.g.,  tig00000001  39136  \"tig00000001\" is the name given to the contig  \"39136\" is the number of base pairs in that contig.   This matches what we were expecting for this sample (approximately 30,000 base pairs). For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output.  We should also look at the  canu.report . To do this:  less canu.report   \"less\" is a command to display the file on the screen.  Use the up and down arrows to scroll up and down.   You will see lots of histograms of read lengths before and after processing, final contig construction, etc.   For a description of the outputs that Canu produces, see:  http://canu.readthedocs.io/en/latest/tutorial.html#outputs  Type  q  to exit viewing the report.",
            "title": "4. Check assembly output"
        },
        {
            "location": "/tutorials/pacbio/#questions",
            "text": "How do long- and short-read assembly methods differ?   Answer (click to reveal) \nShort reads are usually assembled with De Bruijn graphs. For long reads, there is a move back towards simpler overlap-layout-consensus methods.  Where can we find out the what the approximate genome size should be for the species being assembled?   Answer (click to reveal) \nGo to NCBI Genomes, enter species name, click on Genome Assembly and Annotation report, sort table by clicking on the column header Size (Mb), look at range of sizes in this column.  In the assembly output, what are the unassembled reads?    Answer (click to reveal) \nReads and low-coverage contigs that were not used in the assembly.  What are the corrected reads? How did canu correct the reads?   Answer (click to reveal) \nCanu builds overlaps between reads. The consensus is used to correct the reads.   Where could you view the output .gfa and what would it show?   Answer (click to reveal) \nA useful program is  Bandage . If the assembly has multiple contigs, the assembly graph shows how these are connected.",
            "title": "Questions"
        },
        {
            "location": "/tutorials/pacbio/#5-trim-and-circularise",
            "text": "Bacteria have circular chromosomes.    Because of sequencing errors, there may be some \"overhang\" in the assembled linear sequence.   Our assembly may have some overhang because it is 9000 bases longer than expected.     Adapted from Figure 1. Hunt et al. Genome Biology 2015  A tool called  Circlator  identifies and trims overhangs (on chromosomes and plasmids). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.  Move back into your main analysis folder:   cd ..",
            "title": "5. Trim and circularise"
        },
        {
            "location": "/tutorials/pacbio/#run-circlator",
            "text": "circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir    (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.)    --threads  is the number of cores   --verbose  prints progress information to the screen  canu_outdir/canu.contigs.fasta  is the file path to the input Canu assembly  canu_outdir/canu.correctedReads.fasta.gz  is the file path to the corrected Pacbio reads - note, fastA not fastQ  circlator_outdir  is the name of the output directory.   Some output will print to screen. When finished, it should say \"Circularized x of x contig(s)\".",
            "title": "Run Circlator"
        },
        {
            "location": "/tutorials/pacbio/#check-the-output",
            "text": "Move into the Circlator output directory:   cd circlator_outdir  List the files:   ls  Circlator has named the output files with numbers as prefixes.   Were the contigs circularised?  less 04.merge.circularise.log   \"less\" is a command to display the file on the screen.  04.merge.circularise.log  is the name of the file.   Yes, the contig was circularised (last column).  Type  q  to exit.   What are the trimmed contig sizes?   infoseq 06.fixstart.fasta   The contig \"tig00000001\" has a length of 30019.  This is about 9000 bases shorter than before circularisation. This was the \"overhang\" and has now been trimmed.    Copy the circularised contigs file to the main analysis directory with a new name:  cp 06.fixstart.fasta ../contig1.fasta  Move back into the main folder:   cd ..",
            "title": "Check the output"
        },
        {
            "location": "/tutorials/pacbio/#questions_1",
            "text": "Were all the contigs circularised?    Answer (click to reveal) \nIn this example, yes, the contig was circularised.  Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs?   Answer (click to reveal) \nCirclator uses dnaA (if present) for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.",
            "title": "Questions"
        },
        {
            "location": "/tutorials/pacbio/#6-find-smaller-plasmids",
            "text": "Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.  This section involves several steps:   Use the Canu+Circlator output of a trimmed assembly contig.  Map all the Illumina reads against this Pacbio-assembled contig.  Extract any reads that  didn't  map and assemble them together: this could be a plasmid, or part of a plasmid.  Look for overhang: if found, trim.",
            "title": "6. Find smaller plasmids"
        },
        {
            "location": "/tutorials/pacbio/#align-illumina-reads-to-the-pacbio-contig",
            "text": "Index the contigs file:  bwa index contig1.fasta  Align Illumina reads using using bwa mem:  bwa mem -t 4 contig1.fasta R1.fq R2.fq | samtools sort > aln.bam   bwa mem  is the alignment tool  -t 4  is the number of cores  contig1.fasta  is the input assembly file  R1.fq R2.fq  are the Illumina reads  | samtools sort  pipes the output to samtools to sort  > aln.bam  sends the alignment to the file  aln.bam",
            "title": "Align Illumina reads to the PacBio contig"
        },
        {
            "location": "/tutorials/pacbio/#extract-unmapped-illumina-reads",
            "text": "Index the alignment file:  samtools index aln.bam  Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \"unmapped\" files:  samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam   fastq  is a command that coverts a  .bam  file into fastq format  -f 4  : only output unmapped reads  -1  : put R1 reads into a file called  unmapped.R1.fastq  -2  : put R2 reads into a file called  unmapped.R2.fastq  -s  : put singleton reads into a file called  unmapped.RS.fastq  aln.bam  : input alignment file   We now have three files of the unampped reads:   unmapped.R1.fastq ,   unmapped.R2.fastq ,   unmapped.RS.fastq .",
            "title": "Extract unmapped Illumina reads"
        },
        {
            "location": "/tutorials/pacbio/#assemble-the-unmapped-reads",
            "text": "Assemble with Spades:  spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly    (Click on the dark grey slider bar above and move it to the right, to see all the way to the end of the line.)    -1  is input file forward   -2  is input file reverse  -s  is unpaired  --careful  minimizes mismatches and short indels  --cov-cutoff auto  computes the coverage threshold (rather than the default setting, \"off\")  -o  is the output directory   Move into the output directory:   cd spades_assembly  Look at the contigs:   infoseq contigs.fasta   1 contig has been assembled with a length of 2359 bases.    Copy it to a new file:   cp contigs.fasta contig2.fasta",
            "title": "Assemble the unmapped reads"
        },
        {
            "location": "/tutorials/pacbio/#trim-the-plasmid",
            "text": "To trim any overhang on this plasmid, we will blast the start of contig2 against itself.  Take the start of the contig:   head -n 10 contig2.fasta > contig2.fa.head   head -n 10  takes the first ten lines of  contig2.fasta   >  sends that output to a new file called  contig2.fa.head    We want to see if the start of the contig matches the end (overhang).    Format the assembly file for blast:   makeblastdb -in contig2.fasta -dbtype nucl   makeblastdb  makes a database for the tool Blast  This will generate three new files in the directory with suffixes .nhr, .nin and .nsq  -in  sets the input file as   contig2.fasta  -dbtype nucl  sets the type to nucleotide (rather than protein)   Blast the start of the assembly (.head file) against all of the assembly:   blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls   blastn  is the tool Blast, set as blast n  to compare sequences of nucleotides to each other  -query  sets the input sequence as  contig2.fa.head  -db  sets the database as that of the original sequence  contig2.fasta . We don't have to specify the other files that were created when we formatted this file, but they need to present in our current directory.   -evalue  is the number of hits expected by chance, here set as 1e-3  -dust no  turns off the masking of low-complexity regions  -out  sets the output file as  contig2.bls   Look at the hits (the matches):   less contig2.bls   The first hit is at the start, as expected. We can see that \"Query 1\" (the start of the contig) is aligned to \"Sbject 1\" (the whole contig), for the first 540 bases.  Scroll down with the down arrow.   The second hit shows \"Query 1\" (the start of the contig) also matches to \"Sbject 1\" (the whole contig) at position 2253, all the way to the end, position 2359.      This is the overhang.  Therefore, in the next step, we need to trim the contig to position 2252.  Type  q  to exit.    First, change the name of the contig within the file:  nano contig2.fasta   nano  opens up a text editor.   Use the arrow keys to navigate. (The mouse won't work.)  At the first line, delete the text, which will be something like \">NODE_1_length_2359_cov_3.320333\"  Type in \">contig2\"   Don't forget the  >  symbol  Press Control-X  \"Save modified buffer ?\" - type  Y  Press the Enter key   Index the file (this will allow samtools to edit the file as it will have an index):   samtools faidx contig2.fasta   faidx  means index the fasta file   Trim the contig:  samtools faidx contig2.fasta contig2:1-2252 > plasmid.fasta   this extracts contig2 from position 1-2252   > plasmid.fasta  sends the extracted section to a new file    We now have a trimmed plasmid.    Copy the plasmid file into the main folder:   cp plasmid.fasta ../  Move file back into main folder:   cd ..",
            "title": "Trim the plasmid"
        },
        {
            "location": "/tutorials/pacbio/#collect-contigs",
            "text": "Collect the chromosome and the plasmid in one fasta file (they will be 2 records in the file):   cat contig1.fasta plasmid.fasta > genome.fasta  See the contigs and sizes:  infoseq genome.fasta   chromosome: 30019  plasmid: 2252",
            "title": "Collect contigs"
        },
        {
            "location": "/tutorials/pacbio/#questions_2",
            "text": "Why is this section so complicated?   Answer (click to reveal) \nFinding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: https://doi.org/10.1101/086744  Why can PacBio sequencing miss small plasmids?   Answer (click to reveal) \nLibrary prep size selection.  We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing?   Answer (click to reveal) \nRepeats that have mapped to the PacBio assembly.  How do you find a plasmid in a Bandage graph?   Answer (click to reveal) \nIt is probably circular, matches the size of a known plasmid, and has a rep gene.  Are there easier ways to find plasmids?   Answer (click to reveal) \nPossibly. One option is the program called Unicycler which may automate many of these steps. https://github.com/rrwick/Unicycler",
            "title": "Questions"
        },
        {
            "location": "/tutorials/pacbio/#7-correct-the-assembly",
            "text": "Sequences from PacBio can have more errors than those from Illumina. Therefore, although it is useful to use the long PacBio reads to assemble the genome, we can also use the shorter and more accurate Illumina reads to correct errors in the PacBio assembly.",
            "title": "7. Correct the assembly"
        },
        {
            "location": "/tutorials/pacbio/#make-an-alignment-file",
            "text": "Index the fasta file:  bwa index genome.fasta  Align the Illumina reads:  bwa mem -t 4 genome.fasta R1.fq R2.fq | samtools sort > pilon_aln.bam   Aligns Illumina  R1.fq  and  R2.fq  to the PacBio assembly  genome.fasta .   This produces a .bam file  |  pipes the output to samtools to sort (required for downstream processing)  > pilon_aln.bam  redirects the sorted bam to this file   Index the files:  samtools index pilon_aln.bam  samtools faidx genome.fasta   Now we have an alignment file to use  with the tool  Pilon :  pilon_aln.bam",
            "title": "Make an alignment file"
        },
        {
            "location": "/tutorials/pacbio/#run-pilon",
            "text": "Run:  pilon --genome genome.fasta --frags pilon_aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 4   --genome  is the name of the input assembly to be corrected  --frags  is the alignment of the reads against the assembly  --output  is the name of the output prefix  --fix  is an option for types of corrections  --mindepth  gives a minimum read depth to use  --changes  produces an output file of the changes made  --verbose  prints information to the screen during the run  --threads : number of cores   Look at the changes file:   less pilon1.changes  Example:    We can see lots of cases where a deletion (represented by a dot) has been corrected to a base.    Type  q  to exit.    Look at the details of the fasta file:   infoseq pilon1.fasta   chromosome - 30060 (net +41 bases)  plasmid - 2252 (no change)   Change the file name:   cp pilon1.fasta assembly.fasta  We now have the corrected genome assembly of  Staphylococcus aureus  in .fasta format, containing a chromosome and a small plasmid.",
            "title": "Run Pilon"
        },
        {
            "location": "/tutorials/pacbio/#questions_3",
            "text": "Why don't we correct earlier in the assembly process?   Answer (click to reveal) \nWe need to circularise the contigs and trim overhangs first.  Why can we use some reads (Illumina) to correct other reads (PacBio) ?   Answer (click to reveal) \nIllumina reads have higher accuracy.  Could we just use PacBio reads to assemble the genome?   Answer (click to reveal) \nYes, if accuracy adequate.",
            "title": "Questions"
        },
        {
            "location": "/tutorials/pacbio/#8-comparative-genomics",
            "text": "In the workshop so far, we used a partial bacterial genome so that the exercises could run in the time available. As a demonstration, to better see the effect of long and short reads on the assembly, we will examine complete bacterial genome.",
            "title": "8. Comparative Genomics"
        },
        {
            "location": "/tutorials/pacbio/#assemblies",
            "text": "This bacterial genome has been assembled from either long PacBio reads (using Canu) or shorter Illumina reads (using Spades).   Assembly graphs:  Look at the assembly graph (usually has a suffix .gfa), in the program  Bandage . This shows how contigs are related, albeit with ambiguity in some places.  The assembly graph from Illumina reads (Spades assembly):   The assembly graph from PacBio reads (Canu assembly) - this is missing the small plasmid:   Here we can see that the long read data results in a more contiguous assembly - one complete chromosome versus many smaller contigs with ambiguous placement.   Does it matter that an assembly is in many contigs?   Answer (click to reveal) \nYes and No. Yes: broken genes can lead to missing/incorrect annotations; fragmented assemblies provide less information about the genomic structure ( e.g.  the number of plasmids) and the location of genes of interest ( e.g.  gene A is located on plasmid X). No: many or all genes may still be annotated correctly. Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes may be enough information).",
            "title": "Assemblies"
        },
        {
            "location": "/tutorials/pacbio/#annotations",
            "text": "Genomic features such as genes can be identified with annotation tools. We have used a tool called  Prokka  to annotate the two genomes described above.   Some of the output data is displayed here:      assembly:  PacBio  Illumina      size  2,825,804  2,792,905    contigs  2  123    CDS  2614  2575    tRNA  61  65    rRNA  19  4      Why are there more CDS identified in the PacBio assembly?    Answer (click to reveal) \nThe PacBio assembly may have errors (usually a one base indel) which will cause a frame shift, which can result in three things: a longer CDS, a shorter CDS, or a shorter CDS plus an additional CDS. In addition, the Illumina assembly is about 33 kb smaller than the PacBio assembly. In bacteria, a rule of thumb is that 1 kb is roughly equal to one gene. Thus, we would probably expect about 33 fewer identified genes, which fits with these results.   Why are there more rRNA identified in the PacBio assembly?    Answer (click to reveal) \nThere may be multiple copies of the rRNAs and these could have been collapsed as repeats in the Illumina assembly.",
            "title": "Annotations"
        },
        {
            "location": "/tutorials/pacbio/#9-summary",
            "text": "In this workshop, we used bacterial sequencing data from long and short reads to produce a polished genome.   Procedure and tools:    Canu to assemble long-read PacBio data  Circlator to trim and circularise contigs  BWA-MEM to map shorter Illumina reads to the PacBio assembly  Spades to assemble any unmapped, leftover Illumina reads (the plasmid)  Pilon to correct the PacBio assembly with the more accurate Illumina reads   We also looked at comparative genomics:   Bandage to examine assembly graphs  Prokka to annotate genomes with features such as genes   Further research:   Align genomes with Mauve:  tutorial link  Find core and pan genomes with Roary and Phandango:  tutorial link   Melbourne Bioinformatics tutorials:   https://www.melbournebioinformatics.org.au/tutorials/   Additional microbial genomics tutorials:   http://sepsis-omics.github.io/tutorials/",
            "title": "9. Summary"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/",
            "text": "Identifying proteins from mass spec data\n\n\nOverview\n\n\nThis tutorial describes how to identify a list of proteins from tandem mass spectrometry data. \n\n\nAnalyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained \nde novo\n with this method.\n\n\nThe data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer.  The sample itself corresponds to a purified organelle from Mouse cells.  \n\n\nThe aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle.\n\n\nThis tutorial uses free software including;\n\n\n\n\nThe \nX!Tandem\n search engine\n\n\nThe \nTrans Proteomic Pipeline\n (TPP) for post-search validation\n\n\nThe \nProtk\n tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier\n\n\nThe \nGalaxy\n platform to bring all these tools together\n\n\n\n\nLogin to Galaxy\n\n\n\n\n\n\nOpen a browser and go to a Galaxy server.\n\n\n\n\nYou can use a galaxy server of your own \nor\n\n\nGalaxy Tute\n at genome.edu.au\n\n\n\n\n\n\n Use a supported browser. Firefox/Safari/Chrome all work well\n\n\nIf you use your own galaxy server you will need to make sure you have the \nprotk proteomics tools\n installed.\n\n\n\n\n\n\n\n\nRegister as a new user if you don\u2019t already have an account on that particular server\n\n\n\n\n\n\nImport mass spec data\n\n\n\n\nCreate a new history\n in Galaxy and name it \"Organelle Tutorial\"\n\n\n\n\nDownload datasets\n using the Galaxy uploader tool. \n\n\nOpen this tool by clicking the button as shown below\n\n\n\n\nAfter opening the tool select \nPaste/Fetch data\n and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML\n\n\n\nAfter the download is finished you should have a single item in your history.\n\n\n\n\n\n\nRename\n the history item by clicking the pencil icon beside it to \"Edit attributes\".\n\n\n\n\nThis should bring up a dialog box where you can edit the name.  \n\n\nChange the name by removing everything up to the last forward slash \"/\"\n\n\nYour item should then be named \nOrganelleSample.mzML\n\n\nDont forget to click \"Save\"\n\n\n\n\n\n\nBasic properties of the data\n\n\nFormat:\n\n\nMass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about \nmass spectrometry data formats here\n \n\n\n\n\n\n\n 1) What format is the OrganelleSample.mzML file?\n\n\n\n\n\n\n//<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>\n\n\n\n\n\n\nHint\n\n\n\n\nTry clicking the title bar on the data in your galaxy history.  This will toggle display of some additional information about the data.\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nmzML\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nMS versus MS/MS:\n\n\nA key feature of \nTandem\n mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide.  These two types of measurements are called \nMS\n and \nMS/MS\n spectra respectively.  The following schematic shows how an \nMS/MS\n scan results from the fragmentation of a selected product ion.  Often multiple \nMS/MS\n spectra are obtained for each \nMS\n scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed.\n\n\n\n\nNumber of spectra:\n\n\nClick the eye icon on the history item to view the \nmzML\n file as text.  The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of \nMS\n and \nMS/MS\n spectra in the file.\n\n\nNow try searching for the text \"MS1 spectrum\" in the page using your web browser's search function.  Looking closely you should see that this text appears once for every \nMS1\n spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description).  The file is large though and the browser can only see the first megabyte of it. \n\n\nNow search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\".  There are a total of 24941 spectra in the entire file including both \nMS\n and \nMS/MS\n spectra.\n\n\n\n\n\n\n 2) How many \nMS\n spectra are there in this dataset?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nTo answer this question you will need to use the \nSelect\n tool from the \nFilter and Sort\n submenu to select lines matching the text \"MS1 spectrum\" in the whole file.  Then use the \nLine/Word/Character count\n tool from the \nText Manipulation\n submenu to count the number of lines returned by running the \nSelect\n tool.\n\n\n\n\n\n\nMore\n <- and here to show more\n\n\n\n\nThe text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable5\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink5\").text(\"More\");\n            } else {\n                $(\"#showablelink5\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3142\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\nIn the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure. \nSee the instructions here\n\n\n\n\nPrior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching \nMS\n peaks against the masses of whole peptides via \nPeptide Mass Fingerprinting\n.  This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the \nMS/MS\n spectra.  In this tutorial only the \nMS/MS\n spectra will be used.\n\n\n\n\n\n\n 3) How many \nMS/MS\n spectra are there in this dataset?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the fact that the file contains a total of 24941 spectra with your answer to the previous question about \nMS\n spectra.\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n21799\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nAlternate data formats\n\n\nAnother format you are likely to encounter for tandem mass spectrometry data is \nMascot Generic Format\n or \nmgf\n.  Mascot Generic Format (\nmgf\n) is the data file format preferred by the \nMascot\n search engine.  It is a text based format is much easier to read by hand than the \nmzML\n file.  Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of (\nmz\n,\nintensity\n) pairs.  Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included. \n\n\n\n\n\n\nDownload the Organelle Sample data in \nmgf\n format\n\n\nUse the \nPaste/Fetch data\n tool again and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf\n\n\n\n\n\n\n\nInspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file.\n\n\n\n\n\n\n\n\n\n\n 4) How many spectra are there in this dataset and what type of spectra do you think they are?\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file.\n\n\n\n\n\n\nMore\n <- and here to show more\n\n\n\n\nConsider your answers to questions 3 and 4\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable12\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink12\").text(\"More\");\n            } else {\n                $(\"#showablelink12\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n21799\n\n\nMS/MS\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nObtain a Search Database\n\n\nSetting up a search database is a critical step.  For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues;\n\n\n\n\nDatabase size\n\n\nWhether to include decoys\n\n\nWhat types of variants to include if any\n\n\nHow to format your database identifiers\n\n\n\n\nMore details are provided \nhere\n.\n\n\n\n\n\n\nDownload a database of Mouse proteins in \nfasta\n format\n\n\nUse the \nPaste/Fetch data\n tool again and paste the following URL into the box that appears.  Then click \nStart\n to initiate the download.\n\n\nhttps://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta\n\n\n\n\n\n\n\nInspect the first few items in the database in Galaxy.  The file is in \nFasta\n format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information.  The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this;\n\n\nsp|Q9CQV8|1433B_MOUSE\n\n\n\nThe database also includes decoy sequences, appended at the end.  They have identifiers like this;\n\n\ndecoy_rp75404\n\n\n\n\n\n\n\n\n\n\n\n 5) What is the ratio of decoys to non-decoys in the database?\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nDecoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title).\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n1:1\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nRun a search using X!Tandem\n\n\nA large number of search engines now exist for proteomics data.  This exercise uses \nX!Tandem\n which is one of the fastest and most widely used. Other search engines include \nOMSSA\n, \nMS-GF+\n and \nMascot\n.\n\n\n\n\nSelect the \nX!Tandem Search\n tool\n\n\nEnter parameters as shown in the table below (leave all others at their defaults)\n\n\nClick Execute\n\n\n\n\n\n\n\n\n\n\nParameter Name\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nUploaded Fasta File\n\n\nUniprotMouseD_20140716.fasta\n\n\n\n\n\n\nMSMS File\n\n\nOrganelleSample.mgf\n\n\n\n\n\n\nVariable Modifications\n\n\nOxidation M\n\n\n\n\n\n\nFixed Modifications\n\n\nCarbamidomethyl C\n\n\n\n\n\n\nMissed Cleavages Allowed\n\n\n2\n\n\n\n\n\n\nEnzyme\n\n\nTrypsin\n\n\n\n\n\n\nFragment Ion Tolerance\n\n\n0.5\n\n\n\n\n\n\nPrecursor Ion Tolerance\n\n\n10 ppm\n\n\n\n\n\n\n\n\nThe search should run for about 5-10 minutes and will produce an output file in \nX!Tandem\n xml format. A much more useful format is \npepXML\n so the next step in the analysis will be to run a tool to convert from tandem to pepXML.\n\n\n\n\nSelect the \nTandem to pepXML\n tool\n\n\nSelect the output from the previous step as input and click \nExecute\n\n\n\n\nWhile the search is running, read some background theory on \nhow search engine's work\n.\n\n\nConvert Results to tabular format\n\n\nAlthough the \npepXML\n format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert \npepXML\n into tabular (tab separated) format, which is much easier to read.  Tabular format also has the advantage that it can be downloaded and opened using many other programs including \nExcel\n and \nR\n.\n\n\n\n\nSelect the \npepXML to Table\n tool\n\n\nSelect the \npepXML\n file produced in the previous step as input and click \nExecute\n\n\n\n\nTo get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest.  Explore the column assignments in your tabular file by clicking on its title in your galaxy history.  This will show extra details about the item, including a handy preview where column numbers are displayed.\n\n\n\n\n\n\n 6) In what column number is the \nassumed_charge\n of peptides in the \npepXML\n tabular file?\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nSort tabular outputs\n\n\nExamine the tabular output file from the previous step.  It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results.  These decoys should tend to have quite poor scores compared with real hits.  The \nraw score\n for \nX!Tandem\n searches is an \nE-value\n.  To push these decoys to the bottom of the list we can sort the data by \nraw score\n. \n\n\n\n\nSelect the \nSort\n data tool from the \nFilter and Sort\n menu in the left pane of Galaxy\n\n\nChoose to sort on \nraw_score\n.  This is column c10 \n\n\nSelect \nAscending order\n for the sort direction (small E-values are good) and click \nExecute\n\n\n\n\nBrowse the resulting dataset.  The top of the file should now have very few decoys.\n\n\nConvert raw scores to probabilities\n\n\nRaw \nX!Tandem\n scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct.  A number of tools exist for this, and in this tutorial we use \nPeptide Prophet\n, which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by \nPeptide Prophet\n can be used to set a threshold for acceptance.  For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to \nPeptide Prophet\n is \nPercolator\n.\n\n\nIf you're curious about how \nPeptide Prophet\n works, take a look at \nthis explainer\n, or \nthe original paper\n\n\n\n\nSelect the \nPeptide Prophet\n tool\n\n\nSelect the \nX!Tandem\n output in \npepXML\n format generated earlier as input\n\n\nCheck the box that says \nUse decoys to pin down the negative distribution\n.\n\n\nConvert the resulting \npepXML\n file to tabular using the \nPepXML to Table\n tool\n\n\n\n\nTake a look at the resulting tabular file.  Note that this time the \npeptideprophet_prob\n column is populated and contains numbers between 0 and 1.\n\n\n\n\n\n\n 7) How many PSM's have a peptideprophet probability greater than or equal to 0.95\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nUse the \nFilter\n tool from the \nFilter and Sort\n submenu. Also remember that Peptide Prophet probability is given in a column called \npeptideprophet_prob\n.  The syntax for \"greater than or equal to\" in the \nFilter\n tool is >=.\n\n\n\n\n\n\nMore\n <- and here to show more\n\n\n\n\nUse this text in \nmatch with condition\n field of the \nFilter and Sort\n tool.\n\n\nc11>=0.95\n\n\n\nTo answer the second question use the \nSelect\n tool on the filtered table to select lines matching \"decoy_\"\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable21\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink21\").text(\"More\");\n            } else {\n                $(\"#showablelink21\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n3808\n\n\n21\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\n 8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM)\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nConsider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3)\n\n\n\n\n\n\nMore\n <- and here to show more\n\n\n\n\nTo take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect.\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable25\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink25\").text(\"More\");\n            } else {\n                $(\"#showablelink25\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n17.27%\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nPerform Protein Inference\n\n\nUp to this point we have looked at peptide to spectrum matches \nPSMs\n.  Each of the peptides observed will have come from a protein sequence in the \nfasta\n file that we used as a database, and this protein is recorded along with the \nPSM\n itself in all of the result tables we've viewed so far.  Unfortunately, the process of inferring the existence of proteins based on these \nPSMs\n is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one \nPSM\n.\n\n\nThe \nProtein Prophet\n tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins.\n\n\n\n\nSelect the \nProtein Prophet\n tool\n\n\nChoose the \npepXML\n formatted output from \nPeptide Prophet\n as input and click \nExecute\n\n\nConvert the resulting \nprotXML\n to tabular using the \nprotXML to Table\n tool.\n\n\n\n\n\n\n\n\n 9) How many proteins are there with protein prophet probability greater than or equal to 0.99?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nFilter on column 6 \nprotein_probability\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n601\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink29\").click(function(e){\n            e.preventDefault();\n            $(\"#showable29\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\nIf you have time, read over \nthese notes\n on the Protein Prophet output.  Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.\n\n\nFunctional enrichment analysis\n\n\nThis step will allow you to discover the identity of the Organelle that was used to create the sample.  \n\n\nWe use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom).\n\n\n\n\nStart with unfiltered tabular protein prophet results\n\n\nUse the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains \nprotein_name\n). \n\n\nConvert the \"pipes\" that separate parts of the protein_name into separate columns using the \nConvert delimiters to TAB\n tool in the \nText manipulation\n submenu of Galaxy. This should result in a file with 3 columns\n\n\nUse the \nCut columns\n tool again to cut the second column from this dataset\n\n\nDownload this file to your desktop and rename it to \norganelle.txt\n\n\nOpen the \nGOrilla\n web page in your web browser\n\n\nSelect \nOrganism\n as Mouse\n\n\nUpload the \norganelle.txt\n file as a ranged gene list\n\n\nChoose \nComponent\n for the ontology\n\n\nSubmit\n\n\n\n\n\n\n\n\n 9) What \nintracellular\n organelle was enriched in the sample?\n\n\n\n\n\n\n\n\n\n\nHint\n\n\n\n\nIgnore terms relating to \nexosomes\n\n\nIn the resulting output look to the most enriched and most specific GO terms. \n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n    \n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nMitochondria\n\n\n\n\n\n\n\n\n\n\n\n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#identifying-proteins-from-mass-spec-data",
            "text": "",
            "title": "Identifying proteins from mass spec data"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#overview",
            "text": "This tutorial describes how to identify a list of proteins from tandem mass spectrometry data.   Analyses of this type are a fundamental part of most proteomics studies. The basic idea is to match tandem ms spectra obtained from a sample with equivalent theoretical spectra from a reference protein database. The process is referred to as \"protein database search\" or even \"protein sequencing\", although amino acid sequences are not obtained  de novo  with this method.  The data used in this tutorial were obtained from a single run on an Orbitrap Elite mass spectrometer.  The sample itself corresponds to a purified organelle from Mouse cells.    The aim of the tutorial will be to create a list of all proteins that can be confidently said to be present in the sample, and then to use this list to guess the identity of the \"mystery\" organelle.  This tutorial uses free software including;   The  X!Tandem  search engine  The  Trans Proteomic Pipeline  (TPP) for post-search validation  The  Protk  tool suite for various conversion tasks and to make working with X!Tandem and the TPP easier  The  Galaxy  platform to bring all these tools together",
            "title": "Overview"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#login-to-galaxy",
            "text": "Open a browser and go to a Galaxy server.   You can use a galaxy server of your own  or  Galaxy Tute  at genome.edu.au     Use a supported browser. Firefox/Safari/Chrome all work well  If you use your own galaxy server you will need to make sure you have the  protk proteomics tools  installed.     Register as a new user if you don\u2019t already have an account on that particular server",
            "title": "Login to Galaxy"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#import-mass-spec-data",
            "text": "Create a new history  in Galaxy and name it \"Organelle Tutorial\"   Download datasets  using the Galaxy uploader tool.   Open this tool by clicking the button as shown below   After opening the tool select  Paste/Fetch data  and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mzML  After the download is finished you should have a single item in your history.    Rename  the history item by clicking the pencil icon beside it to \"Edit attributes\".   This should bring up a dialog box where you can edit the name.    Change the name by removing everything up to the last forward slash \"/\"  Your item should then be named  OrganelleSample.mzML  Dont forget to click \"Save\"",
            "title": "Import mass spec data"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#basic-properties-of-the-data",
            "text": "Format:  Mass spectrometry data comes in many different formats and the first step in a proteomics analysis often involves data conversion or pre-processing. You can read more about  mass spectrometry data formats here       1) What format is the OrganelleSample.mzML file?    //<![CDATA[<!--\n(function(w,d,u){if(!w.$){w._delayed=true;console.info(\"Delaying JQuery calls\");w.readyQ=[];w.bindReadyQ=[];function p(x,y){if(x==\"ready\"){w.bindReadyQ.push(y);}else{w.readyQ.push(x);}};var a={ready:p,bind:p};w.$=w.jQuery=function(f){if(f===d||f===u){return a}else{p(f)}}}})(window,document)\n//-->]]>    Hint   Try clicking the title bar on the data in your galaxy history.  This will toggle display of some additional information about the data.     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink1\").click(function(e){\n            e.preventDefault();\n            $(\"#showable1\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    mzML      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink2\").click(function(e){\n            e.preventDefault();\n            $(\"#showable2\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n      MS versus MS/MS:  A key feature of  Tandem  mass spectrometry is the acquisition of mass spectra (spectra) that measure the masses of precursor ions (whole peptides) as well as spectra that measure the fragment masses of a single selected peptide.  These two types of measurements are called  MS  and  MS/MS  spectra respectively.  The following schematic shows how an  MS/MS  scan results from the fragmentation of a selected product ion.  Often multiple  MS/MS  spectra are obtained for each  MS  scan, selecting different precursor masses each time so that as many peptides as possible can be analyzed.   Number of spectra:  Click the eye icon on the history item to view the  mzML  file as text.  The file is almost impossible to read by hand but with some text searching we will be able to deduce the number of  MS  and  MS/MS  spectra in the file.  Now try searching for the text \"MS1 spectrum\" in the page using your web browser's search function.  Looking closely you should see that this text appears once for every  MS1  spectrum in the file (plus it occurs one extra time at the top of the file as part of the file description).  The file is large though and the browser can only see the first megabyte of it.   Now search for the text \"spectrumList count\". It should bring you to a line in the file that says spectrumList count=\"24941\".  There are a total of 24941 spectra in the entire file including both  MS  and  MS/MS  spectra.     2) How many  MS  spectra are there in this dataset?       Hint   To answer this question you will need to use the  Select  tool from the  Filter and Sort  submenu to select lines matching the text \"MS1 spectrum\" in the whole file.  Then use the  Line/Word/Character count  tool from the  Text Manipulation  submenu to count the number of lines returned by running the  Select  tool.    More  <- and here to show more   The text \"MS1 spectrum\" also appears at the top of the file as part of its description so you will need to subtract 1 from your answer     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink5\").click(function(e){\n            e.preventDefault();\n            $(\"#showable5\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable5\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink5\").text(\"More\");\n            } else {\n                $(\"#showablelink5\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n         \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink4\").click(function(e){\n            e.preventDefault();\n            $(\"#showable4\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    3142      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink6\").click(function(e){\n            e.preventDefault();\n            $(\"#showable6\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n       In the previous exercise we used two galaxy tools in succession to find out the number of items in a file that matched some text. Future exercises use the same technique so you might find it useful to create a tiny workflow to automate this proceedure.  See the instructions here   Prior to the development of tandem mass spectrometry, peptides and proteins were detected purely by matching  MS  peaks against the masses of whole peptides via  Peptide Mass Fingerprinting .  This has largely been superceded by tandem mass spectrometry which gains much greater specificity by using the  MS/MS  spectra.  In this tutorial only the  MS/MS  spectra will be used.     3) How many  MS/MS  spectra are there in this dataset?       Hint   Use the fact that the file contains a total of 24941 spectra with your answer to the previous question about  MS  spectra.     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink8\").click(function(e){\n            e.preventDefault();\n            $(\"#showable8\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    21799      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink9\").click(function(e){\n            e.preventDefault();\n            $(\"#showable9\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Basic properties of the data"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#alternate-data-formats",
            "text": "Another format you are likely to encounter for tandem mass spectrometry data is  Mascot Generic Format  or  mgf .  Mascot Generic Format ( mgf ) is the data file format preferred by the  Mascot  search engine.  It is a text based format is much easier to read by hand than the  mzML  file.  Each spectrum appears between \"BEGIN IONS\" and \"END IONS\" statements and simply consists of ( mz , intensity ) pairs.  Additional summary information about the precursor (whole peptide) ion such as its mass, retention time and charge are included.     Download the Organelle Sample data in  mgf  format  Use the  Paste/Fetch data  tool again and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/OrganelleSample.mgf    Inspect the data manually by viewing it in Galaxy. Try to get a feel for the way data is organised within the file.       4) How many spectra are there in this dataset and what type of spectra do you think they are?       Hint   Use the same technique you used for the previous exercise (or your workflow). Remember that for every spectrum there is one \"BEGIN IONS\" statement in the file.    More  <- and here to show more   Consider your answers to questions 3 and 4     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink12\").click(function(e){\n            e.preventDefault();\n            $(\"#showable12\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable12\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink12\").text(\"More\");\n            } else {\n                $(\"#showablelink12\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n         \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink11\").click(function(e){\n            e.preventDefault();\n            $(\"#showable11\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    21799  MS/MS      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink13\").click(function(e){\n            e.preventDefault();\n            $(\"#showable13\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Alternate data formats"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#obtain-a-search-database",
            "text": "Setting up a search database is a critical step.  For this tutorial we have created a database for you, but if you need to create a database for your own data you'll need to consider the following key issues;   Database size  Whether to include decoys  What types of variants to include if any  How to format your database identifiers   More details are provided  here .    Download a database of Mouse proteins in  fasta  format  Use the  Paste/Fetch data  tool again and paste the following URL into the box that appears.  Then click  Start  to initiate the download.  https://swift.rc.nectar.org.au:8888/v2/AUTH_ffb00634530a4c37a0b8b08c48068adf/proteomics_tutorial/UniprotMouseD_20140716.fasta    Inspect the first few items in the database in Galaxy.  The file is in  Fasta  format which means that each entry has a single description line that starts with a \">\" followed by a unique identifier and then some general descriptive information.  The actual sequence of amino acids is given after the description line. Take note of the format of the database identifiers. They are in Uniprot format and look like this;  sp|Q9CQV8|1433B_MOUSE  The database also includes decoy sequences, appended at the end.  They have identifiers like this;  decoy_rp75404       5) What is the ratio of decoys to non-decoys in the database?       Hint   Decoys are easiest to search for because they all start with \"decoy_\". The total number of database entries can be found simply expanding the fasta file in your history (by clicking on its title).     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink15\").click(function(e){\n            e.preventDefault();\n            $(\"#showable15\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    1:1      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink16\").click(function(e){\n            e.preventDefault();\n            $(\"#showable16\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Obtain a Search Database"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#run-a-search-using-xtandem",
            "text": "A large number of search engines now exist for proteomics data.  This exercise uses  X!Tandem  which is one of the fastest and most widely used. Other search engines include  OMSSA ,  MS-GF+  and  Mascot .   Select the  X!Tandem Search  tool  Enter parameters as shown in the table below (leave all others at their defaults)  Click Execute      Parameter Name  Value      Uploaded Fasta File  UniprotMouseD_20140716.fasta    MSMS File  OrganelleSample.mgf    Variable Modifications  Oxidation M    Fixed Modifications  Carbamidomethyl C    Missed Cleavages Allowed  2    Enzyme  Trypsin    Fragment Ion Tolerance  0.5    Precursor Ion Tolerance  10 ppm     The search should run for about 5-10 minutes and will produce an output file in  X!Tandem  xml format. A much more useful format is  pepXML  so the next step in the analysis will be to run a tool to convert from tandem to pepXML.   Select the  Tandem to pepXML  tool  Select the output from the previous step as input and click  Execute   While the search is running, read some background theory on  how search engine's work .",
            "title": "Run a search using X!Tandem"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#convert-results-to-tabular-format",
            "text": "Although the  pepXML  format is useful as input to other tools it is not designed to be read or analyzed directly. Galaxy includes a tool to convert  pepXML  into tabular (tab separated) format, which is much easier to read.  Tabular format also has the advantage that it can be downloaded and opened using many other programs including  Excel  and  R .   Select the  pepXML to Table  tool  Select the  pepXML  file produced in the previous step as input and click  Execute   To get the most out of tabular files it is often necessary to know the column number corresponding to columns of interest.  Explore the column assignments in your tabular file by clicking on its title in your galaxy history.  This will show extra details about the item, including a handy preview where column numbers are displayed.     6) In what column number is the  assumed_charge  of peptides in the  pepXML  tabular file?      Answer    3      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink18\").click(function(e){\n            e.preventDefault();\n            $(\"#showable18\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Convert Results to tabular format"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#sort-tabular-outputs",
            "text": "Examine the tabular output file from the previous step.  It contains many columns, of which the most interesting are probably the raw search engine score and the name of the protein from which each peptide has been derived. Note the presence of plenty of decoy proteins among the results.  These decoys should tend to have quite poor scores compared with real hits.  The  raw score  for  X!Tandem  searches is an  E-value .  To push these decoys to the bottom of the list we can sort the data by  raw score .    Select the  Sort  data tool from the  Filter and Sort  menu in the left pane of Galaxy  Choose to sort on  raw_score .  This is column c10   Select  Ascending order  for the sort direction (small E-values are good) and click  Execute   Browse the resulting dataset.  The top of the file should now have very few decoys.",
            "title": "Sort tabular outputs"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#convert-raw-scores-to-probabilities",
            "text": "Raw  X!Tandem  scores only give a rough estimate of the reliability of each peptide to spectrum match (PSM). A better estimate can be obtained by running a tool that uses global statistical properties of the search to assign a probability to each PSM of being correct.  A number of tools exist for this, and in this tutorial we use  Peptide Prophet , which can work with a wide variety of different search engine scoring systems. It is extremely useful as it effectively converts disparate scores to the common scale of probability. The probabilities produced by  Peptide Prophet  can be used to set a threshold for acceptance.  For example we could decide to accept only PSM's with a probability greater than 0.95. Note that this is not the same as the False Discovery Rate which is computed by taking (1-p) for all the accepted PSM's and dividing by the total number of accepted PSM's. A widely used alternative to  Peptide Prophet  is  Percolator .  If you're curious about how  Peptide Prophet  works, take a look at  this explainer , or  the original paper   Select the  Peptide Prophet  tool  Select the  X!Tandem  output in  pepXML  format generated earlier as input  Check the box that says  Use decoys to pin down the negative distribution .  Convert the resulting  pepXML  file to tabular using the  PepXML to Table  tool   Take a look at the resulting tabular file.  Note that this time the  peptideprophet_prob  column is populated and contains numbers between 0 and 1.     7) How many PSM's have a peptideprophet probability greater than or equal to 0.95      Hint   Use the  Filter  tool from the  Filter and Sort  submenu. Also remember that Peptide Prophet probability is given in a column called  peptideprophet_prob .  The syntax for \"greater than or equal to\" in the  Filter  tool is >=.    More  <- and here to show more   Use this text in  match with condition  field of the  Filter and Sort  tool.  c11>=0.95  To answer the second question use the  Select  tool on the filtered table to select lines matching \"decoy_\"     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink21\").click(function(e){\n            e.preventDefault();\n            $(\"#showable21\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable21\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink21\").text(\"More\");\n            } else {\n                $(\"#showablelink21\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n         \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink20\").click(function(e){\n            e.preventDefault();\n            $(\"#showable20\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    3808  21      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink22\").click(function(e){\n            e.preventDefault();\n            $(\"#showable22\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n         8) What proportion of MS/MS spectra in the original data file produce a reliable (probability greater than or equal to 0.95) peptide to spectrum match (PSM)      Hint   Consider your answer to question 7 relative to the total number of MS/MS spectra in the file (question 3)    More  <- and here to show more   To take account of decoys remember that for every decoy in the results there is likely to be another non-decoy that is incorrect.     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink25\").click(function(e){\n            e.preventDefault();\n            $(\"#showable25\").toggleClass(\"showable-hidden\");\n            if ($(\"#showable25\").hasClass(\"showable-hidden\")) {\n                $(\"#showablelink25\").text(\"More\");\n            } else {\n                $(\"#showablelink25\").text(\"Less\");\n            }\n        });\n    }); //-->]]>\n         \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink24\").click(function(e){\n            e.preventDefault();\n            $(\"#showable24\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    17.27%      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink26\").click(function(e){\n            e.preventDefault();\n            $(\"#showable26\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Convert raw scores to probabilities"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#perform-protein-inference",
            "text": "Up to this point we have looked at peptide to spectrum matches  PSMs .  Each of the peptides observed will have come from a protein sequence in the  fasta  file that we used as a database, and this protein is recorded along with the  PSM  itself in all of the result tables we've viewed so far.  Unfortunately, the process of inferring the existence of proteins based on these  PSMs  is much more complicated than that because some peptides are found in more than one protein, and of course some proteins are supported by more than one  PSM .  The  Protein Prophet  tool can be used to run a proper protein inference analysis, and assigns probabilities to individual proteins, as well as groups of related proteins.   Select the  Protein Prophet  tool  Choose the  pepXML  formatted output from  Peptide Prophet  as input and click  Execute  Convert the resulting  protXML  to tabular using the  protXML to Table  tool.      9) How many proteins are there with protein prophet probability greater than or equal to 0.99?      Hint   Filter on column 6  protein_probability     \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink28\").click(function(e){\n            e.preventDefault();\n            $(\"#showable28\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    601      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink29\").click(function(e){\n            e.preventDefault();\n            $(\"#showable29\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n      If you have time, read over  these notes  on the Protein Prophet output.  Explore your output (use the unfiltered tabular output) to find examples of different kinds of Protein groupings.",
            "title": "Perform Protein Inference"
        },
        {
            "location": "/tutorials/proteomics_basic/proteomics_basic/#functional-enrichment-analysis",
            "text": "This step will allow you to discover the identity of the Organelle that was used to create the sample.    We use the GOrilla gene ontology enrichment analysis tool (a web based tool) to discover GO terms that are over-represented in proteins at the top of our list compared with those that are assigned very low probabilities (at the bottom).   Start with unfiltered tabular protein prophet results  Use the Cut columns tool from the Text Manipulation menu to extract the third column from the filtered protein table (contains  protein_name ).   Convert the \"pipes\" that separate parts of the protein_name into separate columns using the  Convert delimiters to TAB  tool in the  Text manipulation  submenu of Galaxy. This should result in a file with 3 columns  Use the  Cut columns  tool again to cut the second column from this dataset  Download this file to your desktop and rename it to  organelle.txt  Open the  GOrilla  web page in your web browser  Select  Organism  as Mouse  Upload the  organelle.txt  file as a ranged gene list  Choose  Component  for the ontology  Submit      9) What  intracellular  organelle was enriched in the sample?      Hint   Ignore terms relating to  exosomes  In the resulting output look to the most enriched and most specific GO terms.      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink31\").click(function(e){\n            e.preventDefault();\n            $(\"#showable31\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>\n        Answer    Mitochondria      \n    //<![CDATA[<!--\n    $(document).ready(function(){\n        $(\"#showablelink32\").click(function(e){\n            e.preventDefault();\n            $(\"#showable32\").toggleClass(\"showable-hidden\");\n        });\n    }); //-->]]>",
            "title": "Functional enrichment analysis"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/",
            "text": "How to use Galaxy\n\n\nThis background wiki gives very brief guides on performing specific tasks in Galaxy.  For much more extensive documentation including many videos, online tutorials and discussion forums please \nconsult the galaxy wiki\n.\n\n\nCreate a new History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nCreate new\n\n\nThe new history will be called \"Unnamed History\". Click its title to rename it.\n\n\n\n\nRename a history item\n\n\n\n\nLocate the item in your history and click its pencil icon\n\n\nEnter a new name in the \nName:\n field and click \nSave\n\n\n\n\n\n\nFind someone's exact username\n\n\nClick the \nUser\n menu at the top of Galaxy.  The menu that appears will show the currently logged in username\n\n\n\n\nShare a History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nShare or Publish\n\n    \n\n\nThen select \nShare with another user\n and enter the user's \nfull username\n\n\n\n\nImport a History\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nHistories Shared with Me\n\n    \n\n\nSelect and view the desired history by clicking on its name\n    \n\n\n\n\nCopy Datasets\n\n\n\n\nClick the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy.\n\n\nSelect \nHistories Shared with Me\n\n    \n\n\nThe screen that appears allows copying of specific datasets between any of your histories\n\n\n\n\nMulti File Inputs\n\n\nTools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input.\nAfter clicking the multi-file icon the display should change to allow multiple inputs to be selected.\n\n\n\n\nSaved Histories\n\n\nTo view a list of your saved histories click the \nHistory Options\n menu (cog icon) in the top-right corner of Galaxy and select \nSaved Histories\n\nOnce the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags",
            "title": "Galaxy Background"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#how-to-use-galaxy",
            "text": "This background wiki gives very brief guides on performing specific tasks in Galaxy.  For much more extensive documentation including many videos, online tutorials and discussion forums please  consult the galaxy wiki .",
            "title": "How to use Galaxy"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#create-a-new-history",
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Create new  The new history will be called \"Unnamed History\". Click its title to rename it.",
            "title": "Create a new History"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#rename-a-history-item",
            "text": "Locate the item in your history and click its pencil icon  Enter a new name in the  Name:  field and click  Save",
            "title": "Rename a history item"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#find-someones-exact-username",
            "text": "Click the  User  menu at the top of Galaxy.  The menu that appears will show the currently logged in username",
            "title": "Find someone's exact username"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#share-a-history",
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Share or Publish \n      Then select  Share with another user  and enter the user's  full username",
            "title": "Share a History"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#import-a-history",
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Histories Shared with Me \n      Select and view the desired history by clicking on its name",
            "title": "Import a History"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#copy-datasets",
            "text": "Click the  History Options  menu (cog icon) in the top-right corner of Galaxy.  Select  Histories Shared with Me \n      The screen that appears allows copying of specific datasets between any of your histories",
            "title": "Copy Datasets"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#multi-file-inputs",
            "text": "Tools that support running over multiple input files indicate this by showing a multi-file icon beside the relevant input.\nAfter clicking the multi-file icon the display should change to allow multiple inputs to be selected.",
            "title": "Multi File Inputs"
        },
        {
            "location": "/tutorials/proteomics_basic/background_galaxy/#saved-histories",
            "text": "To view a list of your saved histories click the  History Options  menu (cog icon) in the top-right corner of Galaxy and select  Saved Histories \nOnce the list of your histories appears you can switch to a history by clicking it. You can also use this view to delete histories or organise them by tags",
            "title": "Saved Histories"
        },
        {
            "location": "/tutorials/proteomics_basic/background_data_formats/",
            "text": "Data Formats and Pre-processing\n\n\nMass Spectrometry data analysis is plagued by an overabundance of file formats.  The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data, \nmzML\n.  The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively.  The reference implementation of the mzML standard is a software suite called \nProteoWizard\n. ProteoWizard includes a very handy tool called \nmsconvert\n that is capable of converting raw data from most instruments into \nmzML\n or into one of many other formats.  In addition to format conversion, \nmsconvert\n can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis.  A typical pre-processing involves;\n\n\n\n\nConversion from instrument .raw to mzML\n\n\nPeak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert)\n\n\nDenoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window\n\n\nConvert spectrum identifiers into a standardized format\n\n\n\n\nTo convert files from raw instrument native formats to \nmzML\n a windows PC is required.  If you need to do this, be sure to download \nProteoWizard\n with \nvendor reader support\n .  This package comes with \nMSConvertGUI\n which allows conversion of raw files using a graphical interface.  Once files are in \nmzML\n or \nmgf\n format they can be converted to various other formats using the \nmsconvert3\n tool in Galaxy.",
            "title": "Data Formats Background"
        },
        {
            "location": "/tutorials/proteomics_basic/background_data_formats/#data-formats-and-pre-processing",
            "text": "Mass Spectrometry data analysis is plagued by an overabundance of file formats.  The good news is that the Mass Spec community, including many instrument vendors have developed a standard file format for raw data,  mzML .  The bad news is that many of the old formats are still in widespread use, and most instruments don't produce it natively.  The reference implementation of the mzML standard is a software suite called  ProteoWizard . ProteoWizard includes a very handy tool called  msconvert  that is capable of converting raw data from most instruments into  mzML  or into one of many other formats.  In addition to format conversion,  msconvert  can also perform a wide variety of noise filtering and peak-picking functions to prepare data for analysis.  A typical pre-processing involves;   Conversion from instrument .raw to mzML  Peak picking on both MS1 and MS2 data using vendor-native peak picking routines (built in to msconvert)  Denoising of MS2 data either by thresholding or by keeping only the largest peaks withing a moving window  Convert spectrum identifiers into a standardized format   To convert files from raw instrument native formats to  mzML  a windows PC is required.  If you need to do this, be sure to download  ProteoWizard  with  vendor reader support  .  This package comes with  MSConvertGUI  which allows conversion of raw files using a graphical interface.  Once files are in  mzML  or  mgf  format they can be converted to various other formats using the  msconvert3  tool in Galaxy.",
            "title": "Data Formats and Pre-processing"
        },
        {
            "location": "/tutorials/proteomics_basic/background_findcountitems_workflow/",
            "text": "Create a workflow\n\n\n\n\nClick the \nWorkflow\n menu item at the top of galaxy\n\n\n\n\nClick the button called \nCreate New Workflow\n\n\n\n\n\n\n\n\nEnter a name and description for the new workflow and click \nCreate\n\n\n\n\n\n\nYou should now have a blank workflow canvas.  Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu.  Under \nWorkflow Control\n and then under \nInputs\n you should see an \nInput dataset\n item. Click it to create a blank input box\n\n\n\n\n\n\n\n\nNow add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are;\n\n\n\n\nThe \nSelect\n tool from the \nFind and Sort\n submenu\n\n\nThe \nLine/Word/Character\n count tool from the \nText Manipulation\n submenu\n\n\n\n\n\n\n\n\nAfter adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next\n\n\n\n\n\n\n\n\nSave your workflow\n\n\n\n\n\n\n\n\nAfter saving the workflow return to the main \nWorkflow\n menu (top of Galaxy) and select your new workflow to run it.  Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.",
            "title": "Create find count items workflow"
        },
        {
            "location": "/tutorials/proteomics_basic/background_findcountitems_workflow/#create-a-workflow",
            "text": "Click the  Workflow  menu item at the top of galaxy   Click the button called  Create New Workflow     Enter a name and description for the new workflow and click  Create    You should now have a blank workflow canvas.  Create an input box for the workflow by scrolling to the bottom left of the galaxy tool menu.  Under  Workflow Control  and then under  Inputs  you should see an  Input dataset  item. Click it to create a blank input box     Now add workflow nodes for other tools by clicking on the relevant tool in the galaxy tool menu (left pane of galaxy). The tools to add are;   The  Select  tool from the  Find and Sort  submenu  The  Line/Word/Character  count tool from the  Text Manipulation  submenu     After adding these tools you can join them up by dragging from the outputs of one node to the inputs of the next     Save your workflow     After saving the workflow return to the main  Workflow  menu (top of Galaxy) and select your new workflow to run it.  Before running the workflow you will be presented with a window that allows you to alter the workflow inputs.",
            "title": "Create a workflow"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/",
            "text": "Protein Databases\n\n\nIn a perfect experiment we would obtain fragment ions for all the \nb,y\n pairs of each peptide.  If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself.  Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins.  This \ndatabase\n is usually just a \nFASTA\n formatted file containing amino acid sequences for all known proteins from your study organism.  Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results.  In order to detect a peptide, its exact sequence must be explicitly included in the database. \n\n\nLarge vs Small Database\n\n\nSince it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of \nNCBInr\n.  There are two problems with this.  The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits.  Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine. \n\n\nNote that very small databases can also cause problems.  In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions.  With a very small database (or with very few spectra) it may not be possible to model these distributions accurately.  In most practical situations this is not an issue.\n\n\nTypical sources of data for search databases\n\n\nUniprot.org\n: This is the canonical resource for publicly available protein sequences. It includes two large databases \nSwissProt\n, which contains manually curated sequences and \nTrembl\n which contains sequences automatically generated from genomic and transcriptomic data.\n\n\nAn Organism Specific database\n: In some cases, a community of researchers working on specific organisms will create their own sequence data repositories.  Some of these are well maintained and are the best source of data for that study organism.  Examples include \nPlasmoDB\n for Malaria for \nFlybase\n for drosophila.\n\n\nTranscriptome derived sequences\n: If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it.  If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database.\n\n\nOther\n: Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome. \n\n\nShould I include decoys?\n\n\nDecoys are often useful, but not always needed.  Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of \nPeptide Prophet\n typically use decoys because it can use these to 'pin down' the negative distribution.",
            "title": "Protein Databases"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#protein-databases",
            "text": "In a perfect experiment we would obtain fragment ions for all the  b,y  pairs of each peptide.  If peaks can be unambiguously identified for all these pairs then the sequence of a peptide can simply be read off from the fragmentation spectrum itself.  Unfortunately this is almost never the case using current instrumentation, and the only practical method to determine the sequences of peptides and proteins present in a sample is to compare spectra with a database of potential proteins.  This  database  is usually just a  FASTA  formatted file containing amino acid sequences for all known proteins from your study organism.  Constructing this database is a crucial first step in any proteomics analysis because only peptide sequences present in the database will appear in the results.  In order to detect a peptide, its exact sequence must be explicitly included in the database.",
            "title": "Protein Databases"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#large-vs-small-database",
            "text": "Since it is impossible to detect a peptide unless it is present in the search database, one might consider using a very large database such as the full content of  NCBInr .  There are two problems with this.  The most important is that the sensivity of a search goes down as the search space goes up, which means that searches on large databases often return far fewer hits.  Another, more practical issue is that searching a large database often takes an extremely long time, and might even crash your search engine.   Note that very small databases can also cause problems.  In particular, some search engines, and most search engine post-processing statistical tools attempt to model the shape of peptide-sequence-match (PSM) score distributions.  With a very small database (or with very few spectra) it may not be possible to model these distributions accurately.  In most practical situations this is not an issue.",
            "title": "Large vs Small Database"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#typical-sources-of-data-for-search-databases",
            "text": "Uniprot.org : This is the canonical resource for publicly available protein sequences. It includes two large databases  SwissProt , which contains manually curated sequences and  Trembl  which contains sequences automatically generated from genomic and transcriptomic data.  An Organism Specific database : In some cases, a community of researchers working on specific organisms will create their own sequence data repositories.  Some of these are well maintained and are the best source of data for that study organism.  Examples include  PlasmoDB  for Malaria for  Flybase  for drosophila.  Transcriptome derived sequences : If you are working on an organism for which public sequence data are scarce, it may be worth obtaining transcriptomic sequences for it.  If sufficient data are obtained, the resulting assembled transcript sequences can be translated to form a good quality proteomic database.  Other : Depending on the project you might want to include sequences for specific variants of interest, or a six-frame translation of a genome.",
            "title": "Typical sources of data for search databases"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_databases/#should-i-include-decoys",
            "text": "Decoys are often useful, but not always needed.  Often, the decision to include decoys depends on the requirements of software that is used downstream of the search. Examples on this wiki that make use of  Peptide Prophet  typically use decoys because it can use these to 'pin down' the negative distribution.",
            "title": "Should I include decoys?"
        },
        {
            "location": "/tutorials/proteomics_basic/background_search_engines/",
            "text": "How Search Engines Work\n\n\nWhen choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below.\n\n\n\n\nThe next spectrum in the dataset is loaded for consideration\n\n\nThe spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database.  This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space.  Database search space size also depends on many other factors including\n\n\nThe size of the protein database\n\n\nVariable modifications allowed on peptides\n\n\nParent ion mass tolerance\n\n\nNumber of allowed missed enzymatic cleavages\n\n\nSpecificity of the enzyme used for digestion\n\n\n\n\n\n\nEach of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other.\n\n\nThe highest scoring \npeptide spectrum match\n (PSM) is recorded along with its score.\n\n\nSome form of global analysis of (PSM) scores is performed in order to determine a threshold of significance\n\n\n\n\nThere are many excellent presentations online that explain this in more detail.  Although it's old, I recommend \nthis presentation by Brian Searle",
            "title": "Search Engines"
        },
        {
            "location": "/tutorials/proteomics_basic/background_search_engines/#how-search-engines-work",
            "text": "When choosing search engine parameters it can be helpful to understand the basic algorithms that most search engines share. The workflow of a typical search engine is roughly as outlined below.   The next spectrum in the dataset is loaded for consideration  The spectrum parent mass and the parent mass tolerance is used to select a small set of matching peptides from the database.  This is a crucial step because the number of peptides that fall within the matching mass window will determine the effective size of the search space.  Database search space size also depends on many other factors including  The size of the protein database  Variable modifications allowed on peptides  Parent ion mass tolerance  Number of allowed missed enzymatic cleavages  Specificity of the enzyme used for digestion    Each of the matching peptides is scored against the spectrum. The nature of this scoring is where search engines typically differ from each other.  The highest scoring  peptide spectrum match  (PSM) is recorded along with its score.  Some form of global analysis of (PSM) scores is performed in order to determine a threshold of significance   There are many excellent presentations online that explain this in more detail.  Although it's old, I recommend  this presentation by Brian Searle",
            "title": "How Search Engines Work"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_prophet/",
            "text": "Protein Prophet\n\n\nThe development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference.  The original paper describing protein prophet is worth reading. It's citation is;\n\n\n\n\nNesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003).\n\n\n\n\nThe practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging.  Let's look at a few examples;\n\n\nUniquely identified protein\n\n\n\n\nsp|P00761|TRYP_PIG\n\n\n\n\nSearch for this protein in the Protein Prophet results file. It should have \ngroup_probability\n and \nprotein_probability\n scores of \n1.0\n.  All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group.\n\n\nIndistinguishable Protein\n\n\n\n\nsp|O08600|NUCG_MOUSE\n\n\n\n\nIn this case there still just one entry for the protein, but \nProtein Prophet\n lists another protein \ntr|Q3UN47|Q3UN47_MOUSE\n in the indistinguishable proteins column.  This protein is indistinguishable from the primary entry \nsp|O08600|NUCG_MOUSE\n because all of the identified peptides are shared between both.\n\n\nA well behaved protein group\n\n\n\n\nsp|O08677|KNG1_MOUSE\n\n\n\n\nThis protein is part of a smallish group of similar proteins.  The overall group probability is high (\n1.0\n) but probabilities group members are different.  The first member of the group has a high probability \n0.99\n but all other members have probabilities of \n0.0\n.  This is because all of the high scoring peptides are contained in the first entry.  Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor;\n\n\n\n\nplurality should not be posited with out necessity\n\n\n\n\nIn other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein.\n\n\nAnomalous groups\n\n\nIn rare cases Protein Prophet fails produces strange results when its algorithm fails to converge.  This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.",
            "title": "Protein Prophet"
        },
        {
            "location": "/tutorials/proteomics_basic/background_protein_prophet/#protein-prophet",
            "text": "The development of the Protein Prophet statistical models, and its associated program was a big step forward for practitioners wanting to perform automated, large scale protein inference.  The original paper describing protein prophet is worth reading. It's citation is;   Nesvizhskii, A. I., Keller, A., Kolker, E. & Aebersold, R. A Statistical Model for Identifying Proteins by Tandem Mass Spectrometry. Anal. Chem. 75, 4646\u20134658 (2003).   The practical reality of using Protein Prophet is a little different however as the program has undergone several significant developments since its original publication, and interpretation of Protein Prophet groupings can be challenging.  Let's look at a few examples;  Uniquely identified protein   sp|P00761|TRYP_PIG   Search for this protein in the Protein Prophet results file. It should have  group_probability  and  protein_probability  scores of  1.0 .  All of the three peptides that contribute evidence for this protein map uniquely to this protein alone so there are no other proteins in this group.  Indistinguishable Protein   sp|O08600|NUCG_MOUSE   In this case there still just one entry for the protein, but  Protein Prophet  lists another protein  tr|Q3UN47|Q3UN47_MOUSE  in the indistinguishable proteins column.  This protein is indistinguishable from the primary entry  sp|O08600|NUCG_MOUSE  because all of the identified peptides are shared between both.  A well behaved protein group   sp|O08677|KNG1_MOUSE   This protein is part of a smallish group of similar proteins.  The overall group probability is high ( 1.0 ) but probabilities group members are different.  The first member of the group has a high probability  0.99  but all other members have probabilities of  0.0 .  This is because all of the high scoring peptides are contained in the first entry.  Evidence for the other entries consists of either (a) peptides that are contained in the first entry or (b) peptides with very low scores. Protein Prophet uses the principle of Occam's razor;   plurality should not be posited with out necessity   In other words, unless otherwise indicated by a unique peptide, we should assume that shared peptides come from the same protein.  Anomalous groups  In rare cases Protein Prophet fails produces strange results when its algorithm fails to converge.  This can result in situations where the group probability is high (1.0) but all of the member proteins within the group are assigned a probability of 0.",
            "title": "Protein Prophet"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/",
            "text": "Molecular Dynamics Tutorial - Introduction to cluster computing\n\n\n\n\nIn the following tutorials we will be logging on a high performance computer (HPC) to submit \nNAMD\n molecular dynamics (MD) jobs and visualising the results with the molecular visualization program \nVMD\n. As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple.\n\n\n\n\n\n\nMD tutorial - Introduction to cluster computing (this tutorial)\n: If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD.\n\n\n\n\n\n\nMD tutorial - Building input files\n: In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.\n\n\n\n\n\n\nAs we will be working from a terminal on the cluster and later downloading data back to our local desktop for visualization and analysis, we will be assuming that the users have basic knowledge of Unix/Linux. If you are unfamiliar with these, we suggest you first work through this \nUnix tutorial\n.\n\n\n\n\n1 - Overview\n\n\nThe aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs.\n\n\nThe program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the \nTheoretical and Computational Biophysics Group at Illinois University at Urbana Champaign\n. It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output.\n\n\n2 - Basic introduction to cluster computing\n\n\na) Logging in to the cluster\n\n\nUsing a terminal on your local computer and your username and password, login to the HPC cluster.\n\n\nssh <username>@snowy.vlsci.unimelb.edu.au\n\n\n\n\nYou should see a welcome screen and a command line prompt.\nIf you type \nls\n at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet!\n\n\n\n\nNote\n: be careful to use the right \nterminal\n when you are typing in commands!\nSometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example:\n\n\n[<username>@snowy ~]$\n               <- tells you your terminal is on snowy\n\n\n\n\nb) Copy across files, starting the job\n\n\nWe\u2019ll need to copy across the basic example directory to our working directory on \nsnowy\n. Do this with:\n\n\ncp -r /vlsci/examples/namd/Namd_simple_example_01 .\n\n\n\n\n\n\nNote that the dot is important!\n\n\n\n\nChange into this directory and launch the job with the command \nsbatch\n and the sbatch script.\n\n\ncd Namd_simple_example_01\n\n\n\n\nThen type:\n\n\nsbatch sbatch_namd_start_job\n\n\n\n\nYour job has now been submitted to the cluster. Easy hey?\nCheck the job is running with the \nshowq\n command. (type it at the command line).\n\n\nToo much information? Try:\n\n\nshowq -u <username>\n\n\n\n\nThis particular job is \nvery\n short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the \n.dcd\n suffix.\n\n\nc) Understanding the input files\n\n\nWhile we wait, let's take a look at the sbatch example script to understand what is going on. Type:\n\n\nless sbatch_namd_start_job\n\n\n\n\n\n\nless\n is a Unix file viewer. \nPress \u201cq\u201d to quit the viewing of the file\n\n\n\n\nYou should see the lines:\n\n\n#SBATCH --nodes=4\n\n\n\n\nThis line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC.\n\n\nNow let us have a look at the NAMD configuration script:\n\n\nless namd_1ubq_example.conf\n\n\n\n\nWoah! There is quite a bit of information here, don't worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top:\n\n\nstructure    1ubq_example.psf\ncoordinates  1ubq_example.pdb\noutputName   1ubq_output_01\n\n\n\n\nThese are simply defining the input files, (the protein structure file \n.psf\n, and the coordinate file, \n.pdb\n) and also the name of the output files.\n\n\nFurther down you will see:\n\n\nset Temp    310\ntemperature $Temp\n\n\n\n\nWhich is setting the temperature to 310 K (37 C) while below that we have:\n\n\n## Parameter file\n\nparaTypeCharmm  on\nparameters      par_all27_prot_na.prm\n\n\n\n\nwhich tells NAMD which parameter file to use. (you'll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file.\n\n\nSomewhere in the middle you will see these lines:\n\n\n## Periodic Boundary Conditions\n\ncellBasisVector1  48. 0.  0.\ncellBasisVector2  0. 48.  0.\ncellBasisVector3  0.  0. 48.\ncellOrigin        0   0   0\nwrapAll on\nwrapWater on\n\n\n\n\nThis defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other.\n\n\nNear the bottom we have the lines:\n\n\n## Output files\n\nrestartfreq 5000\ndcdfreq 100\nxstFreq 100\noutputEnergies 100\noutputPressure 100\noutputTiming 100\n\n\n\n\nThese lines tell us how often to write out to the output files. The most important is the \ndcdfreq\n, (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The \n.dcd\n output file can become ridiculously \nHUGE\n if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs!\n\n\nThe last few line in the configuration file:\n\n\n## Minimize, reinitialize velocities, run dynamics\n\nminimize 500\nrun 10000\n\n\n\n\ntell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. (\nThis is a very short example!\n). Typically you might set \"run\" to 10,000,000 or more.\n\n\nPress \"q\" to quit viewing the configuration file.\n\n\nCheck again on the status of your job:\n\n\nshowq -u <username>\n\n\n\n\nIf you don't see anything it probably means the job has finished.\nList your directory using the command \nls -lrt\n and you should see something like:\n\n\n[mike@snowy Namd_simple_example_01]$ ls -rlt\ntotal 17088\n-rw-r--r-- 1 mike VR0021   243622 Dec  8 11:40 par_all27_prot_na.prm\n-rw-r--r-- 1 mike VR0021      655 Dec  8 11:40 sbatch_namd_start_bluegene\n-rw-r--r-- 1 mike VR0021     3932 Dec  8 11:40 namd_1ubq_example.conf\n-rw-r--r-- 1 mike VR0021   814960 Dec  8 11:40 1ubq_example.pdb\ndrwxr-xr-x 2 mike VR0021      512 Dec  8 11:40 BUILD_DIR\n-rw-r--r-- 1 mike VR0021      700 Dec  8 11:40 sbatch_namd_restartjob_bluegene\n-rw-r--r-- 1 mike VR0021  1182412 Dec  8 11:40 1ubq_example.psf\n-rw-r--r-- 1 mike VR0021     4508 Dec  8 11:40 namd_1ubq_restart_example.conf\n-rw-r--r-- 1 mike VR0021      159 Dec  8 11:40 slurm-2746442.out\n-rw-r--r-- 1 mike VR0021     1371 Dec  8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:41 1ubq_output.restart.coor.old\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:41 1ubq_output.restart.vel.old\n-rw-r--r-- 1 mike VR0021      216 Dec  8 11:41 1ubq_output.restart.xsc.old\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.restart.coor\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.restart.vel\n-rw-r--r-- 1 mike VR0021      218 Dec  8 11:42 1ubq_output.restart.xsc\n-rw-r--r-- 1 mike VR0021     8417 Dec  8 11:42 1ubq_output.xst\n-rw-r--r-- 1 mike VR0021 13005576 Dec  8 11:42 1ubq_output.dcd\n-rw-r--r-- 1 mike VR0021      215 Dec  8 11:42 1ubq_output.xsc\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.coor\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.vel\n-rw-r--r-- 1 mike VR0021   425681 Dec  8 11:43 Namd_1ubq_example_output.txt\n\n\n\n\nThe highlighted \n.dcd\n file is the main output file while the \n.xsc\n, \n.coor\n, \n.vel\n files all have to do with being able to restart the simulation at a later date, while the \nNamd_1ubq_example_output.txt\n file contains the text output from the simulation.\n\n\nCongratulations!\n\n\nYou have just run a short molecular dynamics simulation on the cluster. Next, we'll copy that information back to your local computer and use VMD to visualize the results.\n\n\nNow on to part 2, visualizing the results with VMD.\n\n\n3 - Visualizing NAMD results with VMD\n\n\nWelcome to part 2 of the NAMD tutorial, where you will be using the \nmolecular visualization program VMD\n to look at the trajectory data of the ubiquitin protein you generated in part 1 of the tutorial. If you haven't already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux).\n\n\n\n\nTip\n: the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the \nsnowy\n cluster using the program \nssh\n.\n\n\nYou can usually tell which computer you are logged into by the terminal command line: for example the terminal command line:\n\n\n[mike@snowy Namd_simple_example_01]$\n\n\ntells me I am logged into \nsnowy\n, in the \nNamd_simple_example_01\n directory.\n\n\nCompared to my local terminal command line:\n\n\nmike@axion:~$\n\n\ntells me I am on my local machine (called \"axion\" in this case).\n\n\n\n\nDownload the entire NAMD example directory back to your \nlocal computer\n.\nFor example, in Linux you can type in your \nlocal computer\n terminal: (if you see \nsnowy\n in the command line prompt you are typing in the wrong terminal!)\n\n\nscp -r <username>@snowy.vlsci.unimelb.edu.au:Namd_simple_example_01 .\n\n\n\n\nWhat to do if your simulations didn\u2019t run.\n\n\nIf for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal:\n\n\nscp -r <username>@snowy.vlsci.unimelb.edu.au:vlsci/examples/namd/Namd_simple_example_01_finished .  \n\n\n\n\nYou can now start VMD \nlocally\n and load up the trajectory data. In a new local terminal type:\n\n\nvmd\n\n\n\n\n\n\nNote\n: On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer.\n\n\n\n\nTwo new windows should pop up.\n\n\nThe \nmain panel\n:\n\n\n\n\nThe \ndisplay\n:\n\n\n\n\nAnd the terminal should turn into the \nconsole\n:\n\n\n\n\na) Reading structure data files into VMD\n\n\nThe first file you need to read into VMD is the protein structure file,\n(1ubq_example.psf in this case). The \n.psf\n file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information.\n\n\nFrom the \nmain panel\n:\n\n\n\n\nFile \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load\n\n\n\n\nYou should see nothing in the display, but an entry in the Main panel.\nNext load the coordinates from the \n.pdb\n file:\n\n\n\n\nFile \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load\n\n\n\n\nNow you should have the model in the display that can be moved around with the mouse.\nThis is the initial starting position of the simulation. Next load in the trajectory data into VMD:\n\n\n\n\nFile \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load\n\n\n\n\nThis data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the \nmain panel\n. (Use the speed scroll bar to the left of that button to slow it down).\n\n\nWhat you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases.\n\n\nFrom the \nmain panel\n you can bring up the Representations dialogue\nto play with more rendering types: - try them out!\n\n\n\n\nGraphics \u2192 Representations\n\n\n\n\nAnd this conclude the basic tutorial to running a simple job on a cluster.\nWasn\u2019t so scary now was it?\n\n\nPlease play around with VMD. Once you feel comfortable, try start the next tutorial:\n\nMolecular dynamics - Building input files",
            "title": "Molecular Dynamics - Introduction to cluster computing"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#molecular-dynamics-tutorial-introduction-to-cluster-computing",
            "text": "In the following tutorials we will be logging on a high performance computer (HPC) to submit  NAMD  molecular dynamics (MD) jobs and visualising the results with the molecular visualization program  VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple.    MD tutorial - Introduction to cluster computing (this tutorial) : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD.    MD tutorial - Building input files : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.    As we will be working from a terminal on the cluster and later downloading data back to our local desktop for visualization and analysis, we will be assuming that the users have basic knowledge of Unix/Linux. If you are unfamiliar with these, we suggest you first work through this  Unix tutorial .",
            "title": "Molecular Dynamics Tutorial - Introduction to cluster computing"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#1-overview",
            "text": "The aim of this tutorial is to give the user experience to upload and submit a typical parallel job to the cluster, download the output, and visualize the preliminary results. Though this example is mostly relevant to those studying in the life sciences area, the workflow is representative of launching other parallel jobs.  The program we shall use, NAMD, is a parallel, molecular dynamics simulation program developed by the  Theoretical and Computational Biophysics Group at Illinois University at Urbana Champaign . It is particularly good at modelling large biomolecular systems using HPC clusters and is freely available for academic work. If you are interested in running NAMD simulations you should also install a local copy of VMD on your own computer. VMD is a molecular viewer program developed by the same group that can be used to help set up NAMD simulation and to help visualize NAMD output.",
            "title": "1 - Overview"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#2-basic-introduction-to-cluster-computing",
            "text": "",
            "title": "2 - Basic introduction to cluster computing"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-logging-in-to-the-cluster",
            "text": "Using a terminal on your local computer and your username and password, login to the HPC cluster.  ssh <username>@snowy.vlsci.unimelb.edu.au  You should see a welcome screen and a command line prompt.\nIf you type  ls  at the prompt you should see a list of the files and directories - which should be nothing as we haven\u2019t put anything there yet!   Note : be careful to use the right  terminal  when you are typing in commands!\nSometimes you need to type the commands on the cluster terminal, and sometimes on your local terminal. You can tell which is which by looking at the command line prompt. When you are logged into the cluster, you should see the machine name at the prompt, for example:  [<username>@snowy ~]$                <- tells you your terminal is on snowy",
            "title": "a) Logging in to the cluster"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#b-copy-across-files-starting-the-job",
            "text": "We\u2019ll need to copy across the basic example directory to our working directory on  snowy . Do this with:  cp -r /vlsci/examples/namd/Namd_simple_example_01 .   Note that the dot is important!   Change into this directory and launch the job with the command  sbatch  and the sbatch script.  cd Namd_simple_example_01  Then type:  sbatch sbatch_namd_start_job  Your job has now been submitted to the cluster. Easy hey?\nCheck the job is running with the  showq  command. (type it at the command line).  Too much information? Try:  showq -u <username>  This particular job is  very  short compared to a regular NAMD simulation and should be finished in less than 4 minutes. As the job runs, various output files are produced but the main one you will be interested in is the trajectory file with the  .dcd  suffix.",
            "title": "b) Copy across files, starting the job"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#c-understanding-the-input-files",
            "text": "While we wait, let's take a look at the sbatch example script to understand what is going on. Type:  less sbatch_namd_start_job   less  is a Unix file viewer.  Press \u201cq\u201d to quit the viewing of the file   You should see the lines:  #SBATCH --nodes=4  This line tells us how many cores we are to use for the job. Generally speaking, the more cores used the faster the job runs, but only to a point. Molecular dynamics jobs don\u2019t always scale efficiently so eventually more cores will actually run slower than less cores. This depends very much on the program you use and the architecture of the HPC.  Now let us have a look at the NAMD configuration script:  less namd_1ubq_example.conf  Woah! There is quite a bit of information here, don't worry though. Most of the parameters stay the same for each simulation. Let us just point out the important bits. See the lines near the top:  structure    1ubq_example.psf\ncoordinates  1ubq_example.pdb\noutputName   1ubq_output_01  These are simply defining the input files, (the protein structure file  .psf , and the coordinate file,  .pdb ) and also the name of the output files.  Further down you will see:  set Temp    310\ntemperature $Temp  Which is setting the temperature to 310 K (37 C) while below that we have:  ## Parameter file\n\nparaTypeCharmm  on\nparameters      par_all27_prot_na.prm  which tells NAMD which parameter file to use. (you'll see a copy of the parameter file in the NAMD_example directory). Depending on what molecules you have in your simulation, you may need to use a more detailed parameter file.  Somewhere in the middle you will see these lines:  ## Periodic Boundary Conditions\n\ncellBasisVector1  48. 0.  0.\ncellBasisVector2  0. 48.  0.\ncellBasisVector3  0.  0. 48.\ncellOrigin        0   0   0\nwrapAll on\nwrapWater on  This defines the boundary conditions (a cheats way to simulate an infinite system), where a molecule coming off one side of the boundary will reappear on the other.  Near the bottom we have the lines:  ## Output files\n\nrestartfreq 5000\ndcdfreq 100\nxstFreq 100\noutputEnergies 100\noutputPressure 100\noutputTiming 100  These lines tell us how often to write out to the output files. The most important is the  dcdfreq , (here set to 100), or how often to save a frame of the dynamics trajectory. (Usually this is set around 5,000 to 10,000). The  .dcd  output file can become ridiculously  HUGE  if this is too small. 100 is OK for this short example. Just remember to change it for a much larger value in your own runs!  The last few line in the configuration file:  ## Minimize, reinitialize velocities, run dynamics\n\nminimize 500\nrun 10000  tell us that the simulation is first minimized for 500 steps (a good idea to iron out bad contacts and steric clashes, but usually set to 1000 steps or more), and then told to run only a mere 10,000 steps. ( This is a very short example! ). Typically you might set \"run\" to 10,000,000 or more.  Press \"q\" to quit viewing the configuration file.  Check again on the status of your job:  showq -u <username>  If you don't see anything it probably means the job has finished.\nList your directory using the command  ls -lrt  and you should see something like:  [mike@snowy Namd_simple_example_01]$ ls -rlt\ntotal 17088\n-rw-r--r-- 1 mike VR0021   243622 Dec  8 11:40 par_all27_prot_na.prm\n-rw-r--r-- 1 mike VR0021      655 Dec  8 11:40 sbatch_namd_start_bluegene\n-rw-r--r-- 1 mike VR0021     3932 Dec  8 11:40 namd_1ubq_example.conf\n-rw-r--r-- 1 mike VR0021   814960 Dec  8 11:40 1ubq_example.pdb\ndrwxr-xr-x 2 mike VR0021      512 Dec  8 11:40 BUILD_DIR\n-rw-r--r-- 1 mike VR0021      700 Dec  8 11:40 sbatch_namd_restartjob_bluegene\n-rw-r--r-- 1 mike VR0021  1182412 Dec  8 11:40 1ubq_example.psf\n-rw-r--r-- 1 mike VR0021     4508 Dec  8 11:40 namd_1ubq_restart_example.conf\n-rw-r--r-- 1 mike VR0021      159 Dec  8 11:40 slurm-2746442.out\n-rw-r--r-- 1 mike VR0021     1371 Dec  8 11:40 FFTW_NAMD_2.9_BlueGeneQ.txt\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:41 1ubq_output.restart.coor.old\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:41 1ubq_output.restart.vel.old\n-rw-r--r-- 1 mike VR0021      216 Dec  8 11:41 1ubq_output.restart.xsc.old\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.restart.coor\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.restart.vel\n-rw-r--r-- 1 mike VR0021      218 Dec  8 11:42 1ubq_output.restart.xsc\n-rw-r--r-- 1 mike VR0021     8417 Dec  8 11:42 1ubq_output.xst\n-rw-r--r-- 1 mike VR0021 13005576 Dec  8 11:42 1ubq_output.dcd\n-rw-r--r-- 1 mike VR0021      215 Dec  8 11:42 1ubq_output.xsc\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.coor\n-rw-r--r-- 1 mike VR0021   247564 Dec  8 11:42 1ubq_output.vel\n-rw-r--r-- 1 mike VR0021   425681 Dec  8 11:43 Namd_1ubq_example_output.txt  The highlighted  .dcd  file is the main output file while the  .xsc ,  .coor ,  .vel  files all have to do with being able to restart the simulation at a later date, while the  Namd_1ubq_example_output.txt  file contains the text output from the simulation.  Congratulations!  You have just run a short molecular dynamics simulation on the cluster. Next, we'll copy that information back to your local computer and use VMD to visualize the results.  Now on to part 2, visualizing the results with VMD.",
            "title": "c) Understanding the input files"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#3-visualizing-namd-results-with-vmd",
            "text": "Welcome to part 2 of the NAMD tutorial, where you will be using the  molecular visualization program VMD  to look at the trajectory data of the ubiquitin protein you generated in part 1 of the tutorial. If you haven't already done so, download and install VMD to your local computer (make sure to pick the right flavour: Windows, Mac or Linux).   Tip : the most confusing part of this exercise seems to be remembering to use the right terminal! That is, either one is connected to the cluster, or one is running on your local computer. The exercises we just ran were on the cluster. That is, a terminal which we used to connect to the  snowy  cluster using the program  ssh .  You can usually tell which computer you are logged into by the terminal command line: for example the terminal command line:  [mike@snowy Namd_simple_example_01]$  tells me I am logged into  snowy , in the  Namd_simple_example_01  directory.  Compared to my local terminal command line:  mike@axion:~$  tells me I am on my local machine (called \"axion\" in this case).   Download the entire NAMD example directory back to your  local computer .\nFor example, in Linux you can type in your  local computer  terminal: (if you see  snowy  in the command line prompt you are typing in the wrong terminal!)  scp -r <username>@snowy.vlsci.unimelb.edu.au:Namd_simple_example_01 .  What to do if your simulations didn\u2019t run.  If for some reason your simulations didn\u2019t run properly you can download a copy of the precomputed data to your local computer by using the following command command from a local terminal:  scp -r <username>@snowy.vlsci.unimelb.edu.au:vlsci/examples/namd/Namd_simple_example_01_finished .    You can now start VMD  locally  and load up the trajectory data. In a new local terminal type:  vmd   Note : On Windows, start VMD from the Program menu, under the folder University of Illinios. If this doesn\u2019t work, there could be a problem with defining the path to vmd on your computer.   Two new windows should pop up.  The  main panel :   The  display :   And the terminal should turn into the  console :",
            "title": "3 - Visualizing NAMD results with VMD"
        },
        {
            "location": "/tutorials/molecular_dynamics_101/molecular_dynamics_101/#a-reading-structure-data-files-into-vmd",
            "text": "The first file you need to read into VMD is the protein structure file,\n(1ubq_example.psf in this case). The  .psf  file contains important information about the system such as which atoms are bonded together, what charge they are, and the mass of each atom type, but does not contain any spatial information.  From the  main panel :   File \u2192 New molecule \u2192 (browse: 1ubq_example.psf) \u2192 load   You should see nothing in the display, but an entry in the Main panel.\nNext load the coordinates from the  .pdb  file:   File \u2192 Load data onto molecule \u2192 (browse: 1ubq_example.pdb) \u2192 load   Now you should have the model in the display that can be moved around with the mouse.\nThis is the initial starting position of the simulation. Next load in the trajectory data into VMD:   File \u2192 Load data onto molecule \u2192 (browse:1ubq_output.dcd) \u2192 load   This data represents the \u201cmolecular movie\u201d or trajectory of how the atoms in the model moved around during the course of the NAMD simulation. You can play the trajectory by clicking the bottom right arrow of the  main panel . (Use the speed scroll bar to the left of that button to slow it down).  What you are seeing represents the molecular motion of the protein on an extremely small time scale. The NAMD program calculates how the molecule moves over time given certain parameters such as temperature. These models and simulations can give insight into how proteins behave and their role in biological function and certain diseases.  From the  main panel  you can bring up the Representations dialogue\nto play with more rendering types: - try them out!   Graphics \u2192 Representations   And this conclude the basic tutorial to running a simple job on a cluster.\nWasn\u2019t so scary now was it?  Please play around with VMD. Once you feel comfortable, try start the next tutorial: Molecular dynamics - Building input files",
            "title": "a) Reading structure data files into VMD"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/",
            "text": "Molecular Dynamics Tutorial - Intermediate\n\n\n\n\nIn the following tutorials we will be logging on a high performance computer (HPC) to submit \nNAMD\n molecular dynamics (MD) jobs and visualising the results with the molecular visualization program \nVMD\n. As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple.\n\n\n\n\n\n\nMD tutorial - Introduction to cluster computing\n: If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD.\n\n\n\n\n\n\nMD tutorial - Building input files (this tutorial)\n: In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.\n\n\n\n\n\n\nAs we will be working from a terminal on the cluster and later downloading data back to our local desktop for visualization and analysis, we will be assuming that the users have basic knowledge of Unix/Linux. If you are unfamiliar with these, we suggest you first work through this \nUnix tutorial\n.\n\n\n\n\n1 - Overview\n\n\nThe aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in \nthis molecular dynamics tutorial\n.\n\n\n\n\nTip\n: in conjunction with this tutorial there are some excellent \nNAMD tutorials\n available that are worth working through.\n\n\n\n\n2 - NAMD overview\n\n\na) Recap: what we've done so far\n\n\nIn the previous \nbeginners tutorial\n we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution.\n\n\nFor a cluster computing the minimum files we need to run a job are:\n\n\n<filename>.psf\n - \nprotein structure file\n. A list of the atoms, masses, charges and connections between atoms.\n\n\n<filename>.pdb\n - \nprotein database file\n. The actual starting coordinates of the models. This has to be the same order as the psf file.\n\n\n<filename>.conf\n - \nNAMD configuration file\n. Tells NAMD how to run the job.\n\n\npar_all27_prot_na.par\n - \na parameter file\n. (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). Used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms.\n\n\nsbatch_batchfile\n - \na script file\n to launch the job on the cluster depending on the scheduler used. ie) pbs or slurm. Tells the cluster how to run the NAMD job and how many cores to use.\n\n\nIn our \nbeginners tutorial\n all we had to do was launch the job. We will now go through the process of building a NAMD input file from scratch.\n\n\nb) Building NAMD input files overview\n\n\nIn order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated!\n\n\nTo generate NAMD input files, we will use the \npsfgen\n module within VMD, together with pdb and \ntopology files\n, to generate a new pdb file and psf file.\n\n\nIn a flowchart, the process looks something like this:\n\n\n\n\nc) The \nNamd_intermediate_template\n directory structure\n\n\n\n\nNote\n: one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the \nbeginner tutorial\n you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect output.\n\n\nIn the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories.\n\n\n\n\nThis directory is found on snowy, under \n/vlsci/examples/namd/Namd_intermediate_template\n.\n\n\nIt has a particular directory structure as follows:\n\n\n/Namd_intermediate_template\n\n    sbatch_start            \u2190 script to start equilibration phase\n    sbatch_production       \u2190 script to start production phase\n\n    sim_opt.conf            \u2190 Namd configuration file for optimization\n    sim_production.conf     \u2190 Namd configuration file for production run\n\n    project_plan.txt        \u2190 A guide to thinking about your simulation\n    counter.txt             \u2190 File for keeping track of job number\n    max_jobnumber.txt       \u2190 Defines maximum number of jobs\n\n    /BUILD_DIR              \u2190 this is where we will build our models\n\n    /Errors                 \u2190 errors go here\n\n    /Examples               \u2190 find some example files here\n\n    /InputFiles             \u2190 our input files go here\n        /Parameters\n\n    /JobLog                 \u2190 details of our running jobs end up here\n\n    /LastRestart            \u2190 If we need to restart a job\n\n    /OutputFiles            \u2190 Our NAMD output goes here\n\n    /OutputText             \u2190 Text output from the job\n\n    /RestartFiles           \u2190 Generic restart files saved here\n\n\n\n\nRather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching \n(don't do this just yet!)\n:\n\n\nsbatch sbatch_start\n\n\n\n\nThis will launch a job using the \nsim_opt.conf\n configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run: \nsbatch sbatch_production\n\n\nProduction runs are designed to run less than 24 hours at a time, at the end of which \nrestart files\n are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a \ncounter.txt\n file by 1. Once that reaches the number in the \nMaxJobNumber.txt\n file the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we'd set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure.\n\n\nThe script also date stamps and directs the output to the appropriate folders. A complete \ndcd trajectory\n can be reconstructed from the ordered files in the \n/OutputFile\n directory.\n\n\nMore of running a job later. First we need to build input files!\n\n\n3 - Building a HIV protease model\n\n\n\n\nDownload a copy of the \nNamd_intermediate_template\n to your \nlocal computer\n. We will prepare our files here and then upload them to the cluster once ready.\n\n\nscp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template .\n\n\n\n\n\n\nNote\n: don't forget the dot at the end\n\n\n\n\nChange into the build directory:\n\n\ncd Namd_intermediate_template/BUILD_DIR\n\n\n\n\nWe are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the rcsb entry 7hvp. If you are connected to the web, on your \nlocal computer\n start VMD and download this entry.\n\n\nvmd 7hvp\n\n\n\n\nIn this structure there are 3 \u201cchains\u201d. Chain \nA\n and \nB\n are the monomers and chain \nC\n is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective.\n\n\nHighlight the protein selection in the \nVMD main panel\n and then click:\n\n\n\n\nFile \u2192 save coordinates\n\n\n\n\nIn the \u201cselected atoms\u201d box of the save trajectory window type:\n\n\nchain A and protein\n\n\n\n\nSave the chain as a pdb file with a sensible name and a pdb extension into \nBUILD_DIR\n. (e.g. 7hvp_chainA.pdb)\n\n\nRepeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise.\nWe should now have two pdb files in the \n/BUILD_DIR\n which will be the basis of our model building. ie)\n\n\n7hvp_chainA.pdb\n7hvp_chainB.pdb\n\n\n\n\nWe now have to use a text editor to change the \nbuild_example\n script to point to these files. Open the \nbuild_example\n file with a text editor.\n\n\nFirst thing to look for is that we are calling the right \ntopology\n files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids).\n\n\npackage require psfgen\ntopology ../InputFiles/Parameters/top_all27_prot_na.rtf\n\n\n\n\nIf we were to be building a model with a \nlipid bilayer\n we would need to also include the topology file referring to lipids. ie)\n\n\ntopology ../InputFiles/Parameter/top_all27_prot_lipid.rtf\n\n\n\n\nWe now need to change and add \u201csegment\u201d lines to point to our new pdb chains.\n\n\nEdit:\n\n\nsegment A {pdb model_chainA.pdb}\nsegment B {pdb model_chainB.pdb}\n\n\n\n\nto read:\n\n\nsegment A {pdb 7hvp_chainA.pdb}\nsegment B {pdb 7hvp_chainB.pdb}\n\n\n\n\nWe also need to change the \u201ccoordpdb\u201d lines to reflect our new chains.\n\n\nEdit:\n\n\ncoordpdb model_chainA.pdb A\ncoordpdb model_chainB.pdb B\n\n\n\n\nto read:\n\n\ncoordpdb 7hvp_chainA.pdb A\ncoordpdb 7hvp_chainB.pdb B\n\n\n\n\nBetween the 'segment' and 'coordpdb' lines we can apply \npatches\n to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won't make any modifications for this example.\n\n\nSave and close the \nbuild_example\n script. We will now see if we can build the model using VMD from the command line.\n\n\nType:\n\n\nvmd -dispdev text -e build_example          \n\n\n\n\nYou should see some errors.\n\n\nThis is because in the original chains A and B there are some modified alanine residues labeled \nABA\n. Since the residues ABA are not defined in the topology files vmd psfgen does not know how to build this model. Edit the \n7hvcp_chainA.pdb\n and \n7hvp_chainB.pdb\n files and carefully change any occurrence of \nABA\n to \nALA\n.\n\n\n\n\nNote\n: spacing in pdb files is really important so don't mess it up!\n\n\n\n\nRe-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory:\n\n\nmodel_temp_x.psf\nmodel_temp_x.pdb\n\n\n\n\n\n\nNote\n: here we use a \u201c_x\u201d notation to specify temporary files.\n\n\n\n\nNext load these files into VMD. From \nBUILD_DIR\n start vmd:\n\n\nvmd model_temp_x.psf model_temp_x.pdb\n\n\n\n\nWe will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of \n80 x 64 x 64\n \u00c5. Open the solvation window from the main panel:\n\n\n\n\nExtensions \u2192 Modeling \u2192 add solvation box\n\n\n\n\nIn this window do the following:\n\n\n\n\ntoggle on the \u201crotate to minimize volume\u201d button.\n\n\nChange the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d\n\n\nuntoggle the 'use molecular Dimensions\u201d button.\n\n\nIn the Box Size add:\n\n\nmin: x: 40 y: -32 z: -32\n\n\nmax: x: 40 y: 32 z: 32\n\n\n\n\n\n\nclick \u201cSolvate\u201d\n\n\n\n\nWe now should have two new files, \nsolvate.psf\n and \nsolvate.pdb\n, the solvated version of your original input.\n\n\nNow we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window:\n\n\n\n\nExtensions \u2192 Modeling \u2192 add ions\n\n\n\n\nIn the \n\u201cAutoionize\u201d\n window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files: \nionized.psf\n and \nionized.pdb\n.\n\n\nThese are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command:\n\n\nmv ionized.psf ../InputFiles/hiv_protease.psf\n\n\n\n\nand\n\n\nmv ionized.pdb ../InputFiles/hiv_protease.pdb\n\n\n\n\nYou can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy.\n\n\n4 - Preparing the configuration files\n\n\nBy now we have prepared two new input files for a NAMD simulation called \nhiv_protease.psf\n and \nhiv_protease.pdb\n and placed them in the folder \n/InputFiles\n. We now need to make changes to the NAMD configuration files to match our new inputs. At the top of the directory we have two configuration files and two sbatch files:\n\n\nsbatch_start        sim_opt.conf\nsbatch_production   sim_production.conf\n\n\n\n\nLet us first edit the \n.conf\n files. Open the \nsim_opt.conf\n file.\n\n\nMake changes to the lines:\n\n\nstructure   InputFiles/change_me.psf\ncoordinates InputFiles/change_me.pdb\n\n\n\n\nto match our inputs:\n\n\nstructure   InputFiles/hiv_protease.psf\ncoordinates InputFiles/hiv_protease.pdb\n\n\n\n\nThe next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine:\n\n\nparaTypeCharmm  on\nparameters      InputFiles/Parameters/par_all27_prot_na.prm\n\n\n\n\nIf we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example:\n\n\nparameters  InputFiles/Parameters/par_all27_prot_lipid.prm\nparameters  InputFiles/Parameters/par_for_ligand.prm\n\n\n\n\nWe also need to make changes to match our \nPeriodic Boundary Conditions\n (PBC).\n\n\nThe way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn't interact with itself should it wander too close to a boundary.\n\n\nSince our box ended up being of dimensions 80 x 64 x 64 \u00c5 change the lines:\n\n\ncellBasisVector1   96.  0.  0.\ncellBasisVector2    0. 96.  0.\ncellBasisVector3    0.  0. 96.\ncellOrigin          0   0   0\n\n\n\n\nto\n\n\ncellBasisVector1   80.  0.  0.\ncellBasisVector2    0. 64.  0.\ncellBasisVector3    0.  0. 64.\ncellOrigin          0   0   0\n\n\n\n\nThat should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase.\n\n\nThe \nsim_production.conf\n file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file \nMaxJobNumber.txt\n determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time.\n\n\nOpen the \nsim_production.conf\n file:\n\n\nMake the appropriate changes to the lines:\n\n\nstructure   InputFiles/change_me.psf\ncoordinates InputFiles/change_me.pdb\n\n\n\n\nSince we would like to run a relatively short job segment for this exercise, we will change the line:\n\n\nset NumberSteps 2500000\n\n\n\n\nto:\n\n\nset NumberSteps 20000\n\n\n\n\nThis segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation.\n\n\nIn another terminal edit the \ncounter.txt\n file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use:\n\n\necho 0 > counter.txt\n\n\n\n\nIn another terminal edit the \nMaxJobNumber.txt\n file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value.\n\n\necho 5 > MaxJobNumber.txt\n\n\n\n\nWe can always make this number bigger later and restart the jobs if we want things to go longer.\n\n\nSince we might want to see a bit more data in this short example we will also change more lines in the \nsim_production.conf\n file:\n\n\nrestartfreq     250000\ndcdfreq         50000\nxstFreq         50000\noutputEnergies  50000\noutputPressure  50000\noutputTiming    50000\n\n\n\n\nto:\n\n\nrestartfreq     25000\ndcdfreq         5000\nxstFreq         5000\noutputEnergies  5000\noutputPressure  5000\noutputTiming    5000\n\n\n\n\nWe don't have to change the periodic boundary conditions in the \nsim_production.conf\n file as we read in the restart files from the previous simulation namely:\n\n\nset inputname generic_restartfile\nbinCoordinates $inputname.coor  ; # Coordinates from last run (binary)\nbinVelocities  $inputname.vel   ; # Velocities from last run (binary)\nextendedSystem $inputname.xsc   ; # Cell dimensions from last run\n\n\n\n\nThere are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don't change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values.\n\n\nSave and close your .conf files.\n\n\nA look at the sbatch scripts\n\n\nThe sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated to the ones we saw in the \nbeginners tutorial\n, but most of the details you need to worry about are all in the first few lines.\n\n\nIn a sbatch script we need to pass arguments to the slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line:\n\n\n#SBATCH \u2013nodes=2\n      \u2190 this works!\n\n\n# SBATCH -nodes=2\n     \u2190 this doesn't because of the space between \u201c#\u201d and \u201cS\u201d\n\n\n\n\nNote\n: PBS scripts work in a similar way, but with the code word \u201c\n#PBS\n\u201d )\n\n\nNote\n: people often get confused with this as the \u201c\n#\n\u201d symbol usually denotes a comment line.\n\n\n\n\nTo set the number of nodes used for a job set:\n\n\n#SBATCH \u2013nodes=4\n\n\n\n\nRemember, more nodes is not necessarily faster and can be dramatically slower!\n It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine.\n\n\nTo set the job runtime change this line:\n\n\n#SBATCH \u2013time=2:0:0\n         \u2190 (hours:minutes:seconds)\n\n\nThe time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus %10.  If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster. (but make sure your walltime is appropriate for your configuration file!)\n\n\n5 - Launching the job on the cluster\n\n\nWe are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make.\n\n\nUpload the entire directory to your account. Under Linux this might be:\n\n\nscp -r Namd_intermediate_template <username>@snowy.vlsci.unimelb.edu.au:\n\n\n\n\nLog into your account on \nsnowy\n and change into the top of the directory.\nLaunch the start script:\n\n\nsbatch sbatch_start\n\n\n\n\nThis should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase. \nThis might take an hour or two to complete\n.\n\n\nAll text output is directed to the \n/OutputText\n folder. You can take a peek at how your job is going by using the command \u201ctail \n\u201d which prints out the last few lines of \n.\n\n\nFor the purpose of this exercise\n, we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file\nIt will look something like this:\n\n\nslurm-123456.out\n\n\n\n\nThe number represents the job number on the cluster. Now use \nscancel\n to stop that job (i.e. for above you would use: \nscancel 123456\n).\n\n\nscancel <jobnumber>\n\n\n\n\nNow that your job has finished, we will copy across a completed job run.\nFrom your top directory on snowy: (this should be all on one line:)\n\n\ncp -r /vlsci/examples/namd/Namd_intermediate_template_finished/* Namd_intermediate_template/\n\n\n\n\nOnce the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our desktop for analysis. If you don't have much memory on your laptop, you can do the analysis remotely on the cluster.\n\n\nA smart way to copy files back to your local computer is to use \nrsync\n. This way you only copy new or changed files back to your computer. In Linux from the desktop this would be:\n\n\nrsync -avzt <username>@snowy.vlsci.unimelb.edu.au:Namd_intermediate_template .\n\n\n\n\n\n\nNote\n: the dot is important!\n\n\n\n\nNow that you have your data, we are ready to visualize the results,.....\n\n\n6 - Visualization of the MD trajectory\n\n\nHopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local laptop. We will now have a look at the data you generated.\n\n\na) Combining the trajectory files\n\n\nWhen we run segmented jobs as in this template, we end up with a series of output files in \n/OutputFiles\n such as:\n\n\n[train01@snowy OutputFiles]$ ls -lrt  \ntotal 13184\n-rwxr-xr-x 1 train01 TRAINING    1477 Mar 21 10:18 create_dcd_loader_script\n-rw-r--r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd\n-rw-r--r-- 1 train01 TRAINING     195 Mar 21 11:15 dcd_list.txt\n-rw-r--r-- 1 train01 TRAINING     742 Mar 21 11:15 combined_dcd_file_loader.vmd\n\n\n\n\nThe main output files have the \n.dcd\n extension. We can see that things went well as the sizes of these files are identical as expected.\n\n\nIf you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run:\n\n\n./create_dcd_loader_script\n\n\n\n\nThis creates the file: \ncombined_dcd_file_loader.vmd\n\n\nFrom the main directory we can load in our trajectory using:\n\n\nvmd InputFiles/hiv_protease.psf InputFiles/hiv_protease.pdb\n\n\n\n\nthen from the main panel:\n\n\n\n\nFile \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd\n\n\n\n\nIt is possible to restart the simulations of any segment as the restart files are saved under \n/RestartFiles\n.\n\n\n7 - Looking at longer simulations\n\n\nThe previous examples have not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult.\n\n\nLuckily, there is an easy way to center and visualize our simulations which we will cover next.\n\n\na) Copy across extended files to your local machine\n\n\nWe have prepared some HIV simulation files that ran for a total of 20 nanoseconds, (which is still quite short, but long enough to show off the some of the techniques.)\n\n\nCopy to your desktop the precomputed data in the Snowy folder: \n/vlsci/examples/namd/Namd_intermediate_template_finished\n\n\nscp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template_finished .\n\n\n\n\nIn the \n/OutputFiles\n it has additional data which ran for a longer time, 10 x 2 ns simulations, with data points every 100 ps. The data has the extension \nexample.namd_job_extended_run_1.X.dcd\n etc...\n\n\nStart VMD and load in your starting psf and pdb HIV protease files. (We will come to why we used the starting files shortly). Next, load up the 10 new trajectory data files onto the model in VMD. i.e.:\n\n\n\n\nFile \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.9.dcd\n\n\nFile \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.8.dcd\n\n\n...\n\n\nFile \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.0.dcd\n\n\n\n\nNow display only the protein backbone. You will notice the protein jiggles around quite when you play the trajectory. This is Brownian motion, now more prominent due to longer sampling.\n\n\nThe first thing we might try to ease the jiggling is to increase the \ntrajectory smoothing window size\n. In the VMD Graphical representations window, select your protein representation and toggle the \nTrajectory\n tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values.\n\n\nAlthough this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box.\n\n\nWe will now use the \nRMSD Trajectory Tool\n to center our protein frames.\n\n\nIn the VMD main panel, open:\n\n\n\n\nExtensions \u2192 Analysis \u2192 RMSD Trajectory Tool\n\n\n\n\nThis should open up a new window. Towards the top left we have the selection, for the 'Selection modifiers' tick 'Backbone'. In the top right, click \n\u201cRMSD\u201d\n.\n\n\nWhen you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much you selection drifts through space. At this point nothing has changed in the trajectory yet.\n\n\nNext, click the \n\u201cALIGN\u201d\n button. This will translate each frame to minimize the RMSD of the selection based on the first frame, (In this case, our original input files).\n\n\nIn other word, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis.\nClick \n\u201cRMSD\u201d\n again and you'll see the value becomes much smaller.\n\n\nYou may notice that the loop regions (residues 45 to 55) of the HIV protease dimer almost come apart in the longer simulations. This is an important part of the HIV protease kinetics, as the loops need to open for the substrate to enter the active site. We are getting a glimpse here of proteins in action.\n\n\nb) Using Volmap to map ligand density.\n\n\nNow that we have a nicely centered protein data set we can do something useful like plot the water density. In the VMD main panel, open:\n\n\n\n\nExtensions \u2192 Analysis \u2192 VolMap Tool.\n\n\n\n\nA new VolMap window should open up.\n\n\nIn the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d.\n\n\nThis will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical selection window and select the new \n\u201cIsosurface\u201d\n representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points.\n\n\nWhat you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don't display waters in simulations for clarity, and often forget that they are there.\n\n\nIf all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation.\n\n\n\n\n\n\nExercise\n: see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail.\n\n\n\n\nYou can also do this sort of view for ligands too, to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense!\n\n\n8 - Including a non-standard ligand\n\n\nThe difficulty with simulating new ligands in molecular dynamics simulations is about finding the right parameter files. Your simulation is only as good as the parameters they use, so it is worthwhile trying to validate them. Most of the common residues, such as amino acids, nucleotides and glycosides have decent parameters for the main MD codes, but finding suitable parameters for new drug molecules can be arduous.\n\n\nFor this exercise, we won't go to excessive trouble. We are going to add a drug molecule Amprenavir (APV), a protease inhibitor, into the binding site of our HIV protease using parameters from a website service \nSwissParam\n.\n\n\nFirst of all we need a representative drug molecule for APV and convert it to a mol2 format. This is just a different molecular format to pdb. An easy way to do this is use the program \nChimera\n (similar to VMD).\n\n\nThe pdb structure \n3NU3\n provides a high resolution structure of a wild type HIV protease with APV. You will see there are two APV molecules in the model plus alternative side chain structures. This is the result of crystallography with high resolution data on a symmetrical dimer.\n\n\nIn the \nNamd_intermediate_template_finished\n build directory, we have an edited version of the\n3nu3 pdb file, taking out the artifacts. From this we can extract chain A, B of the protease and the APV as separate molecules. The APV does not have hydrogens attached so these need to be added and a mol2 version made if we are to use SwissParam.\n\n\nThis can be intuitively done with the Chimera program. (We won't cover all the details here, but it shouldn't be too much trouble. Extract and save a pdb version of the APV molecule from the edited 3nu3 pdb file, - (in this case edited to chain C). Open the APV pdb file in Chimera, add hydrogens and save as a mol2 file.\n\n\nIf you submit the APV.mol2 file to the \nSwissParam\n website, you will receive a lig.zip file containing the following files:\n\n\nlig.crd\nlig.itp\nlig.mol2\nlig.par         \u2190 this is the parameter file to run the APV ligand simulation\nlig.pdb         \u2190 we need this pdb file to build the APV in the model\nlig.psf\nlig.rtf         \u2190 we need this topology file to build the model\n\n\n\n\nThis is stored in the \n/BUILD_DIR/lig\n folder.\n\n\nIn the build script \nbuild_hiv_3nu3_APV\n there are some changes to include the new structure such as the lines:\n\n\ntopology  lig/lig.rtf           \nsegment   C {pdb lig/lig.pdb}\ncoordpdb  lig/lig.pdb C\n\n\n\n\nThe model has been already built, solvated and ionized, and stored under: \n/InputFiles/hiv_prot_APV.psf\n\n\nChanges also have to be made to the configuration files to include the new parameters for APV.\nIn the configuration file \nsim_opt_3nu3_APV.conf\n we can see the added line:\n\n\nparameters InputFiles/Parameters/lig.par\n\n\n\n\nThis is to tell NAMD how to treat the new APV molecule in the simulation.\n\n\nA small sample of data is already precomputed and stored under \n/OutputFiles/\n as \nhiv_prot_APV.dcd\n. You can view this using VMD, with the \n/InputFiles/hiv_prot_APV.psf\n file.\n\n\n\n\nExercise\n: With the simulation data, try to visualize the APV alone. Does the molecule behave as you might expect? Do the charges on the atoms seem reasonable? Look up some mutations associated with Amprenavir resistance. Does this make sense in the model?\n\n\n\n\nSo concludes the intermediate tutorial. A more advanced workflow called \nMD_workflow_py\n was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.",
            "title": "Molecular Dynamics - Building input files"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#molecular-dynamics-tutorial-intermediate",
            "text": "In the following tutorials we will be logging on a high performance computer (HPC) to submit  NAMD  molecular dynamics (MD) jobs and visualising the results with the molecular visualization program  VMD . As students have a vast range of skill levels and requirements, the tutorials are divided into two parts described below. Each tutorial is designed to be stand alone, start where you feel most comfortable and skip the exercises that are too simple.    MD tutorial - Introduction to cluster computing : If you have never launched a job on a cluster before, this tutorial is for you. Here we will simply copy across a bare minimum directory containing input files ready to go for a short NAMD simulation. Once the job is finished, we will download the data to your local computer and visualize the trajectory with VMD.    MD tutorial - Building input files (this tutorial) : In this tutorial we will be introducing a more sophisticated directory structure using scripts to direct the output and run the job. We initially download a copy of the directory to our local computer where we then build up the input files using VMD. We then upload this directory to the cluster where we submit the job. Finally we will download the results back to our local computer to visualize with VMD.    As we will be working from a terminal on the cluster and later downloading data back to our local desktop for visualization and analysis, we will be assuming that the users have basic knowledge of Unix/Linux. If you are unfamiliar with these, we suggest you first work through this  Unix tutorial .",
            "title": "Molecular Dynamics Tutorial - Intermediate"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#1-overview",
            "text": "The aim of this tutorial is to give more advanced lessons in setting up and preparing molecular dynamics jobs for submission to the HPC cluster. It is assumed that the user has a basic command of visualization programs such as VMD and has had at least some experience launching and retrieving example tutorial jobs to the cluster, as shown in  this molecular dynamics tutorial .   Tip : in conjunction with this tutorial there are some excellent  NAMD tutorials  available that are worth working through.",
            "title": "1 - Overview"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#2-namd-overview",
            "text": "",
            "title": "2 - NAMD overview"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-recap-what-weve-done-so-far",
            "text": "In the previous  beginners tutorial  we simply launched a short job on the cluster that already had all the required input files to simulate a ubiquitin protein molecule in solution.  For a cluster computing the minimum files we need to run a job are:  <filename>.psf  -  protein structure file . A list of the atoms, masses, charges and connections between atoms.  <filename>.pdb  -  protein database file . The actual starting coordinates of the models. This has to be the same order as the psf file.  <filename>.conf  -  NAMD configuration file . Tells NAMD how to run the job.  par_all27_prot_na.par  -  a parameter file . (there are different types of these depending on the classes of molecules you have in your model, such as lipids or DNA). Used by NAMD to run the simulation, basically a list of all the bonds, angles, dihedrals, improper and VdW terms.  sbatch_batchfile  -  a script file  to launch the job on the cluster depending on the scheduler used. ie) pbs or slurm. Tells the cluster how to run the NAMD job and how many cores to use.  In our  beginners tutorial  all we had to do was launch the job. We will now go through the process of building a NAMD input file from scratch.",
            "title": "a) Recap: what we've done so far"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-building-namd-input-files-overview",
            "text": "In order to build input files for NAMD, we will need a pdb file giving the initial coordinates. We usually get these files from the pdb database. These initial pdb files however are often not ideal, with missing residues and odd chain names making the process more complicated!  To generate NAMD input files, we will use the  psfgen  module within VMD, together with pdb and  topology files , to generate a new pdb file and psf file.  In a flowchart, the process looks something like this:",
            "title": "b) Building NAMD input files overview"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#c-the-namd_intermediate_template-directory-structure",
            "text": "Note : one problem with running molecular dynamics simulations is that you can very quickly build up a mess of output files with the resulting directory becoming disorganized and difficult to navigate. (If you ran the  beginner tutorial  you may have noticed a lot more files at the end of the run with no particular order!). One solution to this is to have specific directories for certain tasks and using scripts to redirect output.  In the next exercise we will download a template directory within which we will build our model and setup our simulation. We will use more sophisticated scripts to launch and manage our jobs that will direct output to appropriate directories.   This directory is found on snowy, under  /vlsci/examples/namd/Namd_intermediate_template .  It has a particular directory structure as follows:  /Namd_intermediate_template\n\n    sbatch_start            \u2190 script to start equilibration phase\n    sbatch_production       \u2190 script to start production phase\n\n    sim_opt.conf            \u2190 Namd configuration file for optimization\n    sim_production.conf     \u2190 Namd configuration file for production run\n\n    project_plan.txt        \u2190 A guide to thinking about your simulation\n    counter.txt             \u2190 File for keeping track of job number\n    max_jobnumber.txt       \u2190 Defines maximum number of jobs\n\n    /BUILD_DIR              \u2190 this is where we will build our models\n\n    /Errors                 \u2190 errors go here\n\n    /Examples               \u2190 find some example files here\n\n    /InputFiles             \u2190 our input files go here\n        /Parameters\n\n    /JobLog                 \u2190 details of our running jobs end up here\n\n    /LastRestart            \u2190 If we need to restart a job\n\n    /OutputFiles            \u2190 Our NAMD output goes here\n\n    /OutputText             \u2190 Text output from the job\n\n    /RestartFiles           \u2190 Generic restart files saved here  Rather than running one long job, this template is designed to run into smaller stages. After we build a model and make the appropriate changes to the input files the run is started by launching  (don't do this just yet!) :  sbatch sbatch_start  This will launch a job using the  sim_opt.conf  configuration file which is designed to equilibrate the system. At the conclusion of the equilibration run, the script will automatically launch the production run:  sbatch sbatch_production  Production runs are designed to run less than 24 hours at a time, at the end of which  restart files  are saved and relaunch themselves for another 24 hour job. Every time they do, the script increases a  counter.txt  file by 1. Once that reaches the number in the  MaxJobNumber.txt  file the job stops. For example, if a 24 hour job completes 5 nanoseconds of simulation a day, and we want a contiguous simulation of say 50 ns, then we'd set the counter.txt file to 0 and the MaxJobNumber.txt to 10 and thus would expect the job to finish in 10 days. The advantage of running a series of smaller jobs rather than one long one is better scheduling on the cluster, and also better protection from data corruption should there be a hardware failure.  The script also date stamps and directs the output to the appropriate folders. A complete  dcd trajectory  can be reconstructed from the ordered files in the  /OutputFile  directory.  More of running a job later. First we need to build input files!",
            "title": "c) The Namd_intermediate_template directory structure"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#3-building-a-hiv-protease-model",
            "text": "Download a copy of the  Namd_intermediate_template  to your  local computer . We will prepare our files here and then upload them to the cluster once ready.  scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template .   Note : don't forget the dot at the end   Change into the build directory:  cd Namd_intermediate_template/BUILD_DIR  We are going to build a model of HIV protease which is a dimer of two 99 amino acids protein monomers. First we need to extract the two chains as separate pdb files. We will use the rcsb entry 7hvp. If you are connected to the web, on your  local computer  start VMD and download this entry.  vmd 7hvp  In this structure there are 3 \u201cchains\u201d. Chain  A  and  B  are the monomers and chain  C  is the inhibitor PRD_000228. Since there are water molecules associated with each chain selection we need to be more selective.  Highlight the protein selection in the  VMD main panel  and then click:   File \u2192 save coordinates   In the \u201cselected atoms\u201d box of the save trajectory window type:  chain A and protein  Save the chain as a pdb file with a sensible name and a pdb extension into  BUILD_DIR . (e.g. 7hvp_chainA.pdb)  Repeat for chain B. Since the inhibitor is complicated we will leave chain C out for now for this exercise.\nWe should now have two pdb files in the  /BUILD_DIR  which will be the basis of our model building. ie)  7hvp_chainA.pdb\n7hvp_chainB.pdb  We now have to use a text editor to change the  build_example  script to point to these files. Open the  build_example  file with a text editor.  First thing to look for is that we are calling the right  topology  files. Since this example is only made from protein then the default topology file is fine (The _na part refers to nucleic acids).  package require psfgen\ntopology ../InputFiles/Parameters/top_all27_prot_na.rtf  If we were to be building a model with a  lipid bilayer  we would need to also include the topology file referring to lipids. ie)  topology ../InputFiles/Parameter/top_all27_prot_lipid.rtf  We now need to change and add \u201csegment\u201d lines to point to our new pdb chains.  Edit:  segment A {pdb model_chainA.pdb}\nsegment B {pdb model_chainB.pdb}  to read:  segment A {pdb 7hvp_chainA.pdb}\nsegment B {pdb 7hvp_chainB.pdb}  We also need to change the \u201ccoordpdb\u201d lines to reflect our new chains.  Edit:  coordpdb model_chainA.pdb A\ncoordpdb model_chainB.pdb B  to read:  coordpdb 7hvp_chainA.pdb A\ncoordpdb 7hvp_chainB.pdb B  Between the 'segment' and 'coordpdb' lines we can apply  patches  to make modifications to the protein. This is useful for example when adding disulphide bonds or phosphate groups or modifying the termini. We won't make any modifications for this example.  Save and close the  build_example  script. We will now see if we can build the model using VMD from the command line.  Type:  vmd -dispdev text -e build_example            You should see some errors.  This is because in the original chains A and B there are some modified alanine residues labeled  ABA . Since the residues ABA are not defined in the topology files vmd psfgen does not know how to build this model. Edit the  7hvcp_chainA.pdb  and  7hvp_chainB.pdb  files and carefully change any occurrence of  ABA  to  ALA .   Note : spacing in pdb files is really important so don't mess it up!   Re-run the above command. This should run vmd in text mode and read through the build script. If all goes well we should see 2 new files in the directory:  model_temp_x.psf\nmodel_temp_x.pdb   Note : here we use a \u201c_x\u201d notation to specify temporary files.   Next load these files into VMD. From  BUILD_DIR  start vmd:  vmd model_temp_x.psf model_temp_x.pdb  We will now use the solvation module to center and create a solvent box around our protein. We will use dimensions of  80 x 64 x 64  \u00c5. Open the solvation window from the main panel:   Extensions \u2192 Modeling \u2192 add solvation box   In this window do the following:   toggle on the \u201crotate to minimize volume\u201d button.  Change the Boundary number from \u201c2.4\u201d to \u201c1.8\u201d  untoggle the 'use molecular Dimensions\u201d button.  In the Box Size add:  min: x: 40 y: -32 z: -32  max: x: 40 y: 32 z: 32    click \u201cSolvate\u201d   We now should have two new files,  solvate.psf  and  solvate.pdb , the solvated version of your original input.  Now we can jump straight to adding ions. Adding ions makes the simulation more physiologically relevant and also balances charges. Open the ionization window:   Extensions \u2192 Modeling \u2192 add ions   In the  \u201cAutoionize\u201d  window simply toggle the \u201cneutralize and set NaCl concentration\u201d option and click \u201cAutoionize\u201d. We should get about 26 sodium ions and 30 chloride ions added to the system in two new files:  ionized.psf  and  ionized.pdb .  These are are final input files. We should now rename and move these files to a better location in the directory structure. In Linux we can use the command:  mv ionized.psf ../InputFiles/hiv_protease.psf  and  mv ionized.pdb ../InputFiles/hiv_protease.pdb  You can also now remove the old \u201csolvate\u201d and \u201c_x\u201d files to keep things tidy.",
            "title": "3 - Building a HIV protease model"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#4-preparing-the-configuration-files",
            "text": "By now we have prepared two new input files for a NAMD simulation called  hiv_protease.psf  and  hiv_protease.pdb  and placed them in the folder  /InputFiles . We now need to make changes to the NAMD configuration files to match our new inputs. At the top of the directory we have two configuration files and two sbatch files:  sbatch_start        sim_opt.conf\nsbatch_production   sim_production.conf  Let us first edit the  .conf  files. Open the  sim_opt.conf  file.  Make changes to the lines:  structure   InputFiles/change_me.psf\ncoordinates InputFiles/change_me.pdb  to match our inputs:  structure   InputFiles/hiv_protease.psf\ncoordinates InputFiles/hiv_protease.pdb  The next thing we have to watch is that we have an appropriate parameter file pointed to. Since we have a simple protein model the default parameter file should be fine:  paraTypeCharmm  on\nparameters      InputFiles/Parameters/par_all27_prot_na.prm  If we were running lipids in the simulation or used a separately parameterized ligand we would add a few extra lines here, say for example:  parameters  InputFiles/Parameters/par_all27_prot_lipid.prm\nparameters  InputFiles/Parameters/par_for_ligand.prm  We also need to make changes to match our  Periodic Boundary Conditions  (PBC).  The way PBC works is that our simulation box has a certain rectangular geometry which is mirrored on all sides to mimic an infinite system. A molecule which falls out of the left and side of the box fall back in on the right hand side. A molecule that falls out of the bottom reappears at the top and so on. Care has to be given when building a solvated PBC system so that a protein molecule has enough room around its edges so that it doesn't interact with itself should it wander too close to a boundary.  Since our box ended up being of dimensions 80 x 64 x 64 \u00c5 change the lines:  cellBasisVector1   96.  0.  0.\ncellBasisVector2    0. 96.  0.\ncellBasisVector3    0.  0. 96.\ncellOrigin          0   0   0  to  cellBasisVector1   80.  0.  0.\ncellBasisVector2    0. 64.  0.\ncellBasisVector3    0.  0. 64.\ncellOrigin          0   0   0  That should do it for the optimization configuration file. The idea of the optimization phase is to equilibrate your simulation and allow it to settle before running production runs. Typically we run this with a NPT ensemble, (whereby the number of particles (N) pressure (P) and temperature (T) remain constant). The volume is allowed to change as water and lipids can find their optimal density. At the end of the optimization phase, the sbatch_start script takes the generic output of the run and renames it to pass on to the production phase of the simulation. The sbatch_script will automatically launch the production phase on successful completion of the optimization phase.  The  sim_production.conf  file controls how we run a segment of the production phase including the type of simulation and how long it runs. The file  MaxJobNumber.txt  determines how many times this production script is run. Each time a production segment finishes, output is date-stamped and placed in appropriate folders, while restart files are written, ready to launch the next segment. The number in the file counter.txt is increased by 1 to keep track of the jobs. Typically we aim to keep these segments running less than 24 hours at a time. This has a number of advantages, firstly, if the machine should have a failure the most you will loose is one days simulation which is easy to recover from the last restart file. Secondly, this strategy helps with efficiently filling the machine with jobs and keeping your groups quota available. Long jobs scheduled for weeks can tie up resources, quota and risk losing all the generated data should there be a failure in that time.  Open the  sim_production.conf  file:  Make the appropriate changes to the lines:  structure   InputFiles/change_me.psf\ncoordinates InputFiles/change_me.pdb  Since we would like to run a relatively short job segment for this exercise, we will change the line:  set NumberSteps 2500000  to:  set NumberSteps 20000  This segment will run for only 20,000 x 2 fs = 0.04 ns at a time. For example, if we set max_jobnumber.txt to be 5 then we will should get 0.2 ns worth of simulation.  In another terminal edit the  counter.txt  file to contain the number 0. (The counter file is a way of running a certain number of jobs. This increments up after each job finished until it reaches the same as MaxJobNumber.txt and then stops). In linux we can simply use:  echo 0 > counter.txt  In another terminal edit the  MaxJobNumber.txt  file to contain the number 5. This will limit the total number of jobs run. Jobs will stop when counter.txt value is equal to or greater than the MaxJobNumber.txt value.  echo 5 > MaxJobNumber.txt  We can always make this number bigger later and restart the jobs if we want things to go longer.  Since we might want to see a bit more data in this short example we will also change more lines in the  sim_production.conf  file:  restartfreq     250000\ndcdfreq         50000\nxstFreq         50000\noutputEnergies  50000\noutputPressure  50000\noutputTiming    50000  to:  restartfreq     25000\ndcdfreq         5000\nxstFreq         5000\noutputEnergies  5000\noutputPressure  5000\noutputTiming    5000  We don't have to change the periodic boundary conditions in the  sim_production.conf  file as we read in the restart files from the previous simulation namely:  set inputname generic_restartfile\nbinCoordinates $inputname.coor  ; # Coordinates from last run (binary)\nbinVelocities  $inputname.vel   ; # Velocities from last run (binary)\nextendedSystem $inputname.xsc   ; # Cell dimensions from last run  There are a number of other control parameters in the production configuration script worth taking a look at including such things as cutoffs and temperature controls if you have time. These don't change much typically between simulations, but are covered better in the online NAMD tutorials and manuals. The setting we use here are reasonable default values.  Save and close your .conf files.  A look at the sbatch scripts  The sbatch scripts tell the cluster how to run the simulation and how to handle the generated data. These scripts are a lot more complicated to the ones we saw in the  beginners tutorial , but most of the details you need to worry about are all in the first few lines.  In a sbatch script we need to pass arguments to the slurm scheduler (the controlling program which launches users jobs to the cluster). The way to do so is use a complete code word \u201c#SBATCH\u201d on the first spaces of a line:  #SBATCH \u2013nodes=2       \u2190 this works!  # SBATCH -nodes=2      \u2190 this doesn't because of the space between \u201c#\u201d and \u201cS\u201d   Note : PBS scripts work in a similar way, but with the code word \u201c #PBS \u201d )  Note : people often get confused with this as the \u201c # \u201d symbol usually denotes a comment line.   To set the number of nodes used for a job set:  #SBATCH \u2013nodes=4  Remember, more nodes is not necessarily faster and can be dramatically slower!  It can be a good way to quickly burn up your quota inefficiently. It is a good idea to benchmark your jobs to find an optimal sweet spot, - more of how to do that another time. 4 nodes for this example will be fine.  To set the job runtime change this line:  #SBATCH \u2013time=2:0:0          \u2190 (hours:minutes:seconds)  The time or \u201cwalltime\u201d tells the cluster how long your job should run. If your job runs longer than this, it will be stopped. As rule of thumb, use the time you expect plus %10.  If you use an excessively long walltime such as days or weeks, the scheduler may take a long time to fit your job into the queue. Generally shorter jobs will get on the cluster faster. (but make sure your walltime is appropriate for your configuration file!)",
            "title": "4 - Preparing the configuration files"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#5-launching-the-job-on-the-cluster",
            "text": "We are now ready to go ahead with launching the job. For convenience I prefer to keep everything associated with the simulation together in the one directory in including all the build scripts and parameter files. The size of the extra files is tiny compared to the data you generate but this way you can ensure to totally replicate and finding that you happen to make.  Upload the entire directory to your account. Under Linux this might be:  scp -r Namd_intermediate_template <username>@snowy.vlsci.unimelb.edu.au:  Log into your account on  snowy  and change into the top of the directory.\nLaunch the start script:  sbatch sbatch_start  This should launch the equilibration phase of the simulation. As mentioned previously, the sbatch_script will automatically direct the output to the proper directories and launch the production phase.  This might take an hour or two to complete .  All text output is directed to the  /OutputText  folder. You can take a peek at how your job is going by using the command \u201ctail  \u201d which prints out the last few lines of  .  For the purpose of this exercise , we will stop the job early and copy across a pre-computed data set. In your directory you should see slurm output file\nIt will look something like this:  slurm-123456.out  The number represents the job number on the cluster. Now use  scancel  to stop that job (i.e. for above you would use:  scancel 123456 ).  scancel <jobnumber>  Now that your job has finished, we will copy across a completed job run.\nFrom your top directory on snowy: (this should be all on one line:)  cp -r /vlsci/examples/namd/Namd_intermediate_template_finished/* Namd_intermediate_template/  Once the jobs are finished (or you have stopped the jobs and copied across the precomputed data), we can download the entire directory back to our desktop for analysis. If you don't have much memory on your laptop, you can do the analysis remotely on the cluster.  A smart way to copy files back to your local computer is to use  rsync . This way you only copy new or changed files back to your computer. In Linux from the desktop this would be:  rsync -avzt <username>@snowy.vlsci.unimelb.edu.au:Namd_intermediate_template .   Note : the dot is important!   Now that you have your data, we are ready to visualize the results,.....",
            "title": "5 - Launching the job on the cluster"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#6-visualization-of-the-md-trajectory",
            "text": "Hopefully by now you have successfully built a model, completed a small run with the template directory on the cluster and downloaded the results on to your local laptop. We will now have a look at the data you generated.",
            "title": "6 - Visualization of the MD trajectory"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-combining-the-trajectory-files",
            "text": "When we run segmented jobs as in this template, we end up with a series of output files in  /OutputFiles  such as:  [train01@snowy OutputFiles]$ ls -lrt  \ntotal 13184\n-rwxr-xr-x 1 train01 TRAINING    1477 Mar 21 10:18 create_dcd_loader_script\n-rw-r--r-- 1 train01 TRAINING 4090120 Mar 21 10:45 opt.2017-03-21-10.19.sim_opt_r1.dcd.x\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:51 2017-03-21-10.45.NamdJob_round01.0.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 10:57 2017-03-21-10.51.NamdJob_round01.1.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:03 2017-03-21-10.57.NamdJob_round01.2.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:09 2017-03-21-11.03.NamdJob_round01.3.dcd\n-rw-r--r-- 1 train01 TRAINING 1859296 Mar 21 11:15 2017-03-21-11.09.NamdJob_round01.4.dcd\n-rw-r--r-- 1 train01 TRAINING     195 Mar 21 11:15 dcd_list.txt\n-rw-r--r-- 1 train01 TRAINING     742 Mar 21 11:15 combined_dcd_file_loader.vmd  The main output files have the  .dcd  extension. We can see that things went well as the sizes of these files are identical as expected.  If you have a lot of these files, it can be tedious to read them into VMD. Luckily we can run a script from this directory (you can do this on the cluster if you are running Windows on your local computer) to generate a list of this output to be read in by VMD. Simply run:  ./create_dcd_loader_script  This creates the file:  combined_dcd_file_loader.vmd  From the main directory we can load in our trajectory using:  vmd InputFiles/hiv_protease.psf InputFiles/hiv_protease.pdb  then from the main panel:   File \u2192 Load Visualization State \u2192 /OutputFiles/combined_dcd_file_loader.vmd   It is possible to restart the simulations of any segment as the restart files are saved under  /RestartFiles .",
            "title": "a) Combining the trajectory files"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#7-looking-at-longer-simulations",
            "text": "The previous examples have not run for a particularly long period of time, barely a few hundred picoseconds, thus the relative movement in the molecule is small. For simulations that run on longer timescales there will be an amount of drifting making the analysis and visualization difficult.  Luckily, there is an easy way to center and visualize our simulations which we will cover next.",
            "title": "7 - Looking at longer simulations"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#a-copy-across-extended-files-to-your-local-machine",
            "text": "We have prepared some HIV simulation files that ran for a total of 20 nanoseconds, (which is still quite short, but long enough to show off the some of the techniques.)  Copy to your desktop the precomputed data in the Snowy folder:  /vlsci/examples/namd/Namd_intermediate_template_finished  scp -r <username>@snowy.vlsci.unimelb.edu.au:/vlsci/examples/namd/Namd_intermediate_template_finished .  In the  /OutputFiles  it has additional data which ran for a longer time, 10 x 2 ns simulations, with data points every 100 ps. The data has the extension  example.namd_job_extended_run_1.X.dcd  etc...  Start VMD and load in your starting psf and pdb HIV protease files. (We will come to why we used the starting files shortly). Next, load up the 10 new trajectory data files onto the model in VMD. i.e.:   File \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.9.dcd  File \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.8.dcd  ...  File \u2192 Load Data Into Molecule \u2192 example.namd_job_extended_run_1.0.dcd   Now display only the protein backbone. You will notice the protein jiggles around quite when you play the trajectory. This is Brownian motion, now more prominent due to longer sampling.  The first thing we might try to ease the jiggling is to increase the  trajectory smoothing window size . In the VMD Graphical representations window, select your protein representation and toggle the  Trajectory  tab. At the bottom of the tab, increase the Trajectory Smoothing value to 5 and replay the trajectory. This should be much smoother. Try increasing to higher values.  Although this view is smoother, it still can be difficult to visualize what relative motion is going on, due to the motion of the protein in the simulation box.  We will now use the  RMSD Trajectory Tool  to center our protein frames.  In the VMD main panel, open:   Extensions \u2192 Analysis \u2192 RMSD Trajectory Tool   This should open up a new window. Towards the top left we have the selection, for the 'Selection modifiers' tick 'Backbone'. In the top right, click  \u201cRMSD\u201d .  When you do so, it will calculate the RMSD, of the protein backbone over the course of the simulation. The average value can be quite large depending on how much you selection drifts through space. At this point nothing has changed in the trajectory yet.  Next, click the  \u201cALIGN\u201d  button. This will translate each frame to minimize the RMSD of the selection based on the first frame, (In this case, our original input files).  In other word, the protein has been centered on a reference frame, but now the water box appears to rotate about the axis. This makes it much more useful for analysis.\nClick  \u201cRMSD\u201d  again and you'll see the value becomes much smaller.  You may notice that the loop regions (residues 45 to 55) of the HIV protease dimer almost come apart in the longer simulations. This is an important part of the HIV protease kinetics, as the loops need to open for the substrate to enter the active site. We are getting a glimpse here of proteins in action.",
            "title": "a) Copy across extended files to your local machine"
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#b-using-volmap-to-map-ligand-density",
            "text": "Now that we have a nicely centered protein data set we can do something useful like plot the water density. In the VMD main panel, open:   Extensions \u2192 Analysis \u2192 VolMap Tool.   A new VolMap window should open up.  In the selection box type: \u201cwater\u201d and tick the box \u201ccompute for all frames\u201d.  This will calculate a density map based on your water selection and create a new graphical selection. You should see a big white box around your molecule. Open up your graphical selection window and select the new  \u201cIsosurface\u201d  representation. Under the \u201cDraw style\u201d tab use the Isovalue slider to scale back the density to just a few points.  What you are seeing here are bound water molecules relative to the protein structure. These water molecules stay relatively still compared to the bulk water so create an area of high water density. These can be quite important for drug interactions, protein stability and catalytic centers. We often don't display waters in simulations for clarity, and often forget that they are there.  If all goes well you might see something like this. The red and blue lines are the chains of the protein, the cyan blobs are regions of high water density averaged from the longer simulation.    Exercise : see if you can identify any ordered waters near the catalytic residues of HIV protease (Asp 25). You may change the resolution of Volmap to 0.5 for more detail.   You can also do this sort of view for ligands too, to show where they bind. Always make sure you first center your target protein or else this sort of representation will not make sense!",
            "title": "b) Using Volmap to map ligand density."
        },
        {
            "location": "/tutorials/molecular_dynamics_201/molecular_dynamics_201/#8-including-a-non-standard-ligand",
            "text": "The difficulty with simulating new ligands in molecular dynamics simulations is about finding the right parameter files. Your simulation is only as good as the parameters they use, so it is worthwhile trying to validate them. Most of the common residues, such as amino acids, nucleotides and glycosides have decent parameters for the main MD codes, but finding suitable parameters for new drug molecules can be arduous.  For this exercise, we won't go to excessive trouble. We are going to add a drug molecule Amprenavir (APV), a protease inhibitor, into the binding site of our HIV protease using parameters from a website service  SwissParam .  First of all we need a representative drug molecule for APV and convert it to a mol2 format. This is just a different molecular format to pdb. An easy way to do this is use the program  Chimera  (similar to VMD).  The pdb structure  3NU3  provides a high resolution structure of a wild type HIV protease with APV. You will see there are two APV molecules in the model plus alternative side chain structures. This is the result of crystallography with high resolution data on a symmetrical dimer.  In the  Namd_intermediate_template_finished  build directory, we have an edited version of the\n3nu3 pdb file, taking out the artifacts. From this we can extract chain A, B of the protease and the APV as separate molecules. The APV does not have hydrogens attached so these need to be added and a mol2 version made if we are to use SwissParam.  This can be intuitively done with the Chimera program. (We won't cover all the details here, but it shouldn't be too much trouble. Extract and save a pdb version of the APV molecule from the edited 3nu3 pdb file, - (in this case edited to chain C). Open the APV pdb file in Chimera, add hydrogens and save as a mol2 file.  If you submit the APV.mol2 file to the  SwissParam  website, you will receive a lig.zip file containing the following files:  lig.crd\nlig.itp\nlig.mol2\nlig.par         \u2190 this is the parameter file to run the APV ligand simulation\nlig.pdb         \u2190 we need this pdb file to build the APV in the model\nlig.psf\nlig.rtf         \u2190 we need this topology file to build the model  This is stored in the  /BUILD_DIR/lig  folder.  In the build script  build_hiv_3nu3_APV  there are some changes to include the new structure such as the lines:  topology  lig/lig.rtf           \nsegment   C {pdb lig/lig.pdb}\ncoordpdb  lig/lig.pdb C  The model has been already built, solvated and ionized, and stored under:  /InputFiles/hiv_prot_APV.psf  Changes also have to be made to the configuration files to include the new parameters for APV.\nIn the configuration file  sim_opt_3nu3_APV.conf  we can see the added line:  parameters InputFiles/Parameters/lig.par  This is to tell NAMD how to treat the new APV molecule in the simulation.  A small sample of data is already precomputed and stored under  /OutputFiles/  as  hiv_prot_APV.dcd . You can view this using VMD, with the  /InputFiles/hiv_prot_APV.psf  file.   Exercise : With the simulation data, try to visualize the APV alone. Does the molecule behave as you might expect? Do the charges on the atoms seem reasonable? Look up some mutations associated with Amprenavir resistance. Does this make sense in the model?   So concludes the intermediate tutorial. A more advanced workflow called  MD_workflow_py  was written with using the Python programming language. This new workflow uses a similar structure to what was shown in this tutorial and has additional capability to manage thousands of independent jobs and generate molecular animations.",
            "title": "8 - Including a non-standard ligand"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/",
            "text": "De novo Genome Assembly for Illumina Data\n\n\nProtocol\n\n\nWritten and maintained by \nSimon Gladman\n - Melbourne Bioinformatics (formerly VLSCI)\n\n\nProtocol Overview / Introduction\n\n\nIn this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.\n\n\nWhat is de novo genome assembly?\n\n\nGenome assembly refers to the process of taking a large number of short \nDNA sequences\n and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used.  Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found \nhere\n.\n\n\nPaired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process.\n\n\nThe goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose.\n\n\nThe mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See \nthis document\n for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d\n\n\nGenome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]\n\n\nWhy do we want to assemble an organism\u2019s DNA?\n\n\nDetermining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].\n\n\nThe protocol in a nutshell:\n\n\n\n\nObtain sequence read file(s) from sequencing machine(s).\n\n\nLook at the reads - get an understanding of what you\u2019ve got and what the quality is like.\n\n\nRaw data cleanup/quality trimming if necessary.\n\n\nChoose an appropriate assembly parameter set.\n\n\nAssemble the data into contigs/scaffolds.\n\n\nExamine the output of the assembly and assess assembly quality.\n\n\n\n\nFigure 1: Flowchart of de novo assembly protocol.\n\n\n\n\nRaw read sequence file formats.\n\n\nRaw read sequences can be stored in a variety of formats. The reads can be stored as text in a \nFasta\n file or with their qualities as a \nFastQ\n file. They can also be stored as alignments to references in other formats such as \nSAM\n or its binary compressed implementation \nBAM\n. All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.)\n\n\nThe most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.\n\n\nBioinformatics tools for this protocol.\n\n\nThere are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.\n\n\nGenomics Virtual Laboratory resources for this protocol.\n\n\nDepending on your requirements and skill base there are two options for running this protocol using GVL computing resources.\n\n\n\n\n\n\nYou can use \nGalaxy-tut\n or your own \nGVL\n server.\n\n\n\n\nAll of the suggested tools for this protocol are installed and available.\n\n\n\n\n\n\n\n\n\nIf you\u2019re happy and comfortable using the command line, you can do this with your own \nGVL Linux\n instance on the \nNeCTAR Research Cloud\n.\n\n\n\n\nMost of the suggested tools are available on the command line as environment modules.\n\n\nEnter \nmodule avail\n at a command prompt on your instance for details.\n\n\n\n\n\n\n\n\nYou can also use your own computing resources.\n\n\n\n\nSection 1: Read Quality Control\n\n\nPurpose:\n\n\nThe purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.\n\n\nSteps involved and suggested tools:\n\n\nExamine the quality of your raw read files.\n\n\nFor FastQ files (the most common), the suggested tool is \nFastQC\n. Details can be found \nhere\n. FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.)\n\n\nFastQC on any GVL Galaxy is located in: \nNGS: QC and Manipulation \u2192 FastQC: Comprehensive QC\n\n\nCommand line: \nfastqc\n\n\n\n\nDetails on installation and use can be found \nhere\n.\n\n\n\n\nSome of the important outputs of FastQC for our purposes are:\n\n\n\n\nRead length - Will be important in setting maximum k-mer size value for assembly\n\n\nQuality encoding type - Important for quality trimming software\n\n\n% GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution.\n\n\nTotal number of reads - Gives you an idea of coverage..\n\n\nDips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run.\n\n\nPresence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc.\n\n\nPresence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.\n\n\n\n\nQuality trimming/cleanup of read files.\n\n\nNow that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called \nTrimmomatic\n. Details on Trimmomatic can be found \nhere\n.\n\n\nTrimmomatic on GVL systems: \nNGS: QC and Manipulation -> Trimmomatic\n\n\nCommand line: details and examples \nhere\n.\n\n\n\n\njava -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE\n for Paired End Files\n\n\njava -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE\n for Single End Files\n\n\n\n\nTrimmomatic can perform many read trimming functions sequentially.\n\n\nSuggested Trimmomatic functions to use:\n\n\n\n\nAdapter trimming\n\n\nThis function trims adapters, barcodes and other contaminants from the reads.\n\n\nYou need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions.\n\n\nThe default quality settings are sensible.\n\n\nThis should always be the first trimming step if it is used.\n\n\n\n\n\n\nSliding window trimming\n\n\nThis function uses a sliding window to measure average quality and trims accordingly.\n\n\nThe default quality parameters are sensible for this step.\n\n\n\n\n\n\nTrailing bases quality trimming\n\n\nThis function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases.\n\n\nUse FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point.\n\n\n\n\n\n\nLeading bases quality trimming\n\n\nThis function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads.\n\n\nUse FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary.\n\n\n\n\n\n\nMinimum read length\n\n\nOnce all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file.\n\n\nThe most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph.\n\n\n\n\n\n\n\n\nThings to look for in the output:\n\n\n\n\nNumber of reads orphaned by the trimming / cleanup process.\n\n\nNumber of pairs lost totally.\n\n\n\n\nTrimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.\n\n\nPossible alternate tools:\n\n\nRead quality trimming: \nnesoni clip\n, part of the \nnesoni\n suite of bioinformatics tools. Available at \nhttp://www.bioinformatics.net.au/software.shtml\n\n\n\n\nSection 2: Assembly\n\n\nPurpose:\n\n\nThe purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method.\n\n\nYou shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d\n\n\nSteps involved and suggested tools:\n\n\nAssembly of the reads.\n\n\nThe suggested assembly software for this protocol is the \nVelvet Optimiser\n which wraps the Velvet Assembler. The \nVelvet\n assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see \nhere\n for details).\n\n\nVelvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously.\n\n\nThe quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c).\n\n\nVelvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line.\n\n\nIn Galaxy: \nNGS-Assembly \u2192 Velvet Optimiser\n\n\nCommand line: details and examples \nhere\n.\n\n\n\n\nExample command line for paired end reads in read files \nreads_R1.fq\n and \nreads_R2.fq\n using a kmer-size search range of \n63\n - \n75\n.\n\n\nVelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory\n\n\n\n\nThe critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.\n\n\nExamine the draft contigs and assessment of the assembly quality.\n\n\nThe Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file.\n\n\nThe assembly parameters used in the final assembly can also be found as part of the last entry in the log file.\n\n\nThe \ncontig_stats.txt\n file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc.\n\n\nMore detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. (\nFasta Manipulation \u2192 Fasta Statistics\n).\n\n\nPossible alternative software:\n\n\nAssembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include:\n\n\n\n\nSpades\n\n\nSOAP-denovo\n\n\nMIRA\n\n\nALLPATHS\n\n\n\n\nSee \nhere\n for a comprehensive list of - and links to - short read assembly programs.\n\n\n\n\nSection 3: What next?\n\n\nPurpose:\n\n\nHelp determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now.\n\n\nSome things to remember about the contigs you have just produced:\n\n\n\n\nThey\u2019re draft contigs.\n\n\nThey may contain some gaps or regions of \u201cN\u201ds.\n\n\nThere may be some mis-assemblies.\n\n\n\n\nWhat happens with your contigs next is determined by what you need them for:\n\n\n\n\nYou only want to look at certain loci or genes in your genome\n\n\nCheck and see if the regions of interest have been assembled in their entirety.\n\n\nIf they have then just use the contigs of interest.\n\n\nIf they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions.\n\n\nPerforming an automatic annotation on your draft contigs can help with this.\n\n\n\n\n\n\nYou want to perform comparative genomics analyses with your contigs\n\n\nDo your contigs cover all of the regions you are interested in?\n\n\nSome of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats.\n\n\nDo your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below.\n\n\n\n\n\n\nYou want to \u201cfinish\u201d the genome and publish it in genbank.\n\n\nCan your assembly be improved with more and/or different read data?\n\n\nCan you use other tools to improve your assembly with your current read data?\n\n\n\n\n\n\n\n\nPossible tools for improving your assemblies:\n\n\nMost of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied!\n\n\nMis-assembly checking and assembly metric tools:\n\n\n\n\nQUAST - Quality assessment tool for genome assembly \nhttp://bioinf.spbau.ru/quast\n\n\nMauve assembly metrics - \nhttp://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve\n\n\nInGAP-SV - \nhttps://sites.google.com/site/nextgengenomics/ingap\n and \nhttp://ingap.sourceforge.net/\n\n\ninGAP is also useful for finding structural variants between genomes from read mappings.\n\n\n\n\n\n\n\n\nGenome finishing tools:\n\n\nSemi-automated gap fillers:\n\n\n\n\nGap filler - \nhttp://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/\n\n\nIMAGE (V2) - \nhttp://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page\n\n\n\n\nGenome visualisers and editors\n\n\n\n\nArtemis - \nhttp://www.sanger.ac.uk/resources/software/artemis/\n\n\nIGV - \nhttp://www.broadinstitute.org/igv/\n\n\nGeneious - \nhttp://www.geneious.com/\n\n\nCLC BioWorkbench - \nhttp://www.clcbio.com/products/clc-genomics-workbench/\n\n\n\n\nAutomated and semi automated annotation tools\n\n\n\n\nProkka - \nhttps://github.com/tseemann/prokka\n\n\nRAST - \nhttp://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer\n\n\nJCVI Annotation Service - \nhttp://www.jcvi.org/cms/research/projects/annotation-service/",
            "title": "De Novo Genome Assembly for Illumina Data"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#de-novo-genome-assembly-for-illumina-data",
            "text": "",
            "title": "De novo Genome Assembly for Illumina Data"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#protocol",
            "text": "Written and maintained by  Simon Gladman  - Melbourne Bioinformatics (formerly VLSCI)",
            "title": "Protocol"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#protocol-overview-introduction",
            "text": "In this protocol we discuss and outline the process of de novo assembly for small to medium sized genomes.",
            "title": "Protocol Overview / Introduction"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#what-is-de-novo-genome-assembly",
            "text": "Genome assembly refers to the process of taking a large number of short  DNA sequences  and putting them back together to create a representation of the original chromosomes from which the DNA originated [1]. De novo genome assemblies assume no prior knowledge of the source DNA sequence length, layout or composition. In a genome sequencing project, the DNA of the target organism is broken up into millions of small pieces and read on a sequencing machine. These \u201creads\u201d vary from 20 to 1000 nucleotide base pairs (bp) in length depending on the sequencing method used.  Typically for Illumina type short read sequencing, reads of length 36 - 150 bp are produced. These reads can be either \u201csingle ended\u201d as described above or \u201cpaired end.\u201d A good summary of other types of DNA sequencing can be found  here .  Paired end reads are produced when the fragment size used in the sequencing process is much longer (typically 250 - 500 bp long) and the ends of the fragment are read in towards the middle. This produces two \u201cpaired\u201d reads. One from the left hand end of a fragment and one from the right with a known separation distance between them. (The known separation distance is actually a distribution with a mean and standard deviation as not all original fragments are of the same length.) This extra information contained in the paired end reads can be useful for helping to tie pieces of sequence together during the assembly process.  The goal of a sequence assembler is to produce long contiguous pieces of sequence (contigs) from these reads. The contigs are sometimes then ordered and oriented in relation to one another to form scaffolds. The distances between pairs of a set of paired end reads is useful information for this purpose.  The mechanisms used by assembly software are varied but the most common type for short reads is assembly by de Bruijn graph. See  this document  for an explanation of the de Bruijn graph genome assembler \u201cVelvet.\u201d  Genome assembly is a very difficult computational problem, made more difficult because many genomes contain large numbers of identical sequences, known as repeats. These repeats can be thousands of nucleotides long, and some occur in thousands of different locations, especially in the large genomes of plants and animals. [1]",
            "title": "What is de novo genome assembly?"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#why-do-we-want-to-assemble-an-organisms-dna",
            "text": "Determining the DNA sequence of an organism is useful in fundamental research into why and how they live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases [2].",
            "title": "Why do we want to assemble an organism\u2019s DNA?"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#the-protocol-in-a-nutshell",
            "text": "Obtain sequence read file(s) from sequencing machine(s).  Look at the reads - get an understanding of what you\u2019ve got and what the quality is like.  Raw data cleanup/quality trimming if necessary.  Choose an appropriate assembly parameter set.  Assemble the data into contigs/scaffolds.  Examine the output of the assembly and assess assembly quality.   Figure 1: Flowchart of de novo assembly protocol.",
            "title": "The protocol in a nutshell:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#raw-read-sequence-file-formats",
            "text": "Raw read sequences can be stored in a variety of formats. The reads can be stored as text in a  Fasta  file or with their qualities as a  FastQ  file. They can also be stored as alignments to references in other formats such as  SAM  or its binary compressed implementation  BAM . All of the file formats (with the exception of the binary BAM format) can be compressed easily and often are stored so (.gz for gzipped files.)  The most common read file format is FastQ as this is what is produced by the Illumina sequencing pipeline. This will be the focus of our discussion henceforth.",
            "title": "Raw read sequence file formats."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#bioinformatics-tools-for-this-protocol",
            "text": "There are a number of tools available for each step in the genome assembly protocol. These tools all have strengths and weaknesses and have their own application space. Suggestions rather than prescriptions for tools will be made for each of the steps. Other tools could be substituted in each case depending on user preference, experience or problem type.",
            "title": "Bioinformatics tools for this protocol."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#genomics-virtual-laboratory-resources-for-this-protocol",
            "text": "Depending on your requirements and skill base there are two options for running this protocol using GVL computing resources.    You can use  Galaxy-tut  or your own  GVL  server.   All of the suggested tools for this protocol are installed and available.     If you\u2019re happy and comfortable using the command line, you can do this with your own  GVL Linux  instance on the  NeCTAR Research Cloud .   Most of the suggested tools are available on the command line as environment modules.  Enter  module avail  at a command prompt on your instance for details.     You can also use your own computing resources.",
            "title": "Genomics Virtual Laboratory resources for this protocol."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-1-read-quality-control",
            "text": "",
            "title": "Section 1: Read Quality Control"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose",
            "text": "The purpose of this section of the protocol is to show you how to understand your raw data, make informed decisions on how to handle it and maximise your chances of getting a good quality assembly. Knowledge of the read types, the number of reads, their GC content, possible contamination and other issues are important. This information will give you an idea of any quality issues with the data and guide you on the choice of data trimming/cleanup methods to use. Cleaning up the raw data before assembly can lead to much better assemblies as contamination and low quality error prone reads will have been removed. It will also give you a better guide as to setting appropriate input parameters for the assembly software. It is a good idea to perform these steps on all of your read files as they could have very different qualities.",
            "title": "Purpose:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools",
            "text": "",
            "title": "Steps involved and suggested tools:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#examine-the-quality-of-your-raw-read-files",
            "text": "For FastQ files (the most common), the suggested tool is  FastQC . Details can be found  here . FastQC can be run from within Galaxy or by command line. (It has a GUI interface for the command line version.)  FastQC on any GVL Galaxy is located in:  NGS: QC and Manipulation \u2192 FastQC: Comprehensive QC  Command line:  fastqc   Details on installation and use can be found  here .   Some of the important outputs of FastQC for our purposes are:   Read length - Will be important in setting maximum k-mer size value for assembly  Quality encoding type - Important for quality trimming software  % GC - High GC organisms don\u2019t tend to assemble well and may have an uneven read coverage distribution.  Total number of reads - Gives you an idea of coverage..  Dips in quality near the beginning, middle or end of the reads - Determines possible trimming/cleanup methods and parameters and may indicate technical problems with the sequencing process/machine run.  Presence of highly recurring k-mers - May point to contamination of reads with barcodes, adapter sequences etc.  Presence of large numbers of N\u2019s in reads - May point to poor quality sequencing run. You need to trim these reads to remove N\u2019s.",
            "title": "Examine the quality of your raw read files."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#quality-trimmingcleanup-of-read-files",
            "text": "Now that you have some knowledge about the raw data, it is important to use this information to clean up and trim the reads to improve its overall quality before assembly. There are a number of tools available in Galaxy and by command line that can perform this step (to varying degrees) but you\u2019ll need one that can handle read pairing if you\u2019ve got paired end reads. If one of the ends of a pair is removed, the orphaned read needs to be put into a separate \u201corphaned reads\u201d file. This maintains the paired ordering of the reads in the paired read files so the assembly software can use them correctly. The suggested tool for this is a pair aware read trimmer called  Trimmomatic . Details on Trimmomatic can be found  here .  Trimmomatic on GVL systems:  NGS: QC and Manipulation -> Trimmomatic  Command line: details and examples  here .   java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticPE  for Paired End Files  java -cp <path to trimmomatic jar> org.usadellab.trimmomatic.TrimmomaticSE  for Single End Files   Trimmomatic can perform many read trimming functions sequentially.  Suggested Trimmomatic functions to use:   Adapter trimming  This function trims adapters, barcodes and other contaminants from the reads.  You need to supply a fasta file of possible adapter sequences, barcodes etc to trim. See Trimmomatic website for detailed instructions.  The default quality settings are sensible.  This should always be the first trimming step if it is used.    Sliding window trimming  This function uses a sliding window to measure average quality and trims accordingly.  The default quality parameters are sensible for this step.    Trailing bases quality trimming  This function trims bases from the end of a read if they drop below a quality threshold. e.g. If base 69 of 75 drops below the threshold, the read is cut to 68 bases.  Use FastQC report to decide whether this step is warranted and what quality value to use. A quality threshold value of 10-15 is a good starting point.    Leading bases quality trimming  This function works in a similar fashion to trailing bases trimming except it performs it at the start of the reads.  Use FastQC report to determine if this step is warranted. If the quality of bases is poor at the beginning of reads it might be necessary.    Minimum read length  Once all trimming steps are complete, this function makes sure that the reads are still longer than this value. If not, the read is removed from the file and its pair is put into the orphan file.  The most appropriate value for this parameter will depend on the FastQC report, specifically the length of the high quality section of the Per Base Sequence Quality graph.     Things to look for in the output:   Number of reads orphaned by the trimming / cleanup process.  Number of pairs lost totally.   Trimmomatic should produce 2 pairs files (1 left and 1 right hand end) and 1 or 2 single \u201corphaned reads\u201d files if you trimmed a pair of read files using paired end mode. It only produces 1 output read file if you used it in single ended mode. Each read library (2 paired files or 1 single ended file) should be trimmed separately with parameters dependent on their own FastQC reports. The output files are the ones you should use for assembly.",
            "title": "Quality trimming/cleanup of read files."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-alternate-tools",
            "text": "Read quality trimming:  nesoni clip , part of the  nesoni  suite of bioinformatics tools. Available at  http://www.bioinformatics.net.au/software.shtml",
            "title": "Possible alternate tools:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-2-assembly",
            "text": "",
            "title": "Section 2: Assembly"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose_1",
            "text": "The purpose of this section of the protocol is to outline the process of assembling the quality trimmed reads into draft contigs. Most assembly software has a number of input parameters which need to be set prior to running. These parameters can and do have a large effect on the outcome of any assembly. Assemblies can be produced which have less gaps, less or no mis-assemblies, less errors by tweaking the input parameters. Therefore, knowledge of the parameters and their effects is essential to getting good assemblies. In most cases an optimum set of parameters for your data can be found using an iterative method.  You shouldn\u2019t just run it once and say, \u201cI\u2019ve assembled!\u201d",
            "title": "Purpose:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#steps-involved-and-suggested-tools_1",
            "text": "",
            "title": "Steps involved and suggested tools:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#assembly-of-the-reads",
            "text": "The suggested assembly software for this protocol is the  Velvet Optimiser  which wraps the Velvet Assembler. The  Velvet  assembler is a short read assembler specifically written for Illumina style reads. It uses the de Bruijn graph approach (see  here  for details).  Velvet and therefore the Velvet Optimiser is capable of taking multiple read files in different formats and types (single ended, paired end, mate pair) simultaneously.  The quality of contigs that Velvet outputs is dependent heavily on its parameter settings, and significantly better assemblies can be had by choosing them appropriately. The three most critical parameters to optimize are the hash size (k), the expected coverage (e), and the coverage cutoff (c).  Velvet Optimiser is a Velvet wrapper that optimises the values for the input parameters in a fast, easy to use and automatic manner for all datasets. It can be run from within GVL Galaxy servers or by command line.  In Galaxy:  NGS-Assembly \u2192 Velvet Optimiser  Command line: details and examples  here .   Example command line for paired end reads in read files  reads_R1.fq  and  reads_R2.fq  using a kmer-size search range of  63  -  75 .  VelvetOptimiser.pl -f \"-shortPaired -fastq -separate reads_R1.fq reads_R2.fq\" -s 63 -e 75 -d output_directory   The critical inputs for Velvet Optimiser are the read files and the k-mer size search range. The read files need to be supplied in a specific order. Single ended reads first, then by increasing paired end insert size. The k-mer size search range needs a start and end value. Each needs to be an odd integer with start < end. If you set the start hash size to be higher than the length of any of the reads in the read files then those reads will be left out of the assembly. i.e. reads of length 36 with a starting hash value of 39 will give no assembly. The output from FastQC can be a very good tool for determining appropriate start and end of the k-mer size search range. The per base sequence quality graph from FastQC shows where the quality of the reads starts to drop off and going just a bit higher can be a good end value for the k-mer size search range.",
            "title": "Assembly of the reads."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#examine-the-draft-contigs-and-assessment-of-the-assembly-quality",
            "text": "The Velvet Optimiser log file contains information about all of the assemblies ran in the optimisation process. At the end of this file is a lot of information regarding the final assembly. This includes some metric data about the draft contigs (n50, maximum length, number of contigs etc) as well as the estimates of the insert lengths for each paired end data set. It also contains information on where to find the final contigs.fa file.  The assembly parameters used in the final assembly can also be found as part of the last entry in the log file.  The  contig_stats.txt  file associated with the assembly shows details regarding the coverage depth of each contig (in k-mer coverage terms NOT read coverage) and this can be useful information for finding repeated contigs etc.  More detailed metrics on the contigs can be gotten using a fasta statistics tool such as fasta-stats on Galaxy. ( Fasta Manipulation \u2192 Fasta Statistics ).",
            "title": "Examine the draft contigs and assessment of the assembly quality."
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-alternative-software",
            "text": "Assembly: There are a large number of short read assemblers available. Each with their own strengths and weaknesses. Some of the available assemblers include:   Spades  SOAP-denovo  MIRA  ALLPATHS   See  here  for a comprehensive list of - and links to - short read assembly programs.",
            "title": "Possible alternative software:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#section-3-what-next",
            "text": "",
            "title": "Section 3: What next?"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#purpose_2",
            "text": "Help determine the suitability of a draft set of contigs for the rest of your analysis and what to do with them now.  Some things to remember about the contigs you have just produced:   They\u2019re draft contigs.  They may contain some gaps or regions of \u201cN\u201ds.  There may be some mis-assemblies.   What happens with your contigs next is determined by what you need them for:   You only want to look at certain loci or genes in your genome  Check and see if the regions of interest have been assembled in their entirety.  If they have then just use the contigs of interest.  If they haven\u2019t, you may need to close gaps or join contigs in these areas. See below for suggestions.  Performing an automatic annotation on your draft contigs can help with this.    You want to perform comparative genomics analyses with your contigs  Do your contigs cover all of the regions you are interested in?  Some of the larger repeated elements (such as the ribosomal RNA loci) may not have all been resolved correctly. Do you care about this? If so then you\u2019ll need to finish these areas to distinguish between the repeats.  Do your contigs show a missing section of the reference genome(s) or a novel section? You may want to check that this is actually the case with some further experiments or by delving deeper into the assembly data. Some tool suggestions for this appear below.    You want to \u201cfinish\u201d the genome and publish it in genbank.  Can your assembly be improved with more and/or different read data?  Can you use other tools to improve your assembly with your current read data?",
            "title": "Purpose:"
        },
        {
            "location": "/tutorials/assembly/assembly-protocol/#possible-tools-for-improving-your-assemblies",
            "text": "Most of these tools are open source and freely available (or at least academically available) but some are commercial software. This is by no means an exhaustive list. No warranties/suitability for purpose supplied or implied!  Mis-assembly checking and assembly metric tools:   QUAST - Quality assessment tool for genome assembly  http://bioinf.spbau.ru/quast  Mauve assembly metrics -  http://code.google.com/p/ngopt/wiki/How_To_Score_Genome_Assemblies_with_Mauve  InGAP-SV -  https://sites.google.com/site/nextgengenomics/ingap  and  http://ingap.sourceforge.net/  inGAP is also useful for finding structural variants between genomes from read mappings.     Genome finishing tools:  Semi-automated gap fillers:   Gap filler -  http://www.baseclear.com/landingpages/basetools-a-wide-range-of-bioinformatics-solutions/gapfiller/  IMAGE (V2) -  http://sourceforge.net/apps/mediawiki/image2/index.php?title=Main_Page   Genome visualisers and editors   Artemis -  http://www.sanger.ac.uk/resources/software/artemis/  IGV -  http://www.broadinstitute.org/igv/  Geneious -  http://www.geneious.com/  CLC BioWorkbench -  http://www.clcbio.com/products/clc-genomics-workbench/   Automated and semi automated annotation tools   Prokka -  https://github.com/tseemann/prokka  RAST -  http://www.nmpdr.org/FIG/wiki/view.cgi/FIG/RapidAnnotationServer  JCVI Annotation Service -  http://www.jcvi.org/cms/research/projects/annotation-service/",
            "title": "Possible tools for improving your assemblies:"
        }
    ]
}